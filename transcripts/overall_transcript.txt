hello and welcome to your basic
debugging lecture so what we're going to
talk about today is
probably things that you've already
covered if you took
2 30 but a little refresher can't hurt
so we're basically going to talk about
when your code isn't working
or when you get errors that you can
understand what do you
practically do we're not going to talk
about anything complicated like using an
actual debugger program or anything
official like that
literally just practically what do we do
in this class when we have errors in
python that
we can't figure out the first thing that
i want to talk about again should
probably be a little bit familiar to you
and this is when you get an error
look to see where python says that the
error
is coming from python error messages are
actually really great
in terms of error messages because they
usually give you at least a little bit
of
an idea of where the problem is coming
from and maybe
what caused it so whenever you get an
be sure to look and see okay what line
is causing the air
and see if there's something obvious
that you can pick out
that might be the problem common things
that people miss are
forgetting to close your parentheses or
forgetting a square bracket or a colon
maybe indenting when you shouldn't or
not
indenting when you should maybe you also
misspelled the name of a variable or a
function or
keyword all of these are things that are
super easy to miss when we're writing
code but hopefully when python
tells you where the problem is you can
focus in a little bit and try and see if
one of these common issues
is what's going on with your code let's
look at an example
okay so first i'm going to open my
notebook
and as an aside this is a really common
issue that i see
when we're working with jupiter
notebooks rather than running entire
scripts
in adam like you might be used to and
that's when you're running a notebook
you need to make sure you're running the
cell
that has all of your import statements
so i'm going to go ahead
and do that first because i want to make
sure that i don't get that error
okay now that we're loaded let's look at
our first example
so i'm going to run this code and it's
pretty simple code
so to speak you're just doing a little
bit of math here i'm doing a bunch of
math
assigning it to a variable let's go
ahead and run
oh no i got an error how unexpected
so when we see the error that it's a
syntax error and python will actually
tell us exactly where
it thinks that the problem is now notice
here
that it thinks that the problem is with
the line j equals two plus two
but i'm looking at that line and i am
looking at the syntax and it looks
beautiful to me so one thing to realize
is python will tell you where it thinks
the error is
but it's not always the case that the
error is exactly there
in this case if you have sharp eyes
you'll see that the error is that i'm
missing a parenthesis on the line above
python thinks that the error is on the
next line because since it doesn't see
a close of the parentheses it moves on
and tries to process the next line as
part of those parentheses and gets very
confused when it sees variable equals in
there
and it throws an error so if you ever
have an
error and you don't see anything wrong
with the line that it says the error is
on
make sure you're checking a line or two
above and seeing if you forgot one of
those things i mentioned
close parentheses is probably the most
common in fact if you've been in my 230
class
you might have seen something i do
that's really silly is i can usually
tell on first glance if someone's code
is missing a parenthesis if that's the
only problem
uh and so something really silly i do is
i'll cover my eyes and i'll tell them
like i bet i can tell you what's wrong
with your code without even looking
and i tell them it's missing parentheses
and i am usually correct and they are
very impressed
so now you know my secret but it is a
really good thing to
check out if you get an error that you
don't understand on a line that looks
perfectly fine okay so obviously the
first thing to do is to see if there's
anything obvious that you
directly can see and fix but that's
usually not going to be the case
so if you don't understand where the
is or what the error actually is your
best bet
is to google the error message and i
know
that that seems super obvious but it is
usually the best thing to do
and luckily in python when you get an
error message you can just
highlight everything in the error
message copy it and put it straight into
google
i highly recommend looking on the site
stack overflow because it's kind of a
one stop shop for
all types of problems that you can have
with coding in any language but
definitely in python
so if you ever have an error that you
don't understand
copy and paste it into google check
first if there's something on stack
overflow because that's usually the most
useful
but even if there's other websites that
can help you at least they have
some information that you don't have
let's look at an example of this
okay so i'm going to run this cell of
code and basically what i'm doing is i'm
taking a range of integers from negative
five
to five and i am calculating the inverse
and the inverse is just one divided by
that number
so my for loop here says for each number
in my sequence
print one divided by i the number so
let's go ahead and run this
oh no we got another error okay so when
we see this
error we might go and say okay what line
is it on print one over
i well okay i'm not missing a colon i'm
not having some weird indenting problem
the line itself looks fine i'm just
printing the result of a division
so i'm really confused i don't
understand the error
i'm going to take my error message now
don't copy your code just copy the error
message
copy and paste and see what google has
to say
and typically when you're googling these
errors something from stack overflow
should come up but
if it doesn't just scan for it a little
bit it's usually really helpful
and i'm going to click on this now this
in stack overflow is going to tell me
what the problem is that people had
and then if you scroll down you'll get
people's responses
to maybe some solutions to that so from
reading this
i can see that i have a zero division
error which essentially is saying
you can't divide by zero but you are
trying to do that anyway
so i go oh okay that makes so much sense
if i'm doing every integer between
negative five and
five then zero is going to be there in
there and when i
divide one by zero of course i'm going
to get a zero division error
so now that i know what the problem is
with the help of stack overflow
i can try and fix it so one way although
there's many ways to fix this i could
say
if i not equal to
zero and then
indent this when i run this now i get
perfectly running code and i'm no longer
getting my zero division error
okay so if you can't see anything
directly wrong and maybe stack overflow
and google
haven't been super helpful one other
thing you can try
is to use print statements print
statements are incredibly useful to
check whether
certain blocks of your code are running
or what
objects look like that you're using and
print statements
are just kind of like a poor man's
debugger because you're able to
see what objects are or see when code is
running
so for instance let's say that you have
an if suite
and it's not running or at least
it's not running according to what you
can see in your output
well if you want to check if you
actually are running that if suite
what better way than to just add a print
statement at the beginning of that suite
and see if it prints out in the output
of your code
or let's say you have an error when
you're working with some data and the
program is telling you that the data
should be only zeros and ones like a
binary variable
but it has other numbers well you
expected it to be a binary variable
but let's print it out to see if it
actually is
print statements are really useful to
look at your data to look at your
objects your lists your dictionaries to
see if they actually contain the things
that you think they do
and make sure that everything looks good
let's look at an example
what we're going to do here is we're
going to look at some data and this is
something that we're going to do a lot
in this class so don't worry if the
syntax of the pandas or python syntax is
a little bit confusing to you because
this is something we'll do all the time
but what i'm going to do is i'm going to
read in some data you can think of it
like an excel spreadsheet or a google
sheet spreadsheet
i'm going to print the data and then i'm
going to look at a certain column
so when i run this oh no
i get an error so when i
printed out my data you can see that
right here before i got there
you can see this is essentially a
spreadsheet and i have an
age column an id column and a number of
cats
column maybe i'm trying to do some cat
lady related analysis
well i wanted to look at just the age
column
but i got an error but everything looks
fine to me so i'm kind of confused as to
why i got this error
and just as a note when you're using
other packages like
pandas or numpy or anything that we're
using sometimes the error messages can
look a little more complicated than what
we were looking at before
because there's a lot of stuff in here
that is not code
you wrote and so what you want to do
when you get an error like this
is to look for the line of code that you
recognize
that is in your code and not in the
actual package
so for instance i'm going to look
through here okay all of this
is not code that i wrote okay but this
is so i can see that the cat's age is
where i got my ear
and it says key error which basically
means that it doesn't think that column
age exists in my data frame
but i literally just saw it when we
printed it out
so let's use a print statement to see
what the actual names of my columns are
to make sure i don't have some kind of
silly error
okay and we'll comment this out to make
sure it doesn't give us another error
okay so now we're running our code again
and we're printing out
our data frame again and all of the
names of the columns
and oh okay i see what the problem is
now that i've printed out the names of
the columns
i can see that there's a random space
before
the column name in age so that's
probably what was giving me an
error because python thinks the name is
not age but
is space age so i could go in and fix
this and we'll learn how to do that
later
but for now what i'm going to do is i'm
just going to say
cats space age to get the column of ages
that i want
let's run this and see if it works and
it does
perfect so now i've grabbed just the
column of all of the ages and if i
wanted to do something with that
plot it take the mean whatever now i can
okay let's talk about some common issues
that you might come across when
programming
i'm sure you've come across a lot of
these before but i just wanted to give
you an
overview of things that might come up um
especially as it pertains the
to the kind of programming that we're
going to do in this class so as i
mentioned before
first thing to look for is syntax errors
did you miss a parenthesis a colon
a closed bracket did you indent when you
shouldn't have
and sort of relatedly did you misspell
anything
when we're doing data science our
variable names can get a little crazy
long
because we're usually describing
something in the data for instance if
your variable name
is children's age standardized that's
pretty long it's a lot of opportunity
for misspelling something
so make sure when you see weird errors
that you don't understand and your code
looks okay make sure you don't have one
of those silly
syntax errors in your code somewhere
secondly as i also mentioned make sure
you're importing the correct packages
if you're using something from numpy
make sure you've imported numpy
and as i mentioned before when we're
working in the jupiter notebooks uh
format
it's a little bit easy to sometimes skip
over cells so make sure that you're
running your import cell
if you're importing any packages again
as i mentioned before
make sure your data and your lists and
your dictionaries and etc
all look the way that you think they do
printing them out is a really good way
to do this
and make sure that for instance if you
have a data frame
that you think only has um
column names with no spaces in it print
them out and make sure that's the case
like the example that we just saw
often when we're working with data
that's not something that we created
which is
going to be almost always there might be
some surprises or things might not look
the way that we assume they will maybe
there's missing values maybe there's
unexpected values
i'm printing out these objects is a
really good way to see what they look
like and see if that's what's causing
any errors
now this next one is a little bit less
common but it definitely still
happens and this is basically what
happens
when you create a function which we'll
be doing a lot of
or you use a function that doesn't
return anything
so if you're like setting a variable
equal to the output of a function
but you didn't return anything you're
going to get something called
none so if you ever see like a nun type
or the word none somehow coming out in
your output when you didn't expect it
make sure that your functions that you
want to return a value
actually do return a value
and lastly this is very data specific
but check if there's any missing values
in your data
sometimes things don't look the way that
you think it will and missing values is
a really common way of that happening
when we have real world data we usually
don't have complete data where every
single row
has every single value for every single
checking if you have missing values or
printing out your data frame to check
yourself if there are missing values is
a really important way
to address errors in your code because
sometimes the algorithms or functions
that you're using
won't accept missing data but there
actually will be some in your data frame
okay so last but not least if you've
tried all of these things and you still
don't understand what's
happening and you're still having
trouble you can of course come to me
with help but i highly recommend making
sure that you try all of those things
first
and see if you can fix the error on your
own not just because
i don't want to have to keep you waiting
but also just because it's empowering to
be able to fix your own
code and errors when you have them so
if you ever need to ask me for help with
a coding problem make sure you send me
all of these things
first i need a screenshot of your code
now sometimes i'll ask you to actually
send me the file or copy and paste into
slack or something
but for first round of just trying to
see what's wrong
a screenshot of the code where the
problem is where the error is and a
couple lines above and below
is very useful similarly if there's an
error make sure you send me a screenshot
of what the error actually says
third make sure you tell me what you've
tried and looked
up that did or did not work for you
for instance you could say here look i
looked at the stack overflow
but doing this didn't help or well i
googled this error but this didn't help
whatever you found make sure you tell me
so that i'm not making you try things
that you already have tried
uh and just wasting your time and last
just give me a quick summary of what you
tried to do
to fix it and make sure that you're
trying to fix things before you're
reaching out to me
just because i don't want you waiting on
a slack or email from me when you could
be working on a project or homework or
something
so make sure you try a couple things and
tell me what those things are
okay hopefully that wasn't too
overwhelming just wanted to do a little
review of how we handle errors and what
debugging is in python
i will see you next time

 
hello and welcome to your logistic
regression 2 lecture in Python so as
usual the first thing we're going to do
run all of our Imports and before we go
and move on any further I just want to
remind you of this General sklearn
framework for building a model because
now that we're in our second model and
we're about to learn a new one next week
I just want to go over this again really
quick so remember that in sklearn we
have a basic workflow that is the same
for pretty much any of the models that
we're going to learn the first thing is
that we're going to create a model then
we are going to fit our model using dot
fit and then we are going to use dot
predict to predict values and that's a
part of helping assess our model so
again create an empty model dot fit and
Dot predict now that's a little
oversimplified but I want you to
remember that because it really is the
same for all of the models that we're
going to use
all right so let's review this example
which we did in the previous logistic
regression python lecture where we were
predicting whether or not customers
would upgrade to a premium version of a
service based on their age their income
and the month subscribed
so this should all look familiar so I've
loaded in the data I've set my
predictors and actually let's do it
again x equals uh fashion
uh Big Dot predictors and Y is going to
be y equals fashion big
square bracket upgrade now this is gonna
do the same thing as what our code
already does just to simplify I'm going
to do this here so we'll say X and Y so
once we have our X and our Y data the
train test split function allows us to
split our data to our train in our test
set and it takes in our X data our Y
data and then we can specify what what
kind of split we want so here we've done
an 80 20 train test split where 20 of
our data is in the test set
then we went ahead and created a z-score
object fit it and then used it to
transform both the training and the
testing set notice we never fit our
z-score object or any of our models on
the test set only the train set then
we've created an empty model and notice
this penalty equals none that's a little
bit different and we'll talk about it
later in this lecture but for now you
can just leave it so we've created an
empty model we fit it using our training
data we've grabbed our predicted values
spelled that wrong predict there we go
um grabbed our predicted values and our
predicted probabilities and then we can
do various metrics like our training
metrics and our testing metrics as well
as confusion matrices
all right now that we've reviewed how to
build and run a model let's go ahead and
talk a little bit about the logistic
regression coefficients now similar to
the linear regression coefficients we
can pull our coefficients directly from
our model you'll notice this code looks
almost exactly the same as it did when
we pulled regression coefficients from
linear regression models the only
difference is that when we're pulling
our coefficients and our intercept
you'll notice the added square bracket 0
at the end after the co-f underscore or
the dot intercept underscore this is
because an SK learn when you build a
logistic regression for more than two
categories you actually end up building
more than one logistic regression so you
have more than one set of coefficients
we only have a binary classification so
there is only one set of coefficients
but in order to grab them we need to add
this square bracket to zero and again
only for logistic regression
so once we run this we will have a data
frame of our coefficients just like we
did for linear regression now let's just
take a look at them really quickly
so that looks as we expected we have the
coefficients which again are in terms of
log odds by default and we have all the
names of our predictors but often we
want to use odds coefficients in
addition to or instead of the log odds
coefficients in order to do that we need
to add a new column to our coefficient
data frame to do that I'm going to call
it odds Co-op
and I'm going to set it equal to the
exponentiated versions of the log odds
co-ops remember the opposite of a log is
an exponent so we are going to
exponentiate our coefficients by using
the NP dot exp function and grabbing
co-f square bracket
cos so this is going to take our
existing coeffs and exponentiate them to
give us the odds coefficients so let's
go ahead and run that
and actually let's print it out that
would be helpful
beautiful so now we have both the log
odds coefficients and the odds
coefficients so let's go ahead and take
a look so here we can see that in this
model income month subbed and intercept
all have positive or slightly positive
relationships with the predicted
probability that means in this data set
all of them are associated with an
increase so as age increases income
increases and months subbed increases so
does the predicted probability of
upgrading to the premium version of the
service
to go over how we interpret these
coefficients again we don't usually
interpret the intercept
let's start with age so we know that
we've z-scored so we're saying that a
one standard deviation increase in age
is associated with a 0.44 increase in
the predicted log odds
similarly as income increases by one
standard deviation the predicted log
odds increase by 0.01199
Etc
for months subbed as month subbed
increases by one standard deviation the
predicted log odds increase by 0.024
so again that formula is as our
predictor increases by one unit standard
deviation if it's Z squared our
predicted log odds increase or decrease
by whatever the coefficient amount is
all right that's for log odds let's talk
about the odds coefficients so odds
coefficients remember are multiplicative
meaning that when we increase by one
unit we multiply by the coefficient so
for age we say as age increases by one
standard deviation the predicted odds
increase by 1.55 times or or multiplied
by 1.55 times in other words there's
about a 55 increase in the predicted
odds for income as income increases by
one standard deviation the predicted
odds are multiplied by 1.01
for months subbed as a month subbed
predicted odds are multiplied by 1.025
so again odds coefficients are in terms
of multiplication log odds like linear
regression are in terms of addition
every time we increase the predictor in
law Gods we add the coefficient every
time we increase the predictor and odds
we multiply by the coefficient
now another thing I want to talk about
is that threshold of 0.05 we talked
about in an earlier lecture that when we
have these predicted probabilities we
turn those into zeros and ones typically
by putting anything with a predicted
probability above 0.5 as a one and
anything with a predicted probability
below 0.5 as a zero but as I mentioned
before and as we've kind of explored
with the Roca you see you don't have to
use the 0.5 threshold so let's go ahead
and try it out when we don't use that
threshold and we'll do it by hand so you
can see exactly what's happening here so
I'm going to go ahead and I'm going to
load in some new data like we did before
in the previous python lecture and I'm
going to say x Nu equals fashion new
square brackets
um predictors
and then I'm in a z-score I'm going to
say x new predictors equals uh Z what
did we call it z-score
Z score dot transform X new
so now we have our new future data that
we can use and let's grab the predicted
probabilities to do that I'm going to
say y pred prob equals our model and
again the model that we've been training
is my logit
mylogit dot predict
praba
and we're going to give it our X new
data now let's just go ahead and see
what that looks like so we're going to
say why pred prawn there we go and run
so you can see again for every single
data point we have the probability it's
in class 0 versus the probability it's
in class one and we can just grab one of
these columns right because they're
basically the same information one
column is one minus the other column so
I'm going to go ahead and just grab the
probability that they're in class 1 by
saying give me all the rows comma only
the column at index one so now we just
have all of the predicted probabilities
for every data point of it being in the
category one oh and I should probably
reassign this so why pred
prob equals okay we'll rerun that
all right so now we normally use a
threshold of 0.5 but I now have a
variable called thresh that we can set
to whatever value we want between 0 and
1. so I'm going to set it at point three
which basically means that it's easier
to classify something as a one because
it only has to be above a probability of
0.3 in order to classify something as
one and if you think about real world
contacts in which we might want this to
be the case think about a case where
we're detecting something that's really
important to us say we're in a marketing
department and we want to know if
someone might be open to opening a
company credit card with us we want to
make sure that anyone who might be
willing is targeted with a credit card
ad and so we want to be very liberal
with how we predict that we want to make
sure that anyone that is maybe a little
bit likely is getting that credit card
ad because of course we want them to
open that credit card
on the other hand you can think of cases
where we might want to raise that
threshold for instance we've talked
before about like a cancer diagnosis or
a disease diagnosis if this is something
that can really negatively impact
someone we might want to be very careful
about when we're predicting that someone
is in that category in those cases where
we want to be very careful about
predicting ones we might want to raise
our threshold
so again this is going to depend a
little bit on the situation or a little
bit on your model but we can raise or
lower the threshold and I want to show
you how we can do that here by hand
all right so I'm going to go ahead and
create a new variable called why pred
prob Thresh
um and I'm going to set it equal to a
Boolean so I'm going to check whether or
not ypread prom right our probability of
being in class 1 is greater than our
threshold or not so I'm going to say y
pred prob and actually here I'll say y
pred
prob is greater than the threshold
um so actually let me just print this
out so you see what's happening so say
bro I put prom fresh okay
um and let's run this so you can see
that when we get this we get a bunch of
true and false values true if it's
greater than the threshold false if it's
not I actually want these to be zeros
and ones so to do that I'm just going to
multiply these by one when you multiply
a Boolean by one it uh converts it to a
zero or a one
so you can see here we now have a bunch
of zeros and ones actually not very many
zeros at all so you can see that when we
lowered our threshold we are much more
likely to classify someone as a one
compared to when we have the Threshold
at 0.5 and we can change this around so
say we want to be very careful about
classifying people as going to upgrade
so we can run this and you can see now
we have a lot more zeros because we
raised the threshold to be considered a
one now let's go ahead and actually
let's not print this out so it's not
messy let's go ahead and look at the
accuracy when our threshold is at 0.7 so
we can calculate an accuracy score I'm
going to say accuracy score and remember
this takes actual values comma predicted
values so I'm going to give it the
actual values which we can grab from
Fashion new so we'll say fashion new
square brackets upgrade that's the
actual values of whether they upgraded
and then we'll put ypread prob thrush
so when we run this you can see that
when our threshold is at 0.7 we have an
accuracy of 42.7 percent which is not
great
let's see what it is when we do our
traditional 0.5 so we can run this run
this and there we get our very easily
recognizable about 60 accuracy how about
0.3 which we tried earlier
okay very similar there we get about a
sixty percent accuracy as well so you
can play around with this and see what
different thresholds do but you may in
your life when you're running some
logistic regressions want to change the
threshold that you're using maybe you
want to be more conservative about
predicting ones or more liberal about
conserving ones and you can change that
by changing the threshold that you use
to classify things as a category one or
zero all right last but not least I want
to talk to you about regularization now
regularization is actually a very
important topic that we'll talk about
later in the semester but unfortunately
I have to preview it now so when you do
logistic regression it actually applies
something called regularization by
default and I have the documentation
here in case you want to look now to
give you a little bit of nerd programmer
statistician T there was actually a huge
uproar about this decision and if you
ever want to hear me rant about this I
was there for it and it was a chaotic
time but logistic regression applies
default you notice that this is in bold
because of all that tea and drama that I
was talking about before they made sure
it is very clear in the documentation
that this is happening now when we learn
about regularization hopefully we'll
have a better intuition for what this is
but for now I just want to tell you that
it basically means that they're altering
the way the logistic regression Works in
a way that makes our coefficients just a
little bit smaller or actually closer to
zero on average that means the big
positive and big negative coefficients
are kind of shrunk a little bit more
towards zero and it's not too extreme
but I want to show you what happens so
let's go ahead and create our models so
I'm going to say my logit equals
logistic regression and I'm not going to
do anything I'm not going to put any
arguments in there it's just going to be
empty so these are the default settings
and then I'm going to say my logit DOT
fit X train y
train and then let's go ahead and print
out our coefficients okay so I'm going
to go ahead and run this and print out
my coefficients but let's go ahead and
run our second model I'm going to call
it my load it to equals logistic
regression okay but this time I'm going
to say penalty equals none now when
penalty equals none we are turning off
this regularization and remember
regularization just kind of squishes our
coefficients a little bit closer to zero
so I'm going to do that I'm going to say
my logit2 DOT fit X train y train and
then as before I'm going to print out
the coefficient all right let's go ahead
and run that so you can see that the
results are pretty similar in this case
however you can see that between the
model without regularization and the
model the default one with
regularization all of the coefficients
are just a tiny bit closer to zero so
you can see for instance here this is a
little bit smaller even The Intercept is
a little bit smaller here now the effect
isn't super strong here it can depend on
your model a little bit but all I need
you to know for now until we get to the
regularization section later on in our
class is that if you don't put penalty
equals none your coefficients will just
be a little closer to zero on average
now it shouldn't affect your results too
much I just want you to be aware of that
at and again if you would like to hear
my rant about when this all happened I
would love to share it with you again it
was a very chaotic time but that is all
that I have for you today and I will see
you next time

 
hello and welcome to your python data
visualization lecture
so first i'm going to of course run all
of my imports and notice we're importing
some new things today for instance this
plot nine package which is going to
allow us to do our plotting as well as
this mt cars data set
now ggplot is a beautiful way of making
data visualizations and as i mentioned
in the conceptual lecture is basically a
way that you can build these little
parts of the graph one by one and add
them together to get your total graph
which we'll talk about more in a second
but it's basically like lego blocks for
data visualization which is why i love
it i think that the way that ggplot
works forces you to be a little bit more
thoughtful about your data visualization
process
so now that that's done importing i
actually just want to show you a couple
of graphs first before we dig deep into
maybe some of the more foundational
stuff that we're going to learn
all right so first let's just take a
look at mt cars
so we have this data set about cars and
it has things like the miles per gallon
this number of cylinders whatever this
variable is uh and all of these
different variables like the number of
carburetors gears etc um so we're going
to use this data to just create a couple
of plots see what ggplot can do and then
i'll go back and tell you a little bit
more about the basics of building a gg
plot
all right so first things first let's do
the most basic graph that someone can do
we're going to say ggplot oops there we
go
and then we're going to save the name of
the data frame mt cars aes we're going
to say x equals the weight of the car
and then uh y equals the mpg so we're
basically going to look at the
relationship between mpg and weight
which basically in my mind would
probably be negative right if you weigh
more your miles per gallon is worse so
we're going to say plus the gm point
and let's look at a scatter plot
beautiful so my suspicions were
confirmed the more a car weighs the
worse its miles per gallons are which
makes sense if you have more stuff that
you're lugging around it's going to take
more fuel
now let's make this a little prettier
and this gets into some of the lego
brick stuff that i was talking about
so let's go ahead and add some things so
first let's add
a line that shows the downward trend
we're gonna do that with stat smooth
method equals lm
and then let's um
yeah let's leave it at that okay so
we're going to run this
and you can see that's exactly what we
get so the same graph that we had before
we're now adding a new lego brick of
that downward trend line this just helps
us kind of see more clearly what that
trend is
and we can add even more
so let's go ahead and say that i want to
make the same graph but i want to make
it for the different gears of car
because maybe there's a different
relationship between weight and mpg
depending on the number of gears that
your car has
i'm going to do that using the facet
wrap function
and save by
gear
and so you can see we made this exact
same graph but once for
cars with three four and five gears and
you can see the relationships are
actually a little bit different
last but not least let's just make this
graph a little prettier so in addition
to having the facet wrapped by gear
let's go ahead and color
by
factor
gotta put that in strings
and we'll run
oh okay so now we have a slightly more
aesthetically pleasing graph where we
have the exact same graph but now our
points and our lines are colored by what
gear the car is just to help
differentiate those
alright so i didn't expect you to
completely know ggplot from that i just
wanted to show you the types of things
that ggplot can do and now we are going
to dig back into the basics
so let's go ahead and reload that
penguin data that we had been working
with before
and we'll just say penguin.have so we
can see it
okay so we are going to build a couple
of graphs that give us more information
about penguins
so the first major thing that you always
have to tell ggplot is what data you
want to use and the way we do that is by
using the ggplot function
and the first argument that goes in the
ggplot function is the data frame that
we want to use so in this case we want
to use the penguin data frame
and then the second argument is going to
be this aes function so first we're just
going to type a blank aes function right
here so aes stands for aesthetics and
then these open parenthesis
inside the aes function we're going to
tell python which variables we
specifically want to use so we've told
it where to grab these variables from
and now which ones we want to use
so let's go ahead and make a scatter
of um bill length and build depth for
the different penguins so we're going to
say x equals bill
length
m so we're going to give it the name of
a column as a string
and then for y let's say
bill
depth
mm
so that's the first thing that we want
to do we can also add things for
instance let's say we want to see the
different species of penguin and like
how their build length and depth
compare so we can also for instance add
color this is going to make any of the
things that we add colored differently
by species so we're going to say color
equals species
but that's all we're going to do for now
so let's run this
and you can see what we get from ggplot
is a blank canvas we've told it where to
get the data we've told it what data
we're eventually going to want we want
bill length build up and species to be
included but we haven't told it how we
want it to include those do we want dots
do we want lines what do we want
so let's go ahead and copy and paste
this down here and do the second step of
making a gg plot which is to tell it how
to plot the data
so i for in this instance want a scatter
plot so i'm going to say plus gm
point
and then let's go ahead and run that
beautiful so we get this plot where we
have the same background as before but
now ggplot knows how to plot the data it
knows that we want points
the way that we tell a ggplot often how
to plot the data is with a geom the gms
are going to say for instance do we want
a g on point do we want a scatter plot g
on box plot do we want a box plot so
we're going to always have our ggplot
function telling ggplot what data you
use and what columns from that data and
then we also have a geom usually that
tells it how to plot that data
so let's do another example we're going
to say ggplot and again the penguin
gotta spell that right
penguin data set we're going to say aes
and we're going to say let's say x
equals body mass
oops in
grams
so we have so far told ggplot to use the
penguin data frame from that data frame
we're going to want to do something with
body mass and grams but we haven't told
it what yet so again when you just run
that alone you're going to get this
blank plot
so let's do the second step and tell it
how we would like to plot
in this case i want a histogram i want
to see the distribution of the different
weights of all the penguins in my data
set and i can do that by adding a geom
in this case geom histogram
and so when we run that we get the same
exact background but now it knows how to
plot the data and so we're getting this
beautiful histogram all right let's do
another one so we're going to say ggplot
we're going to tell it where to grab the
data so in this case penguin and we're
going to say aes x equals let's how
about this time we'll say species
so when we run this you can see again a
blank plot it knows what to plot but not
how to plot it
so we're going to copy and paste this
and then we're going to go and tell it
how go ahead and tell it how to plot in
this case let's say i want to do a bar
chart because i want to know the count
of different penguin species so i'm
going to say plus geom
bar
and when i run this
you can see it gives us a bar chart
telling us how many adelie penguins chin
strap penguins and gentoo penguins we
have in our data set for instance here
we see chin strap is the most rare and a
deli is the most common
all right let's do one more so we're
going to say gg ggplot
penguin because that is going to be
where like with everything else our data
is coming from
and then inside our aes function we're
going to tell it which columns we want
so i'm going to say x equals species
just like before and this time i'm going
to add y and i'm going to say
fill
mn
so again we've told a gigi
got to spell that right so we'll say
link
there we go
so when i run this again ggplot knows
what to plot but not how to plot it so
it gives us a blank but we can obviously
tell it what geom to use and in this
case i'm going to use a box plot i'm
box plot
and when i do that now it knows how to
plot the data so it's going to go ahead
and give us a box plot of the different
data type or excuse me of the different
penguin species so for instance we see
that a deli penguins tend to have very
short bills whereas gen 2 and chin strap
are a little more similar although
chinstrap seems to be like the highest
the length of all
all right so now that we have the basics
down which is how to tell ggplot what to
plot and how to plot it we can work on
adding extras that just make our graphs
prettier and easier to understand
so i've copy and pasted our code for the
plot above and let's just make some
changes to make it a little prettier
for instance this is a little boring in
black and white maybe i want each of
these box plots to be a different color
i can do that by going into the aes
function and saying
equal species this is basically saying
that i want the different geoms to be
different colors based on which species
it is
so i'm going to go ahead and run this
and you can see that's exactly what i
got now i have my different box plots
different colors
now let's copy that down here one other
thing we might want to change is all of
this gray we'll talk a little bit more
about this in the second visualization
lecture but when we're doing data
visualization we want to make sure that
our message is as clear as possible in
this case our message is about the
differences in bill length between the
species of penguins and all of this gray
in my personal opinion is a little bit
distracting and not really giving us any
new information
so one other thing that we can add
besides a
geom is a theme so i'm going to say plus
theme minimal
and this is going to go ahead and get
rid of that gray background and make
some other changes to just make our
graph a little simpler so let's go ahead
and run this
beautiful so you can see same
information same exact plot we're just
now having a little bit of a less
intrusive
background and again ggplots are just
like legos you can keep adding different
pieces so for instance let's say that i
want to overlay these box plots with the
raw data i can do that by saying plus gm
so you can see we have two different
gm's here we have gm point and gm box
so let's go ahead and run this
beautiful so you can see more than
before the spread of the raw data
because we now have both g on point
which gives us the individual data
points as well as gm box plot
now notice that because we put fill
equals species in the aes function it
applies to every single geom that we
have it both filled the individual box
plots as well as the individual data
points from g on point
now in this case i think it looks really
pretty but say we don't want that
i'm going to go ahead and copy and paste
this code
so we can put different aesthetics like
fill or something like that inside the
general aes function but we don't have
to so i'm going to delete it here
say that we want the same graph as above
but we want all the points to be black
instead of being filled different colors
based on which species of penguin it's
referring to
we can actually set just the fill
aesthetic for the box plot by going into
g on box plot and saying aes
oops there we go fill
so when we run this you can see
that our dots our gm points are no
longer affected by the fill equal
species they're all black no matter what
but our box plots are still filled by
species so basically when you set an
aesthetic in the aes function in our
ggplot function which is right the first
thing that we always do
it will apply to every single geom
that is in your graph
so remember when we did it up here
because fill within the aes function in
ggplot it applied to point and box plot
however if there's ever a situation
where you only want the aesthetic to
apply to a single geom you can actually
just specify it in that specific geom
instead of in the ggplot function when
you first start out all right let's look
at a couple different types of charts
that we can make i'm trying to cover
some of the really common ones but
honestly ggplot is far too big for me to
show you every possible combination of
graph so please as you're doing your
data analysis or if you've taken r make
sure that you look up different geoms or
different plots that you can make
because there's far more than we're
covering here
but let's go ahead and make a bar chart
so let's go ahead and start off with
ggply and we're going to give it the
penguin data frame and then say aes
and i want to make a bar chart that
shows the body mass of the different
species of penguin so for each species i
want a bar where the height of the bar
is whatever the average body mass is for
that species of penguin
so as i imagine my graph i'm going to
say okay x is going to be the species
because i want one bar per species and y
is going to be the body
mass in grams because i want the height
of the bar to do with the body mass of
the penguins
and just to make it beautiful i'm going
to say fill equal species so this is
going to make each bar a different color
based on the species
so i have again and let's just run this
i've told ggplot what to plot but not
so we're actually going to use something
new here instead of a geom we're going
to use something called stat summary
now because our data frame is each
individual penguin we don't actually
have like a row or a column that tells
us the average body mass so that's where
stat summary comes in
so stat summary is going to take
something called fun data it stands for
function of the data
and this is basically saying what
computation do you want me to do and use
as the height of the bar
so in this case i'm going to say fun
data equals mean
sdl
this just stands for like the mean and
the standard deviation so it's going to
calculate the mean and the standard
deviation of the body mass for each
species
and then i'm going to say what type of
graph i want so in this case i want a
bar plot so i'm going to say
geom
equals bar
so again just to review we've told it
everything we're used to right what data
we're going to plot but now we're using
the stat summary function to do some
calculations and then plot it for us in
this case it's going to calculate the
mean body mass for each
penguin species and then use that to
create a bar chart so let's go ahead and
run this
and you can see that's exactly what it
did so now we know that gentoo penguins
are by far the largest penguin species
of the three
and again we'll go over this a little
bit more in the next lecture because
that's all about making good data
visualization this is more about just
the basics i'm going to copy and paste
this here and then
i'm just going to make some aesthetic
changes so that our plot is beautiful
so i'm going to add and again we'll talk
about this more in the next lecture i'm
going to add i'm going to relabel the x
and y axis so it's more readable i'm
going to add a title i'm going to say
theme minimal because again sometimes
this gray background is just really
distracting and not actually helpful
and then i'm going to make some
aesthetic changes to make this graph
just a little more clear
so let's go ahead and run
beautiful so you can see exact same
information
just a little more clear and a little
more useful
all right so let's talk about another
way that we can make a similar bar chart
so say now i'm interested in the average
bill length for the different species of
penguins
instead of just using stat summary one
thing that i could do is make a data
frame that creates the summary that i
want i'm gonna call this penguin
and i'm gonna use some of my panda
skills that i learned in our last
lecture
so i'm going to say
penguin
dot
group by again this is going to separate
our data into groups and i'm going to
group by species
and i'm just going to add this argument
here
oops i'm going to add this argument here
that is as index equals false you'll see
why in a second
and then like i did before i'm going to
say grab bill
length in millimeters
and then
dot mean
um so when i run this let's actually
print it out penguin bill
you can see that it gives us a data
frame where it has the species of the
penguin and then it has the average bill
length which we grabbed using dot mean
all right so now that we have this data
frame we can use that instead of the raw
data to make our plot so we can say gg
plot penguin
aes x equals species
y equals bill
length m m
uh and then let's fill by species as
well like last time
and then we're going to say plus geom
bar because we want to have a bar plot
now the only thing we need to do here
unlike some of the bar plots that we
made above
is let's scroll back down
um inside gm bar there's an argument
called stat and we're going to set it
equal to the string identity
okay so this is basically saying the
height of the bar should be the actual
bill length by default it will like
count the number of observations but we
want
the actual bill length to be the height
of the bar
beautiful we have a similar bar chart
that tells us the average bill length
for the different species and again we
can add things like plus theme
minimal
plus labs x equals species
y equals average
in millimeters um so we can make the
graph a little prettier if we would like
to but this is another way to make a
similar bar chart
so let's do another practice with that
so i'm going to make this data frame
book df which basically has different
books and my ratings of them
now let's say i want to make a bar chart
with the individual books on the x-axis
and then the height of the bar is going
to be its rating well we know i can do
that with ggplot
uh
book
df
aes x equals
books which is going to be the books
column y equals ratings and then of
course i'm going to add plus geom
and then stat equals identity
you can see that i have all of the
different readings
listed with the height of the bar being
the rating so for instance let's see in
this data set the highest rating is home
before dark and you can see indeed the
home before dark bar is the highest bar
and if i want i can also do things to
make this prettier for instance i can
say aes actually we'll put that first
inside bar i can say aes
fill equals
books
and this is going to fill each bar with
a different color so we can run that ooh
beautiful already
and then we can also add things just to
make our aesthetics a little bigger or
excuse me better
so we can see this is just a slightly
more clear version of the plot we had
before again all of these different like
kind of aesthetic changes will cover a
lot more in the next lecture but i
wanted to show you what beautiful graphs
we can make using ggplot
so last but not least let's pull up this
new data set this is going to be a data
set of different cereals in a grocery
store and you can see there's a bunch of
different information about the serials
and we're going to explore this data set
using visualization
so first let's make a bar chart looking
at the different manufacturers and how
many cereals each of them have in this
data set
so to do that i'm going to say ggplot
serial because that's where we're
getting our data from
i'm going to say aes x equals
manufacturer
and then i'm going to say plus g on bar
and we're going to make a bar part uh
bar charts excuse me uh and we're just
going to run this
and you can see how many
serials are in the data set for each of
these manufacturers for instance these
big two i think k is probably kellogg's
and g is probably general mills have the
most serials in our data set again we
can make things a little prettier so for
instance we can do
um
bill equals
and we can add a theme we've so far been
using theme minimal but there's also one
called theme bw black and white um so we
can run that and we get the same plot
just a little bit prettier um another
thing that we can do let's say ggplot
cereal
uh let's see i want to make a scatter
uh of the different amounts of protein
versus sugar in the cereals maybe i'm
trying to choose something that works
best for my diet so i'm going to say x
equals protein
y equals sugars
sugars
there we go and again i'm just getting
these from the names of the columns
and then i'm going to say plus gm point
and let's add a
theme bw because i want it to be you
know not super distracting so we can
make a plot like this that shows us the
relationship between protein and sugar
all right so it looks like uh there's
not a huge relationship maybe a slightly
negative relationship where the higher
sugar cereals tend to not have a ton of
protein but there's a lot of difference
but let's see if that relationship is
different between different
manufacturers so we can add our little
lego block of facet
wrap
and we're going to say the little
squiggly tilde by manufacturer so this
is basically going to say make this
exact plot but make it once per
manufacturer separately so let's go
ahead and run
okay so you can see for our different
manufacturers the relationship looks
pretty much the same but like
interestingly for instance whatever
manufacturer p is they don't really have
any high protein cereals at all they
just have very sugary cereals um and
then for instance general mills tends to
have pretty low protein but there is one
high protein one so maybe there's like a
health
cereal or something like that but we can
tell that most of them have the same
relationship that we saw in the overall
graph and we can make all different
types of plots too so for instance let's
look at ggplot serials
there we go and we're saying aes we'll
say x equals manufacture
and y equals sugars
and let's make a box plot
we'll add a theme minimal here
all right oops
misspelled cereals let's make sure i do
that oh there's no s okay
so when we run this
you can see the distribution of
different sugar amounts for each of the
cereals for each of the manufacturers so
for instance uh kellogg's has a huge
range of sugars which makes sense
because i think uh they have like a
healthier kind of line of cereal so
maybe those are some of the low sugar
ones um but you know they have some high
sugar like uh you know frosted flakes i
believe is kellogg's um whatever this n
brand is tends to have very low sugar uh
general mills tends to be
moderate sugar but a little bit less
spread so they don't have the really low
sugar options but they also don't have
the really high sugar options
so as you can see we can use plots to
really explore our data and learn a lot
about it alright that is all i have for
you i will see you guys next time

 
hello and welcome to your logistic
regression 2 lecture
so last time when we talked about
logistic regression we talked about how
it's just linear regression in disguise
and we did a bunch of algebra to get
from probabilities to log odds which
were better for our linear model to
predict today i want to talk a little
bit more about those three things
probabilities odds and log odds a little
bit more about their scales and then how
that affects the interpretation of our
model first let's talk about those three
things so hopefully you're pretty
familiar with probabilities we know that
a probability of zero means that
something will never happen and a
probability of one means it's guaranteed
to happen
an important value to remember in
probabilities is 0.5
that is the threshold of when an event
goes from more likely to happen than not
to less likely to happen than not for
instance anything above 0.5 means that
that event is more likely to happen than
not and anything below 0.5 means that
that event is less likely to happen than
not for odds we have a different range
odds unlike probabilities range from
zero all the way to positive infinity
when an event has a zero odds it means
it's not going to happen it will never
happen
as it approaches infinity it's getting
closer and closer to it definitely will
happen the important threshold value to
remember for odds is 1. when something
has an odds greater than 1 it means that
it is more likely to happen than not
when something has odds less than 1 it
means it is more likely that it won't
happen than that it will and finally log
odds
log odds ranges from negative infinity
to positive infinity with negative
infinity being
things that will never happen and
positive infinity being things that will
definitely happen
with log odds the important threshold to
remember is zero
things with a log odds of above zero are
more likely to happen than not
things with the log odds below
zero are more likely to not happen to
review probability has a range of zero
to one point five is an important
threshold above which things are more
likely to happen than not
for odds the range is 0 to infinity with
1 being the important threshold anything
with an odds above 1 is more likely to
happen than not and finally log odds log
odds ranges from negative infinity to
positive infinity and that important
threshold is zero things with log odds
above zero are more likely to happen
than not
so what does this have to do with you
well let's talk a little bit about the
coefficients of a logistic regression
model
the first thing i want to show you is
the intercept of the model remember just
like a linear regression a logistic
regression has an intercept and then a
bunch of slopes associated with
different predictors
the intercept of a logistic regression
is very interesting as you can see from
these curves on your screen
each of these curves is a logistic curve
with a slope of 5 and some different
intercept which you can see at the
bottom of the screen
as you can see when you increase the
intercept the logistic curve moves to
the left which means that it takes a
lower value of our predictor x to get a
classification of positive or 1. you can
see here that x only needs to be about
negative 0.5 in order for us to start
classifying it as positive whereas when
we have a negative intercept you can see
that it's a lot higher this has about
this would require about an x value of
0.5 before we started classifying things
as a positive case more interesting
let's look at different slopes now
remember slopes are the coefficients in
our model and those are the things that
we're most often interested in
interpreting
here you can see a bunch of different
slopes first let's look at the positive
slopes which are these three right here
you can see that as we increase the
slope our logistic curve gets steeper
which means that smaller changes in our
predictor produce larger changes in our
predicted probability
predictors that have very strong effects
will have large slopes because that
means that a small change in that
predictor has a big change in our
outcome and notice that with these three
positive curves the higher x is the
higher the probability is of something
happening
now let's look at our negative curves
which are these three right here
you can see that just like with the
positive curves the negative curves the
greater the magnitude of the coefficient
in other words the further it is from
zero the steeper it is again the larger
the magnitude of the coefficient the
more a change in the predictor has an
effect on the predicted outcome
what you might notice though is that
these are moving in the opposite
direction they basically have the
opposite of an s-shaped curve which
makes sense when we have a negative
coefficient it means that increasing the
predictor means that we're actually
decreasing the probability of something
being classified as one
to review the larger the magnitude of
the coefficient so the further it is
from zero the steeper the logistic curve
is meaning smaller changes in our
predictors produce big changes in our
positive coefficients mean as the
predictor increases so does the
probability of it being in class one
negative coefficients mean that as the
predictor increases the probability of
being in class one actually decreases
now when we get coefficients from a
logistic regression model
they are by default in log odds terms
because remember our model is actually
using linear regression to predict the
log odds of an event so the coefficients
that we get directly from the model are
in terms of log odds
now just like we talked about before
with the threshold that means that any
predictor that has a positive
coefficient is going to be associated
with an increase in the probability of
having that data point be in class one
anything with a negative coefficient is
going to be inversely related or
negatively related meaning that as that
being in class 1 decreases
when we grab our coefficients it's often
good to just quickly scan for positives
and negatives just to see what the
relationship is between our variables
direct or inverse here's an example of
some coefficients from a model here
we're predicting whether or not a
customer will upgrade to a premium
version of a service based on their age
their income and thousands of dollars
and the number of months they've been
subscribed to the service
quickly scanning we can see that income
actually seems to have a negative
relationship with whether or not they're
going to upgrade meaning the higher your
income the less likely it is that you
will upgrade to the premium service on
the other hand age and month sub have
positive relationships with the outcome
because they have positive log odds
coefficients this means that as age or
month sub goes up the probability that
they will upgrade to the premium
subscription also goes up and we can
actually write out an interpretation for
these coefficients just like we did for
linear regression
basically we say
as age increases by one unit and again
you want to specify whether that's one
standard deviation for a z-score or just
one unit if it's not z-scored but again
as our predictor increases by one unit
our predicted log odds increase by
0.1445 so to put that together as a full
sentence i would say as age increases by
one standard deviation because i z
scored
the predicted log odds increased by
0.1445
again we can do that for income i would
say as income increases by one standard
deviation
the predicted log odds of upgrading to
the premium subscription go down by
0.0066
last but not least i could say as month
subscribed increases by one standard
deviation the predicted log odds
increases by 0.0015
but log odds is kind of weird i mean who
really understands what it means
intuitively i definitely don't and
pretty much no one does
and so even though we can give those
interpretations and we can obviously
scan for inverse versus
direct relationships in terms of the
signs of the coefficients it can
actually be more useful to use that
algebra we talked about in the last
theoretical logistic regression lecture
and convert our coefficients from ah
from log odds to odds to do that we
exponentiate all of the coefficients
remember to go from odds to log odds we
use the natural log and the opposite of
a natural log is just to take e
to that number
so we can use a computer to get that
conversion for us and now we have a list
of coefficients that are in terms of
odds instead of log odds and while log
odds are a little bit confusing odds are
a lot more intuitive to people however
it does change the interpretation of our
coefficients because now instead of
being the raw coefficients from the
model we've exponentiated them to get
the odds coefficients however we can
still interpret them fairly easily
let's start with age the formula you're
going to use is actually starts off the
same as our previous linear regression
or our log odds coefficient
interpretation we're going to say as our
predictor age increases by one unit
again standard deviation of its z-score
raw unit if it's not
so as age increases by one standard
the predicted odds are multiplied by
1.155
etc
now notice that difference instead of
saying it increases by instead of adding
we are now multiplying so whatever the
odds were before
we are going to multiply them by
1.115 so let me put that all together as
age increases by one standard deviation
the predicted odds of upgrading are
multiplied by 1.15
in other words there's about a 15.5
percent increase in the odds that
someone will upgrade for every one
standard deviation of age that they gain
all right let's try that for income
for income as income increases by one
standard deviation the predicted odds
are multiplied by 0.99
notice that anytime we multiply
something by less than 1 it's going to
go down so when we multiply the odds by
0.99 it will actually decrease those
odds last but not least as month
multiplied by 1.0015
with odds any coefficient above 1 is
going to have a positive relationship
with the outcome
anything with odds below 1 is going to
have a negative relationship as it goes
up the predicted odds are going to go
down
now one thing to be really careful about
with odds coefficients is when you're
looking for the largest effect
with linear regression or even with log
odds coefficients it's pretty easy to
tell which predictors have the
biggest effect you just look for the
largest in magnitude coefficient right
whether it's negative or positive
whichever one has the largest
coefficient is going to have the biggest
impact on the outcome but remember odds
are on a slightly different scale
in logistic regression with the log odds
coefficients and in linear regression
that threshold value we're looking at is
zero things that are further away from
zero are going to be bigger effects for
odds coefficients the threshold value is
one and we're looking for things that
are going to be
further away from one but we have to be
careful for instance think about an
effect that doubles the odds
that's going to have an odds coefficient
of 2.
something that halves our odds which is
a similar magnitude of effect is going
to have an odds coefficient of 0.5
now 2 is technically further away from
one however those both have pretty
similar effects one doubles the other
halves so when you're looking for the
biggest effect and you're using odds
coefficients just make sure that you
keep that in mind so at this point you
might be wondering okay log odds are
very confusing they don't make sense so
it's sometimes better to interpret our
coefficients in terms of odds which at
least people have somewhat of an
intuition behind well why not
probability everyone knows probability
it makes so much more sense let's just
use probability
well there's a reason why we don't do
that
and it's represented in this graph here
when we look at log odds coefficients
there's a consistent
constant relationship between increasing
the predictor and what happens to the
predicted log odds
every time we increase our predictor by
one unit our log odds increase by
whatever the coefficient is in other
words every time we increase our
predictor by 1 we add the coefficient
value of the log odds coefficient this
results in a constant relationship which
you can see with this straight line
for every increase in one unit we're
changing the log odds by adding by the
same amount with odds the relationship
is a little different but it's still
constant with odds instead of adding we
multiply by the same number
so for every one unit increase in our
predictor we multiply by the same number
in other words we multiply by the odds
coefficient so as we move along this
curve we are multiplying by the same
number so even though it's
multiplicative it is a constant
relationship there is a single number
the odds coefficients that can represent
the relationship between increasing our
predictor and the predicted odds so log
odds and odds both have constant
relationships with our predictors for
log odds it's constant and additive we
add the same number each time our
predictor increases for odds it's
constant and multiplicative meaning we
multiply by the same number each time
for probability the relationship isn't
constant there is no one single number
like the odds coefficient or the log
odds coefficient that defines the
relationship between the predictor and
the predicted probability you can see
that by that logistic sigmoid curve that
we've seen for this model
when we're at this range of the model
there are pretty non-steep increases in
the predicted probability as x increases
when we get to the middle of the curve
we actually have very steep almost
linear increases in the predicted
probability as x increases by one unit
but then it plateaus at the end here
meaning that we actually have very tiny
increases in x all this to say there's
no single number that can represent the
increase in probability that goes with
increasing our predictor by one on the
extreme ends of the curves the changes
are actually quite subtle and small
and in the middle of the curve the
changes are quite steep
because there's not a single number to
represent the relationship between
predicted probability and our predictors
we actually don't use any probability
coefficients because there isn't one so
even though probability is incredibly
easy to explain to people and something
that they intuitively understand even
more so than odds we don't use
probability to describe the
relationships between our predictors and
the outcome because it's not a constant
relationship
log odds and odds on the other hand are
constant relationships there is a single
number that we can either add in the
case of log odds or multiply by in the
case of odds that represents the
the predicted log odds or odds this is
why you'll often see us interpreting and
using the coefficients for log odds and
odds but not for probability so just to
summarize log odds and odds both have
constant relationships so we use them to
interpret our models log odds are a bit
tough to understand though so often
especially for presenting to other
people we will use odds coefficients
because people have a little more
experience with odds than they do with
log odds even though probabilities are
really simple and something that almost
everyone understands because
probabilities do not have a constant
relationship we do not use them as a way
to interpret the model now you might
notice that there's a little asterisk by
probabilities on the screen and that is
because there are technically some cases
in which it is okay to use probabilities
usually it has to do with cases where
you have only binary predictors in your
model but it's a rare enough case that i
don't really want you to count on that
so for our class we are not going to use
probabilities as a way to interpret our
model instead we'll stick with odds and
log odds coefficients all right that is
all i have for you i will see you next
time

 
hello and welcome to the first part of
your data visualization lecture
so this part is going to be pretty short
and then we'll move over in a separate
video to talk about actually how to do
data visualization
in python so the reason why i'm
spreading out the data visualization
lectures across a couple courses and
or classes instead of just doing it all
in one go
is because i think there's a lot more to
than teaching you how to make a box plot
or make a scatter plot effective data
visualization means you have to think
about
how to clearly communicate a message
using your data visualization and that's
not something that stays the same
if you have a different message the way
that you communicate that
is probably going to be different it's
not
the truth to say that every time you
have a continuous variable you should
make a box plot
or every time you have a categorical
variable you should do a bar plot
it really depends on what the message is
that you're sending and it's our job as
people who make data visualizations
to know the message that we're trying to
send and then to recognize
what are the elements of my data
visualization that support the message
that i'm trying to send
and what are the elements of my data
visualization that detract
from the message i'm trying to send and
depending on the message
those things are going to be different
every single time that you make a data
visualization
i can't give you hard and fast rules
that will work for any situation
so what i want to do is teach you how to
ask the correct
questions in order to be an effective
data communicator
using data visualizations when we're
thinking about
what we're trying to communicate we also
need to think about
who the audience is that we are
communicating to
for instance a data visualization might
look really different if you're
presenting to
a group of experts who know so much
about the topic
that you are presenting on it might look
very different from if you were trying
to make a data visualization to go
in the new york times where people with
all levels of knowledge and experience
would read it
we need to be really cognizant of who
the audience is
when we're making a data visualization
because if we're truly thinking about
the message and how we're communicating
that
the audience plays a huge role in them
and finally i really really am
passionate
about talking about accessibility
when it comes to data visualization and
this comes in so many forms more forms
than i could
possibly list out here but some things
that we'll specifically talk about are
colorblindness if you're not colorblind
you probably don't think a lot about
what colors you're using
and how that appears to people who do
have some type of color blindness
but we'll talk about that we'll talk
about questions to ask yourself in order
to help make graphs
accessible and easily understandable by
people who are colorblind
or people who have visual impairments
often the default graphs that you build
or that are built for you
in python or r or whatever you're using
are
not built with people with visual
impairment
in mind and that's something that you
want to think about
is could someone with a certain type of
visual impairment
still appreciate the message that i'm
trying to communicate
similarly we have to think about again
our audience
what is the cultural context for the
people who are going to be consuming
this message
are they people from a different culture
than you if so
you need to make sure that you're not
making any assumptions or
any references that someone outside of
your culture is going to
not understand accessibility basically
means
we have to think about people who aren't
like us we have to
make our data visualization inclusive so
that people of
all different visual ability cultures or
any other difference that you could
possibly list
are still going to be able to appreciate
the message that we're trying to
communicate
the data visualization package that
we're going to use is called plot9
and if you've ever worked with r you
might recognize a lot of what we're
going to do
because plot 9 is essentially a port of
ggplot
and this is a really beautiful way to
make graphs which we'll cover in the
next video which will go over the actual
python of creating graphs
but i'm really partial to this and i
think it really lends itself
to being a thoughtful data visualizer
and so that's why we're going to use
this so we should have installed this
before but if you're having problems
now is the time to make sure that you
have installed plot 9
and that it is running on your computer
all right i will see you for the coding
part of this lecture in the next video

 
hello and welcome to your logistic
regression in python lecture so the
first thing we're going to do as always
is run our import and as that's running
the next thing that we're going to do is
we're going to load in some data now
this data is looking at whether or not
customers upgraded to a premium version
of a service based on their age their
income in thousands and their months
subscribed now the first thing that i
did was i put all of the predictors the
names of those columns into a list so
that i can use it later
then we're going to grab our predictors
and store them in the variable x and our
predicted value or our outcome and store
it in y then we're going to do an 80 20
train test split now remember the first
thing we put in a train test split is
the predictors and then the second thing
is the outcome the thing that we're
predicting and finally we tell it how
big our test set is going to be for an
80 20 train test split we're going to
use 0.2 because that's the proportion of
our data that we would like to be in the
test set now that we'll know what's in
our train and our test set we can
actually start setting up for our model
building including z scoring
to z-score remember i'm first going to
create an empty z-score object standard
scalar
next we need to actually fit this so i'm
going to say z dot fit and remember the
test set should never see the inside of
a dot fit function so i'm only going to
fit on the training set
so because all of my variables are
things that i want to z-score i'm just
going to give an x-train instead of
specifying which columns that i would
like to z-score and then i'm going to
transform so i'm going to say x train
square bracket predictors
equals z dot transform
x train
and then i'm just going to copy and
paste and do the same thing for the test
set so i'm going to say x test
and then x
test
so now we have our data we've split it
into train and test we've gotten our x's
our predictors and our outcome
so now we can actually set up for our
model building the first thing that i'm
going to do is i'm going to create an
empty model i'm going to call it my
logit if i can spell it and i'm going to
set it equal to logistic regression
which is just the name
of the logistic regression function in
sklearn
and i'm going to go ahead and run that
and that's just going to be an empty
model
now that i have an empty model i need to
fit it using my data so i'm going to say
my logit
dot fit and again test set should never
see the inside of a dot fit so we're
going to say x train
y train no test set in sight
so i'm going to run that and we will now
get a fitted model
then i am going to use predict in order
to get our predicted value so i'm going
to say predicted vowels equals
my logit the name of our model dot
predict and then what i want it to
predict for in this case let's do the
test set so i'm going to say x test
and remember
there we go um that our logistic
regression actually also will output
predicted probabilities for us so i'm
going to grab those as well i'm going to
say predicted
probs
equals my logit dot
predict
prava so for any of our algorithms or
classification algorithms you can either
use predict to get the actual category
or predict prabha to get the probability
of that category so i'm going to do that
x test
okay now that i have those let's go
ahead and run
so now we need to assess our model
performance the first thing i'm going to
do is plot my confusion matrix i can do
that using plot underscore confusion
underscore matrix and this function
takes in three things it first takes in
the model that we use so my logit then
it takes in our x values and our y
values so i'm going to use the confusion
matrix for the test set you could also
do it for the train set
so we say x test y test and when we run
this
it will output our confusion matrix for
us so the confusion matrix gives us a
good overall story but we can also
assess some of the other metrics that we
talked about in the lecture
so first let's do the most basic i'm
going to say print accuracy
and then i'm going to say i'm going to
use the accuracy score function so the
accuracy
score function
is a function that calculates an
accuracy score and it takes in two
arguments it takes in actual values and
predicted values so our actual values
let's first do this for the test set our
actual values are stored in y test
these are the actual outcomes
our predicted outcomes are stored in
predicted vowels so i'm going to say
predicted
vowels
now accuracy isn't the only metric that
we've learned so let's do a couple more
first i can do the f1 score
for unscore
to calculate that we're going to use the
f1 score function and just like accuracy
it takes actual comma predicted so we're
going to say y test comma y or a
valve
okay
next let's do the recall so we're going
to say print
recall
and all of these print statements are
just so it's easier to be organized and
then we're going to do recall
score
and again just like the others it takes
in the actual comma predicted so we're
going to say y test predicted valve
and last but not least let's do the
precision so we'll say print
precision
and we'll say precision
actual y test comma predicted
perfect so now we have all of these
different metrics for our test set let's
also do this for our training set so i'm
going to go ahead and i'm going to copy
these here but i'm going to change some
things because instead of the test set i
want this to work with the training set
so first of all for all of these
functions instead of y test the actual
values are not now stored in y trains
i'm going to go ahead and change all of
those
and then we need the predicted values
for the training set now we could create
a new variable and store them there i'm
actually going to calculate them
directly here so i'm going to say my
logit.predict
x
train
and this is going to give us the
predicted values for the training set so
i'm going to copy and paste that here
and replace all of the predicted vowels
with that all right let's go ahead and
run our metrics
so we run test
and train
all right so we can see that the metrics
are pretty similar for accuracy there's
a one percent difference so i'm not
really worried about overfitting in this
model both of them are doing
mediocre about a 60 accuracy the f1
score is similar here as well about 70.
recall for both of them is about 85
percent and precision is
about 60.
remember that accuracy is how many
guesses our model got correct
recall is how often our model is correct
for positive or ones
and precision is out of all the things
that we guessed were positive how many
were actually positive and then f1
scores are not as intuitive but they're
a combination of precision and recall so
by all accounts our model is doing okay
it might not be anything to write home
about but it's definitely better than
flipping a coin
next now that we have all these
individual metrics let's look at an roc
auc
and actually plot the curve so i'm going
to say plot roc
curve and this is going to take three
things like our plot confusion matrix
it's going to take the model my logit
it's going to take our x value so let's
do the test set first
so x test and y test and then we're
going to go ahead and run this
and then we can do the same for the
training sets we'll say plot our c curve
my logit so the model x train and y
so similar to the story that the other
metrics told us these are okay models
they're not great remember a straight
diagonal line from corner to corner
would be a random model basically
flipping a coin so our models are doing
better than that they're closer to that
upper left hand corner
but they're not
stellar right they're not hugging this
upper left hand corner like a really
good model would which means that our
models might be having a little bit of a
tough time distinguishing between
positive and negative cases at least
that's what both the curve as well as
the overall auc values are telling us
remember an area under the curve of
about .5 is a random model and one would
be
perfect model so ours is doing a little
bit better but not a ton better than a
random model
all right to better understand the roc
auc curves let's make a couple of fun
plots so i'm going to make a data frame
i'm going to call it overlap plot
df
and i am going to say pd.data
frame and i'm going to have two columns
first is going to be the predicted
probabilities from our model and the
second column is going to be the actual
categories that our data points are in
so i'm going to say prabha
and i'm going to say predicted probs
those are the predicted probabilities
for the test set that we calculated
above
and then the second column is going to
be outcome and it's just going to be the
actual category so why test in this case
now predicted probabilities actually let
me show this to you really quickly
predicted probabilities gives us the
probabilities for both classes so you
can see here for every data point we
have the probability of it being in
class 0 and the probability of it being
in class 1. well we actually don't need
both of those because this is just 1
minus this so i'm going to go ahead and
just grab the one column so in other
words we're going to grab the
probability of it being in the category
1.
the way i'm going to do that is by
saying i want all the rows comma i only
want the column at index 1.
all right so now we have overlap
plot df
now we have our data frame and we're
going to use it to make some graphs
now i won't bother going over the ggplot
code with you but the first graph we're
going to make is going to look at the
predicted probability by the different
categories so you can see here that
every single data point has a predicted
probability the ones that are actually
in category 1 are blue and the ones that
are actually in category red or excuse
me category 0 are red
now when we look at this there's a lot
of overlap but you can kind of tell why
maybe our model isn't doing so well and
specifically why it has a very poor auc
value
you can see that there's a lot of mixing
of the red and blue it would be kind of
hard to decide on a threshold here
pretty much anywhere you put it you're
gonna have a lot of misclassifications
another plot that we can look at
is a distribution graph
so here we have basically the same
information on the x-axis you can see
the predicted probabilities but instead
of dots we actually have distributions
with blue on the right being the
positive cases and red being the
negative ones
now notice this red line which is
basically acting like our threshold
because these two distributions overlap
so much there isn't really a good place
that you could put this threshold where
you'd classify most of the positives as
positives and most of the negatives as
negatives so we can kind of see why our
model is having a hard time and
specifically we can connect this to our
auc our roc curve and the area under the
auc of that curve
as we move the threshold you can see
that while moving it the threshold to
the left might better classify some of
the positives it gives us more false
positives and even if we move the
threshold over here we're still going to
do pretty poorly because even though
we're classifying most of the positives
correctly we're now mostly
incorrectly classifying the negative
cases as positives and you can see from
this graph if you kind of imagine moving
that threshold that that's what's
creating this pretty poor
roc curve and auc value
now while our model is doing pretty
poorly here's a theoretical one that
would actually do pretty well
here even though there's some overlap
and again this is a different data set
than the one we're working with even
though there's some overlap you can
clearly tell where a good threshold
might go in order to have a pretty high
accuracy in classifying positive and
negative cases for instance i might put
it approximately here in the middle when
we put it here we're getting a lot of
true positives and we're avoiding most
but not all of our false positives
this is in very stark contrast to this
graph where basically the distributions
are almost on top of each other
and again this is going to lead to a
pretty poor performance and poor looking
roc curve where something like this
would probably have a better performance
and a better roc curve and now that
we've built our model we can go ahead
and use it to predict future data
remember when we're actually building
our models we're usually going to be
building them in cases where we want to
use them in the future to predict new
data so let's do just that i happen to
have future data for our upgraded
subscription data set and i'm loading it
in into the variable fashion new
now let's go ahead and grab the
predictors for this new data set so i'm
going to call it x new i'm going to say
fashion
new
actually before we do this let's just
show you fashion
so you can see that this is just the
same data frame as before just new
future data okay so we're going to grab
our predictive values which again are
age income and months subbed
but these data are in raw form so we
need to z score so i'm going to say x
new square bracket predictors
equals
z
dot
transform
x new so with our new data when we get
new data to use in our model we're not
going to refit our entire model we are
going to use what we've already fit in
this case we're using our already fitted
z-score object in order to transform our
new data
all right and so once we do this we then
have our data ready to go into the model
so let's grab some predicted values so
i'm going to say y pred new equals my
logit
dot predict
x new
and then let's look at the accuracy so
i'm going to say print
and then i'm going to use accuracy
score the actual values which are going
to be fashion
new square bracket up grade and the
predicted values which are going to be
in y pred new
so when i go ahead and print this out
didn't run this all right let's run this
my logit okay
[Music]
all right so let's run this
and then let's run this
so we can see that on the new data
actually kind of similar to our old data
this has about a 60
accuracy and we could also grab any
other metric or confusion matrix that we
wanted for this new data
so let's go ahead and do the confusion
matrix i'm going to say plot
confusion
matrix and again that's going to take
our model my logit it's going to take
our x values which is now x new and it's
going to take our y values which is
new square bracket
up
grade
beautiful and now we have a confusion
matrix for our new future data
all of this to say that usually we're
building our models to predict future
data so if we had future data this is
what we would do we wouldn't refit our
models we'd use our existing model to
make predictions about our new data
so in the previous model that we looked
at we used a train test split but like
with linear regression when we learned
about leave when out or k fold we can
also use those for any of our models
including logistic regression so let's
go ahead and do that i'm going to use
the same data set and i am just
specifying x and y
and then we need to specify our k fold
object so i'm going to say k
fold oops nope there we go k fold
actually i'm going to say kf i need to
store that equals k fold
and i'm gonna do n
splits equals five
uh then i'm gonna create an empty model
i'm gonna call it lr this time and set
it equal to logistic
regression
there we go okay so empty model
and now because we're doing k fold and
this also is how leave went out works we
are running multiple models instead of
just a single model with our train test
split we are going to be building five
models because we're doing five-fold
cross-validation so we're going to need
to collect the different metrics for all
of our models to do that i'm going to
have a list to store the say accuracy
and let's say the roc auc for every
single model
so i'm going to say ack
train equals empty list
ack test equals empty list and then
let's also grab the roc auc so the under
the area under the curve for the roc
graph um we can actually just calculate
this as a single value instead of
plotting out the entire roc curve so i'm
going to say roc train equals empty roc
test equals empty
and we can grab all or any of the
metrics that we've learned about in this
case i'm going to go ahead and just grab
these two all right let's go ahead and
run this
and now we get to our k-fold for loop so
as always we're saying for the train
indices comma the test indices in
kf that's our k-fold object dot split
and we're going to give it our x-values
and now inside this for loop we are
going to set our training set and our
testing set for both x and y
now remember until we get inside this
for loop we don't know what's in our
training in our test set which is why we
didn't do something like z-score outside
of the k-fold for loop
but now that i know what's in my train
in my test set i'm going to go ahead and
z-score first i'm going to say z equals
standard scalar
and then i need to fit and transform and
actually normally we fit in transform
separately for the training set but we
can do it in one step so i'm going to do
that and i'm going to say x train square
bracket predictors
z dot fit transform
x train so for the training set this is
basically doing both the fit and the
transform step in one now with x test we
do not want to refit so we're going to
say x
tests excuse me
predictors there you go
equals z dot transform x test notice we
are not refitting
all right now that i've z scored i can
fit my model and i've already created an
empty model lr so i'm going to use that
i'm going to say lr.fit
x train y train again test that never
should see the inside of a fit function
and then we need to record our
performance metric so we want the
training accuracy the testing accuracy
the training uh roc auc and the testing
roc auc first so i'm gonna say ack train
dot append and then i'm gonna use
accuracy accuracy
and remember this takes actual comma
predicted so for the training set that's
going to be y train and then i'm going
to grab the predictions directly from
the model so i'm going to say lr.predict
and then not to reinvent the wheel i'm
going to copy and paste this for the
test set but i'm going to now append to
act test
use y test and x test
okay and now let's do the same for the
roc auc values now these are a little
bit different starts at the same i'm
going to say roc train dot append
roc
and this takes a couple different
arguments so it's going to take the
actual value so we'll say y train but
now it's going to take the predicted
probabilities instead of the actual
predictions of zeros and ones so we can
grab that from our model using predict
prabha
and then we are going to give it x train
now remember that predict prabha gives
us both the probability of being in the
class 0 and the class 1
we only want the probability that it's
in the class 1. so we're going to grab
that by saying give us all the rows
comma give us just the column at index
one
and then i'm going to go ahead and i'm
going to copy and paste this and do it
again for the test set so we'll say roc
uh y
and x
all right so now we should have all of
the metrics that we want and again our
model should run five times once for
each of the five folds
so when this is over we should have a
list of five train accuracies five test
accuracies and same for the roc auc
values
now i'm gonna want to see those so let's
print them out so first i'm gonna print
out just the list of values and then i'm
gonna print out the average because
sometimes we want to see both the
individual values for the model as well
as the actual average across all of our
models
so let's start we'll say print ack train
and print np dot mean
ack train so the first one is going to
print out the whole list so all of the
accuracies and the second one is going
to print out the average
then we'll do the same for the test so
print ack
test print np dot mean ack test and same
for the roc so we'll say print
uh
print np dot mean roc
and then print roc test
print np dot mean
all right so now we are ready to run all
of our models we'll go ahead and run
and you can see all of the values
printed out
so some of these are a little long in
the list so i'm going to use the
np.round function in order to just round
to a couple decimal places so mp.round
say
2
oops
and mp.round
two
all right let's go ahead and rerun that
okay it just looks a little bit prettier
all right so you can see for instance
here are our training accuracies oh i
didn't rerun this all right let's go
ahead and rerun this because we didn't
empty our list first and then run
okay now these only have our five values
so let's look at the uh training
accuracy so we can see it ranges a
little bit from a low end of about of
0.591
to about 0.6 on average we have an
accuracy of about 59
for our testing accuracy we again see a
range of about
0.565 to about 0.6
so again about a 59 accuracy and i'm
again not worried about overfitting here
for one there's not a huge difference
between the average training and test
accuracies but even here you can see
that the variability within the
accuracies is a lot bigger than the
variability between the training and
test accuracy so it's normal to see some
differences and it doesn't look like
there's anything overfitting going on
for the roc a you see you can see here
they're very similar about
61
area under the curve and you can see
that those values range for both the
training and the testing set all right
that is all that i have for you i will
see you next time

 
hello and welcome to your all the stuff
you need to know math lecture so before
we start i just want to say everything's
fine you're going to be fine i know math
can be really anxiety producing or
stressful for some of you we're just
going to review some key math ideas that
you're going to need to do well in this
class and i'll let you know which ones
you'll actually need to know how to like
do and calculate and which ones are just
ideas that you're going to need to use
later on in the class the first topic i
want to review are logarithms and
logarithms are just the opposite of
exponents so what i mean by that is when
you have log
base b
of n
equals x that's just kind of a fancy way
of writing b
to the x power equals n
and you can see where we have the same
variables in both of these equations so
we have n
n
we have
x
b
b so logarithms basically answer the
question of what power do i have to take
b to in order to get n
and one of the useful things about
logarithms is that it takes
positive numbers in so those ends can be
any positive number and it actually
transforms them to take up the entire
number line negative to positive
so you can see on this graph of log base
2
that any number between 0 and 1 will
give us a negative number and that's the
true no matter what the base of your
logarithm is if you plug in log of some
number between 0 and 1 you're going to
get a negative value
any time you plug in a number greater
than 1 you're going to get a positive
value for that log and that's because
we've just defined for logarithms that
any number to the power of zero is equal
to one one thing that we love about
logarithms is that they help with
multiplication now multiplication is
just doing addition a bunch of times in
a row but it can be a little bit
computationally expensive or difficult
to do
with logarithms because of the way that
we work if we take the log
of two numbers multiplied together let's
say x times y
that's actually equal to the log
of x
plus
the log
of y
so we've turned on the left hand side a
multiplication problem which might be
kind of difficult into a very simple
addition problem
which brings me to a very silly and a
very popular joke about logarithms so
noah is getting off his arc and he's
checking on all of his animals seeing if
they're able to reproduce and are living
well
and he checks on all the animals and all
of them are doing well with the
exception of a pair of snakes
so noah goes up to them and says hey
like what's going on why aren't you
reproducing is there something wrong
and the snakes say oh well could you
just chop down some trees for us and let
us live in what remains of them and noah
goes okay and does so
then a couple of weeks later noah comes
back and everything's great the snakes
are reproducing and they look great so
we asked them oh
why did having all of those chopped down
trees help you reproduce and the snakes
say well we're adders we need logs to
multiply
so one last thing i want you to know
about logarithms so remember when i said
that logarithms are just the opposite of
exponents so if we want to undo a
logarithm we just take the base to the
power of that logarithm so for instance
there's a logarithm called the natural
logarithm which hits bases e
so we say if there's a natural log of x
if we want to undo this logarithm we're
just going to say the base e to the
power of the logarithm ln of x and that
equals x
similarly if we have log base 10
then we can say 10 the base to the power
of log base 10 of x
that also equals x
so again
logarithms exponents opposite so when
you apply them together you're undoing
the logarithm
next let's talk about derivatives
derivatives are an instantaneous rate of
change basically they answer the
question how is this function changing
at a particular point in that function
so for instance here we could look at a
derivative that tells us at this point
right here
how is the function changing is it
increasing is it decreasing or is it not
doing anything at all what's important
to remember about derivatives is that
when they're positive it means a
function is increasing
when they're negative it means the
function is decreasing and when they're
zero it means that the function is flat
at that point and that's really useful
to us because flat points in a function
are often minima or maxima and a lot of
times in data science we want to find
the min or the max of some function
next let's talk about integrals
integrals are the opposite of
derivatives and the way we're going to
use them in data science is to find the
area under different curves so for
instance if we have this very weird
shaped green probability distribution we
might be interested in finding out what
is the probability that a value will
land between these two boundaries
well the way that we calculate that of
saying what is the likelihood that a
variable will be in this dark green area
is by using integration
so again both with probabilities or
general areas under a curve we're going
to need integration in order to do that
next i want to talk about the elbow
method which is oddly going to come up a
lot in this class
so let's do a really silly example first
say your friend is can get a little
annoyed with you and you don't want them
to be annoyed with you and you want to
decrease the amount of annoyance that
they have for you and one way of doing
that is by buying them
starbucks so we're going to replace this
here
and we're going to say number of
starbucks
and here
is going to be their annoyance with you
now you might be asking yourself as the
true data scientist that you are
how many starbucks do i need to buy my
friend before they're at an acceptable
level of annoyance with me
well let's assume that every time you
buy them starbucks it does decrease
their annoyance for you a little bit
so you can see in this blue line on the
graph let's say that this is their level
of annoyance and if you buy them one on
starbucks only in a month
they're going to be that level annoyed
with you if you buy them two they'll be
here three here for etc so you can
basically see as you buy them more
starbucks their level of annoyance with
you decreases but
there's diminishing returns
the first starbucks decreases their
annoyance with you a lot and is very
worth the steep price that starbuck
charges
now the second starbucks it does a lot
but
not as much and the third starbucks even
less than that and fourth and fifth and
they're all decreasing annoyance but not
that much anymore and at a certain point
we have what's called diminishing
returns
yeah it makes your friend a little less
annoyed with you but by less and less
each time you buy them starbucks
this graph could basically tell you when
should you stop buying them starbucks
and one way of deciding that is by using
the elbow method in this method we
basically look at the graph like it's an
arm and we try and find where the elbow
is in this case it's pretty obvious
it's right here
this is the point at which there are
diminishing returns the point at which
yeah you can buy them a starbucks it'll
make them a little less annoyed but it's
not really worth it anymore
and this elbow method is really useful
for selecting much more than the number
of times that you buy your friend
starbucks and it'll actually come up a
lot in this class so just remember when
we have a graph like this where we're
trying to decrease something
we can use the elbow method to figure
out
where are we getting diminishing returns
where is increasing the number of
starbucks orders not really doing much
for us at all
let's talk a little bit about matrices
and vectors matrices and vectors are
basically just different ways of
thinking about data structures that can
contain data for us for instance
matrices for example are a way that we
can think of just like data sets that we
would have for instance imagine an excel
spreadsheet that's basically what a
matrix is it has rows
and it has columns each of which
represent different things for instance
a row might be like a person and a
column might be their height there are
also special matrices like a correlation
matrix that come up a lot in data
science and a matrix is just a
convenient way to store those values
while a matrix has rows and columns a
vector has only rows or columns so for
instance here we have a column vector
because it's just a single column it
doesn't have two dimensions like a
matrix one thing we might want to do
with matrices and vectors is multiply
them so here we have a two by two matrix
it has two rows and two columns and then
a two by one vector because it has two
rows but only one column
now we know that we can multiply them
together because their inner values see
this on the right and this on the left
match that means these are compatible to
be multiplied together now in order to
do so what we're going to do is we're
going to take the column from this
vector
and the row from this matrix and we are
going to multiply each element together
so we're going to take 1 times 1 plus
2 times 3 and that's going to be 1 plus
6 equals 7.
next we're going to take that same
column from the vector but using this
row from the matrix and then we are
going to get
1 times 0 plus
3 times 1 and that's going to give you
3.
so the result of this multiplication is
going to be a vector of 7
and 3. now i promise you're not going to
have to do a lot of these by hand and
rarely without a formula or a friend to
help you but i just want you to
understand that that's how matrix
multiplication works the next thing i
want to review with you is eigen
decomposition which is just a fancy way
of saying calculate the eigenvectors and
the eigenvalues for a matrix
we can think of a matrix as just a way
to transform different points via matrix
multiplication like the one we just did
to give you a brief overview of why this
is so important eigen decomposition and
calculating those eigenvalues and
eigenvectors basically gives us
information about a matrix so sometimes
we want to know more about the matrix
that we're looking at and we can use the
eigenvectors and the eigenvalues to get
that information
on a more technical note you can think
of that matrix as a transformation
that's just stretching or squishing
vectors when you do that matrix
multiplication
so for instance let's start off with
this square here
you can see that this square has corners
at 0 0 1 1 1 0 and 0 1.
well what happens when we take those
vectors and we multiply them by the
matrix on the left we'll get some sort
of new shape
so let's start with the bottom left
corner 0 0
we are going to multiply this vector by
this matrix and i'll do this one by hand
because it's very easy
so we're going to get 0 times
1
plus 0 times 5
equals 0
and then 0 times
0.5 plus 0 times 1
equals make sure that's a multiplication
0.
so our new point is actually our old
point the vector 0 0. so i'm going to
add that to our new graph here
all right now that we've done one by
hand let's let a computer do the rest of
them for us
so the point 1 1 at the top right corner
now becomes 1.5
1.5 so we'll draw that in here
the point 1 0 becomes 1 0.5 so we'll
draw that in here
and the top left corner 0 1 becomes
0.51 so we'll say 0.5 and up 1.
so now we've created this entirely new
shape
and that's because the matrix
transformed our space
so i'm going to go ahead and color that
in here
so you can see basically the matrix
stretched and squished our original
square
and we might want to know a little bit
about that stretching and squishing
for instance what directions are the
main directions of stretching and
squishing
we can do this by looking at whether or
not there are vectors that aren't
actually moved or rotated they're just
stretched or squished those are going to
be the main directions of stretching or
switching
for instance we originally had this
diagonal vector in our square and in our
new shape it didn't actually move to the
left to the right or rotate at all
it just got a little bit longer it went
from 1 1 to 1.5 1.5 so for instance
that's a main direction of stretching
and that tells us a little bit about
what this matrix is doing
so basically we want to find a vector
where when we multiply it by a matrix
it's the same thing as just multiplying
it by a single number so either making
it bigger or smaller
to do that the formula is a so the
matrix times a vector equals
lambda times a vector so when we
multiply a matrix by a vector it's the
same thing as multiplying that vector by
a number
now i'm going to let a computer do all
of this calculation for us but it turns
out that the eigenvectors of this matrix
are 1 1 which we already saw here right
that's this vector when we do the matrix
transformation it just gets longer and
the other one is negative 1 1. in other
words negative 1 up 1 this vector
this one doesn't actually change or
rotate at all it just gets squished a
little bit
these eigenvectors or directions have
numbers associated with them called
eigenvalues and these basically just
tell us how much things are stretched or
squished
in this case the eigenvalue for the 1 1
vector is 1.5 which makes sense because
we saw that it gets longer it gets
stretched when we apply the
transformation the eigenvalue for the
other vector is 0.5 which again makes
sense because that vector even though
it's not changing directions or twisting
or anything it is getting squished so it
makes sense that it's being multiplied
by something smaller than one
now i'm never really going to make you
calculate eigenvectors or eigenvalues
but it's important you know what those
things are theoretically the eigenvector
are those directions in which vectors
don't change they just get bigger or
smaller and the eigenvalues are the
amount that we scale them eigenvalues
bigger than one means something gets
bigger eigenvalues less than one means
something gets smaller
and again this is important because
eigenvalues and eigenvectors give you
information about a matrix and we have
lots of matrices that we'll look at in
data science from data matrices to
covariance or correlation matrices and
sometimes the eigenvalues and the
eigenvectors are a good way to extract
information from those matrices
speaking of covariance let's talk about
it
so variance and covariance are very
similar concepts
variance is basically the idea of how
spread out data is
so you can see on the top line we have a
bunch of data points and on the bottom
line we have a bunch of data points but
the ones on the bottom are more spread
out so it has more variance the formula
for variance is here it's basically the
square deviation of every data point
from the mean so how far is the data
point from the mean square that and take
the average
covariance is very similar and actually
you might recognize how similar this
formula is to the variance formula
covariance instead of just a single
variable is looking at two variables and
covariance is a measure of whether or
not they move together or not and how
strongly they move together
for instance when we look at these two
graphs we can see that this graph has
better or higher covariance these two
variables move together a lot more
strongly than the ones in the right hand
graph so again higher covariance means a
stronger relationship when one goes up
the other goes up next let's talk a
little bit about the normal distribution
now i'm sure you've come across this at
some point but the normal distribution
is just a probability distribution that
is really useful to us first of all it's
really well behaved it's symmetric it's
unimodal you may have seen this as a
bell curve in one of your high school
classes and the nice thing about a
normal distribution is that we know a
lot about it everything from the area
under it to the formula for calculating
it we just know a lot about the normal
distribution so when we can assume that
variables or parameters or other values
are normally distributed it just helps
us calculate things when they're normal
because again we know a lot about the
normal distribution
all right now let's talk about data
types so when we have data sets we have
different variables that have different
types for instance height or weight or
gpa
are all examples of continuous data
continuous data is data that has
basically an infinite number of values
it could theoretically take on for
instance i might be five foot two point
one four three six five inches and you
might be six foot three point two seven
four six six inches so continuous
variables are basically what we're used
to seeing like numeric data right
something that could take on like a
decimal or something like that when data
isn't continuous another thing it often
is is categorical
these are data variables that are
groups basically so for instance i could
ask are you a freshman a sophomore a
junior or a senior those are groups and
there's not an infinite amount of
options of values that you could have
for that variable you're one of four
things freshman to senior another
example of a categorical variable might
be your major you only have a certain
number of choices of majors here at
chapman and you're one of them there's
not an infinite number of values that
that variable could take on but
categorical variables come in a lot of
different flavors so let's start with
the basic one nominal
nominal data is something that doesn't
have a meaningful order
for instance if i went around the
classroom and said what's your favorite
color red blue green i would be able to
put you into groups red blue or green
and there's no real order to them i
can't necessarily say that red is higher
or better or green is worse than any of
the other colors because there's no
order it's a nominal variable it's a
categorical variable but there's no
relationship or order between the three
categories one really simple version of
nominal data is dummy variables which
only have two categories for instance
registered to vote versus not or goes to
chapman doesn't now while nominal
variables don't have any relationship or
order to the groups that they exist
ordinal data does ordinal data is a
categorical variable where there is a
meaningful order that we can use to
organize the different categories
for instance i can look at the type of
degree that someone has so we might have
a high school diploma or ged
an aaa a bachelor's a master's or a
doctoral degree
now there there's still categories right
everyone's in a specific category you
can't have infinite values or if
infinite ways that you can be in that
category
but there is a meaningful order we could
for instance organize it from least to
most education from high school diploma
to a doctoral degree so because there is
an order it's not nominal anymore it's
an ordinal variable so again ordinal
means there's an order
and it's a categorical variable however
ordinal variables don't have to have
equal spacing between each of the
categories for instance in the example i
gave when you go from high school degree
to aaa or aaa to bachelors or masters to
phd
it's not necessarily that the difference
between those or the change between each
successive category is equal
if it were then we would have an
interval data type interval data is when
you have an order
and the difference between each of your
categories successively is the same for
instance let's say i show you a picture
of a really cute corgi and i ask you to
rate this corgi on a scale of 1 to 10
where you can only pick 1 2 3 4 all the
way up to 10. now i could technically
put you into groups right here's a group
of dog haters who rated this corgi a 1
and a group of dog lovers who rated this
corgi a 10
and there's also a meaningful order we
could rank you from
hates this corgi to loves this corgi
but in addition to the order we also
could argue that the distance between
rating the corgi a 1 and writing it a 2
is equal to the distance between rating
at a 6 to rating it a 7. because these
values or these groups are equally
spaced this categorical data is interval
it has an order and there is equal
spacing between each of the options now
when we have interval data we basically
treat it as numeric data or continuous
data because usually it is so for
instance if i asked you to rate this
corgi on a scale of 1 to 10 we can
basically treat that like a continuous
variable so to review for categorical
variables the most basic level of
categorical variable is nominal there
are groups that you can be a part of
there's no real order to them like your
favorite color being red green or blue
next we have ordinal variables ordinal
variables are categorical variables that
do have an order like the amount of
education that you have next we have
interval interval data is basically
ordinal data where we also know that the
distance between each of our categories
is equal and usually this is going to be
some number like again the one to ten
corgi rating scale okay enough
categorical next let's talk about
boolean data which is actually kind of
categorical as well
boolean data is sort of like dummy data
where it's either yes or no you're
registered to vote you're not you're
married you're not
boolean data though specifically is just
a true or a false value whereas dummy
variables are zeros and ones
basically the same thing but slightly
different last but not least we have
text data now text data may seem to you
like categorical data but there are some
things that are text data which aren't
really categorical
for instance if you've ever taken a
survey and they had a free response
question that they said you know what do
you like about this app and there's a
text box for you to type in your answer
or anytime you've had to enter your
email address or your actual physical
address
these things are text similar to a
categorical variable but unlike a
categorical variable there's not
groupings in the variable
for instance when i asked your favorite
color there were distinct options you
could have chosen red blue or green
when you enter your address or your
email address it's still text data like
the categorical variable but there's no
groups
basically my rule of thumb for text
versus categorical data is do i expect a
lot of people in this data set to have
the same
value when i ask if your favorite color
is red blue or green i do expect a lot
of you to be red green or blue but when
i ask you for your email address i don't
expect people to have the same email
addresses even some with something like
a physical address where two or more
people in your data set might have the
same address it's not like i expect big
groups of people to have the same value
when entering their address
next let's talk about probability
probabilities are basically a measure of
how likely we think it is that something
is going to happen
for instance i might say there's a 90
probability that i'm going to get an a
in this class one way we can calculate
probabilities is with continuous values
for instance if we have a normal
distribution we might ask what is the
probability that a number will fall in
this darker yellow section of the
distribution we can also have
probabilities with discrete events like
whether or not it's raining and whether
or not i brought an umbrella for
instance here you can see that it is
much more likely as it is in southern
california that it will not rain
compared to the chance that it will rain
maybe this is about a 75 chance of not
raining and a 25 chance of it raining in
general the formula for a probability
is the number of times that something
happens
divided by the total
number of things
so for instance let's say i want to know
like what is the probability that
chelsea is going to get coffee
well if out of a week i get coffee once
the probability would be 1 over 7
whatever that decimal is because the
number of times i got coffee one is
divided by the total number of days all
right let's talk about conditional
probabilities conditional probabilities
are just like regular probabilities but
with extra steps which i'll show you
so let's look at a plane probability
first i'm going to use this p notation
which means the probability of so we're
going to look at what is the probability
that i bring an umbrella to campus
so in this picture you can see that's
represented by this kind of like
transparent area
so this is a little high but let's say
it's about 20
20 of the time i will bring an umbrella
to campus well
let's look now at a
conditional probability we can say what
is the probability that i bring an
umbrella given that's what this little
pipe operator means that it's raining so
now when we calculate this conditional
probability we're not just looking at
overall how often do i bring an umbrella
which we estimated to be about 20
instead we are looking at just the times
when it's raining so we're going to go
ahead and basically ignore
everything on this half of the circle
when we have a conditional probability
what we're doing is saying let's only
look at cases
that we know
it's raining so everything after that
given we're saying assuming we know it's
raining
let's look at the probability that you
bring an umbrella well now that we're
ignoring basically this entire right
half of the circle the probability of me
bringing umbrella is pretty big compared
to me not bringing an umbrella
so the probability of bringing an
umbrella just generally is pretty low
the probability of me bringing umbrella
given that it's raining is pretty high
all right let's think of an example
that's a little closer to home let's
assume that the probability of being a
business student
is
0.1 about 10
now let's look at the conditional
probability of p business student
given you're taking 392.
well that's a lot higher in my classes
it tends to be about 50 percent
so given we're only looking at people in
the class cpsc 392 the probability of
being a business student is about 50
however just in the general population
the probability of being a business
student at chapman is probably about 10
i'm actually making these numbers up so
you can see the difference between a
regular probability and a conditional
probability
the conditional probability has a
condition it basically restricts the
denominator of our probability
in the probability of being a business
student the denominator is every student
at chapman in the conditional
probability our denominator is
restricted to people who were taking
cpsc 392. all right let's talk about
something related to probability which
is odds now if you're really into sports
betting or betting on horses or
something you're probably familiar with
odds odds are just a different way of
representing probability and the formula
for odds
is probability
over one minus probability in other
words the odds basically tell you how
many times more likely it is that
something will happen
versus it not happening
so for instance let's say that i have a
probability of 50 that i'm going to get
coffee before my next class odds that i
get coffee are the probability i get
coffee times one minus the probability i
get coffee well that's just 0.5 divided
by 0.5 so the odds one represent the
fact that it is just as likely that i
will get coffee as that i won't get
coffee now let's look at the odds that i
will fill my water bottle before class
let's say the probability of me filling
my water bottle is 0.9
so the odds would be 0.9 divided by 0.1
which equals 9.
it is 9 times more likely that i am
going to fill my water bottle before
class than that i won't odds that are
greater than one mean it's more likely
that something will happen then it won't
odds less than one mean that it's more
likely that something won't happen for
instance let's say that we're playing in
a soccer game and the probability that i
skull a gor goal is 0.4 or 40 percent
the odds that i score a goal are going
to be 0.4 over 1 minus 0.4 or 0.6 and
those odds are going to equal two thirds
in other words it is two-thirds as
likely that i will score a goal as not
meaning it's more likely that i'm not
going to score a goal last but not least
let's talk a little bit about the
difference between inference and
prediction
so when we have a model that estimates
something like say i have a model that
estimates your cat's weight based on its
height and age when we have that model
there might be two things that we care
about one might be the output of the
model whatever height that the model is
spitting out we want it to be accurate
so you can imagine we're putting into
our model the height
and the age and we're spitting out
the weight now one thing we could focus
on is just hey is this number that's
coming out
is this accurate when i give it new cat
information is it accurately predicting
their weight
that would be a focus on the prediction
of the model we don't really care what's
going on in here as long as what's
coming out of that model is accurate on
the other hand we could do inference
inference basically cares about what's
going on in this model
so we want to make sure that the way
that the model represents the
relationships between height and weight
or age and weight for your cat are
accurate to the real world we're
basically trying to learn something
about the structure of the world from
our model so we need to make sure that
that model is accurately structured
and that's the difference between
inference and prediction they can
sometimes go hand in hand but prediction
inherently cares more about the output
and inference cares more about the
structure of the model
in data science and machine learning we
unfortunately often focus a lot on
prediction and sometimes that makes
sense right if something can predict
whether or not someone is going to
default on a loan
maybe we don't really care why we don't
care about the structure of the model as
long as the answer we're getting is
accurate
but other times we might care about that
structure we might want to make sure
that the information we're putting into
the model uh is getting used by the
model in a way that's accurate so just
wanted to put this into your heads that
there is a difference between inference
caring about the structure of the model
being accurate versus prediction just
making sure that whatever comes out of
the model is good in this class a lot of
the things we're going to be doing is
going to be focusing on prediction we're
going to be assessing the output of our
model to make sure that it is accurate
all right thanks for sitting through
this one that's all i have for you i
will see you next time

 
hello and welcome to your decision trees
lecture
decision trees are basically flow charts
that you make with math
and we all use flowcharts even just
subconsciously in our day-to-day life
for instance if you've ever played the
game 20 questions you've probably used a
flow chart that looks a lot like a
decision tree to make a guess
say that i'm thinking of a skunk and
you're gonna try and guess what i'm
thinking of well your first question
might be something general like is it
alive
and so we would create this chart where
we have a live and we have either yes
or no
and i would of course say yes because
hopefully the skunk is alive
next you might have another question
like is it human
and so we can have another split here we
can say human
with yes
and no and of course i would say no
because a skunk is not a human
lastly you might ask oh is it a pet
and we have yes and we have no and i
would of course say no because hopefully
you're not keeping skunks as pets now we
could go on and on obviously you have 20
questions but you can see that what
we've built is basically a decision tree
that helps you guess
what i'm thinking of which in this case
is a skunk let's take a look at a really
simple decision tree this one is trying
to predict whether or not something is a
bird and the only question that it asks
is can it fly if it can we classify it
as a bird and if it can't we classify it
as not a bird
now this isn't perfect because there are
things that can fly that aren't birds
for instance bees and there are things
that are birds that can't fly like
penguins
but this simple decision tree which is
actually called a decision stump because
it only has one layer
is going to do pretty well at
classifying between birds and non-birds
but of course we can add some complexity
to this tree for instance if we have
something that can't fly we might then
ask does it have feathers if it can't
fly but does have feathers then we
classify it as a bird if it can't fly
but it doesn't have feathers then we
classify it as non-bird now we've added
a little more complexity and hopefully a
little more accuracy because things like
penguins would now get classified as
birds
and we can use continuous predictors as
well as categorical ones for instance i
could ask whether or not someone plays
fortnite or not based on their age and i
decided that my threshold is 20. so if
they're below 20 i'm going to guess they
play fortnite and if they're above 20
i'm gonna guess that they don't just
like with the categorical predictor we
can just have a binary decision based on
whether or not something exceeds or does
not exceed a certain threshold so
hopefully the structure of decision
trees and how you can make a decision
with one once it exists makes sense but
let's talk a little bit about how we
build them in order to understand how
decision trees are built we first need
to understand these two metrics first
let's talk about genie impurity
genie impurity is
one minus
the sum for every possible category of
the outcome
of the probability of that outcome
squared
so for instance let's look at a simple
example
let's say that you're going to pull 10
of your friends and ask them whether or
not they voted in the last election
there are two possible categories here
yes they voted or no they didn't and
let's say if you're friends eight of
them voted and two didn't
well to calculate the genie impurity we
would say 1 minus the sum
of
8 over 10 squared because again
8 over 10 is the probability in this
case
of being a voter
plus
2 over 10 squared because that 2 over 10
is the probability of not being a voter
in this case
so the genie impurity of this group
would be 1 minus 8
over 10 squared minus 2 over 10 squared
or 0.32
theoretically you can think of genie
impurity as the probability of correctly
classifying a data point if you first
randomly drew a data point from our
population and then secondly
randomly drew a classification based on
the classifications here so in this case
it would be like drawing one of your
eight friends and trying to guess
whether or not they voted or didn't in
the last election and the way you
classify them is just by randomly
drawing whether they voted or not with
eight chances of them voting and two
chances of them not you can see that
when a node has low impurity it's very
uniform it's mostly one group or another
when a node has high impurity it means
that it's pretty chaotic there's a mix
of a bunch of categories in there
next let's talk a little bit about
entropy now i'm really going to make you
calculate entropy by hand unless i give
you a computer or the formula
but it is a common way to measure the
chaos in different tree nodes
so impurity is equal to
negative times the sum
of each probability times the log base 2
of that probability
so you can see it probably looks really
familiar and similar to genie and purity
in fact both of these metrics will give
you very similar answers as to which
nodes have the best or worst measure of
chaos in order to calculate the entropy
of the example that we did with genie
impurity we would say
0.8
times
log base 2
of 0.8
0.2
times log base 2
of 0.2
and then once we sum those together we
would just make all of it negative
and in this case you would get about
0.71 as your entropy now because of the
logs i would never make you calculate
this without a calculator or a computer
of some sort but what i want you to take
away from this is that both of these
measures are measures of the chaos or
the purity of a node or a situation
you can see that when the node or the
situation is very pure in this case it's
pretty pure right we have 8 out of 10
people voting only 2 out of 10 not
voting then the genie impurity and the
entropy will tend to be low
when there's more chaos in other words
when there's a situation where maybe
it's about 50 50. your genie impurity
and your entropy are going to be very
high and in both cases hopefully you can
see that if you plug in the numbers
genie impurity and entropy will both be
zero when one category is the only thing
there imagine a situation where all 10
of your friends had voted in the last
election what would the genie impurity
and entropy be there so how does that
relate to how we build trees well we
actually build trees iteratively one
layer at a time we start at what is
called the root node
and then we build out splits from there
now here's just a little bit of
terminology for you remember that first
node the initial split is called the
root node
and anytime our tree is done and we have
just the bottom of our tree we call
those terminal or
leaf nodes
and as we'll see in a second leaf nodes
are where we end up making our decisions
as a data point moves through the
decision tree it'll finally end up in
some node at the bottom a leaf node and
we use that leaf node in order to
classify what category a data point
belongs to one term that we don't use a
ton but is technically there is the
internal node and an internal node is
just a node that then is followed by a
split
so in this picture you can see we have
the root node at the top and we keep
adding nodes and once a node has a split
below it it becomes an internal node and
we go all the way throughout the tree
until we reach the bottom where there is
no more splitting at that point a node
is a leaf node and the leaf nodes are
the point at which we make a decision
so how do we actually do this
mathematically well let's look at a
simple example
say we're trying to predict whether or
not someone has a cat based on whether
they not they had a pet as a child
whether they work from home whether they
have children and then their income in
thousands of dollars
of our four predictors we need to choose
one to be the root node
for simplicity here i'm just going to
ignore income because it's continuous
and we'll talk about that in the next
slide
for now between whether they had a
childhood pet whether they work from
home and whether they have children
let's decide which one is the best at
splitting our cat owners versus our not
cat owners
so first let's just try it out let's
build a decision stump with just one
split let's start with whether or not
they had a pet as a child
so if we went and we split our data on
whether or not they had a pet as a child
we would get two groups of people people
who did have a pet as a child
represented by ones and people who
didn't represented by zeros when we go
through and count we can see that of the
people that have pets in other words
this row
this row this row this row and this row
there are four that have cats and one
that does not
so we're going to go ahead and put a
four here and a one here four for half
cats one for dozen for people who don't
have pets again the remaining rows we
have two people that have cats and three
that do not
so we've made our split we have to see
how much the split improved our ability
to predict whether or not someone has a
cat
to do that we're going to use the genie
impurity just because it's a little
easier to calculate by hand
so to calculate the genie impurity for a
split we need to calculate the genie
impurity of each node and then
add them together in a weighted fashion
in order to figure out the overall
impurity of the split
so remember the genie impurity is 1
minus the sum of all of the
probabilities squared so let's do this
for the left-hand node for the left-hand
node i would have one minus
four over five
minus one over 5
squared and again i'm getting those
numbers from the fact that 4 out of 5
people in this node have cats and 1 out
of 5 don't have cats the 5 again is the
total number of people in this node and
so when i do this calculation i get
0.32 approximately
now for the right hand node where i have
two people that own cats and three that
don't i would get the formula 1 minus 2
over 5 squared minus 3 over 5 squared
and when we put that all together we get
approximately 0.48
now notice the node on the left is a
little more pure we can tell because
it's mostly one category cat owners and
it indeed has a lower genie impurity
than the right hand node which is a
little more chaotic with almost an even
split between cat owners and non-cat
owners now that we have the impurity for
each of our nodes left and right we need
to combine them together in order to get
the overall genie impurity for this
the way that we do that is by weighting
them by the number of data points in the
left node and right node respectively
for instance let's imagine a situation
in which the left node had a hundred
different people and the right node had
ten which one do you think should have
more influence on the overall genie
well the one that has more people that's
what the weighting does in this
situation
so in order to do this we're first going
to weight the scores that we have
so on the left-hand node 5 out of 10
of our data points are in the left-hand
node
and the impurity for this node was 0.32
then we are going to add 5
over 10 because 5 over 10 people were in
the right hand node times the impurity
for that node which is 0.48
and overall we are going to get 0.4
as the genie impurity for this split now
we have a measure of how well this split
allows us to classify people with cats
and without let's quickly look at what
would happen if we used work from home
as our first split
so if we did work from home
we have yes and we have no
we can see that for people that work
from home which would be this row
four of the people who work from home
have cats and one does not of the people
who do not work from home
two of them have cats and three of them
do not
well those numbers actually look very
familiar in fact it looks just like our
pet split so without redoing the math we
know that the overall impurity of this
split is 0.4
but we have one more variable left let's
take a look at whether or not someone
has children when we look at people who
have children which would be this row
this row this row and this row we can
see that three of them have cats and
four of them do not for people who do
not have children three of them have
cats and zero of them do not have cats
now let's start with the right note this
time because it's super easy
when a node is all one category the
genie impurity is zero so we're just
going to write that there no calculation
needed although you can double check on
your own
for the left hand node we actually need
to do some math so we're going to say 1
minus 3 over 7 squared minus 4 over 7
and when we plug that into a calculator
we get 0.4
not
now we know that the impurity for the
left node is 0.49 and the impurity for
the right hand node is zero now just a
little tangent here if the impurity of a
node is zero that means that we
literally couldn't improve it anymore no
matter what else we would possibly split
that node by it couldn't possibly do
better than this current node because
currently every single data point in
this node owns cats because this node
couldn't possibly be improved it's going
to be a leaf node we're not going to
split it any further when we have a leaf
node the way that we decide what
category to predict for any data point
that reaches that node is by taking the
mode or the most common category here
that would be owns a cat so anyone who
doesn't have a child in our data set
we're going to predict that they own a
all right back to the math
so now that we have our impurities for
the left and right node we need to
weight them and add them together
on the left hand node we have 7 out of
10 of our cases so we take 7 out of 10
times the impurity for that node which
is 0.49
and add well we would add 3 over 10
but then we're going to multiply it by
the impurity which is zero so basically
we're taking 7 out of 10 times 0.49
when we do that we get approximately
0.343
so out of the three categorical
variables splitting on whether or not
someone has a child
was the best split that we could have it
had the lowest overall genie impurity
again that is compared to both our pet
and work from home nodes which had an
overall impurity of 0.4
and that's how we choose which node is
going to be the root node in this case
whenever we are going to add a node to
our tree whether it's the root node or a
further split after that we always
choose the one that improves the genie
impurity or entropy the most in this
case that would be whether or not
now you might be thinking but you just
totally ignored one of our columns and
that's someone's income in thousands
well let's get to that
so when you have a categorical variable
it's very easy to imagine what those
splits could be we can have you have a
child or not you had a pet as a child or
not you work from home or not
with continuous variables there's no one
obvious way to split that variable so
what we have to do is try every single
possible split i'm going to color code
this so it's a little easier to
understand so what we're going to do is
we're going to first put all of our data
in order of whatever variable we're
looking at
here we're looking at income so we are
going to put it in order of income from
low to high
and then we are going to look at every
possible way you could split these data
points so the first way is of course
going to be like this where only one
data point is below the threshold and
the others are all above it
so let's go ahead and take a look at
what a root node would look like if we
use this split
the way we choose the number for the
greater than or less than is by
averaging the two numbers we split
between
in this case that's going to be 46.15
so we're going to say greater than 46.15
and how many of them are yes and how
many of them are no
well let's start with no because that's
a lot easier for no there's only one
data point that is not greater than
46.15
and it does have a cat
so we have one person with a cat and
zero without for the other node for
basically everyone else we have five
people with cats
and four
without to calculate the genie and
purity we'll use the same formula for
the left hand node it'll be 1 minus 5
over 9 squared minus 4 over 9 squared
and that's going to be equal to 0.49
right hand node like before is super
easy to calculate any time a node is all
one category the genie impurity is zero
but of course we have to take the
weighted average of these two because
most data points are in the left-hand
node so it should have a bigger
influence over what the overall impurity
is of this split
to do that we're going to take the
probability or the proportion of data
points in the left-hand node which would
be 9 out of 10
and multiply it by the impurity for that
node which is 0.49
and then we're going to add the
proportion of data points in the other
node which is 1 out of 10 and multiply
it by the impurity which in this case is
0.
when we do all of that math we get about
0.44
as the impurity for this split
now we need to check the next possible
so let's take a look at this split
first we have to choose the value that
we're actually splitting on and remember
it's going to be the average of these
two numbers we're splitting between so
we're going to say is the number greater
than or not 64.9
and then of course some are going to be
yes and some are going to be no
again let's start with the no side it's
going to be a lot easier
so when we look at people who are not
having an income
greater than 64.9 000 we have two people
one has a cat
and one does not for people who do have
an income above that level five of them
have cats
and three of them
do not so then of course we must go
ahead and calculate the genie impurity
for the left hand node that's going to
be 1 minus 5 over 8 squared minus 3 over
8 squared
and that's going to be
0.47 approximately
for the right hand node this is going to
be
1
minus
1 over 2 squared minus 1 over 2 squared
in other words 0.5
now notice when we only have two
categories the worst case scenario is
that 50 of our data points are in one
category and 50 percent are in another
which is the case here and you can see
that with two categories the worst
impurity we can get is 0.5
now let's weight those two together on
the left hand side we have 8 out of 10
data points
times the impurity 0.47
2 out of 10 for the right hand node
times its impurity of 0.5
altogether that gives us 0.476
as our overall impurity
so already we can see that first split
is a little bit better than our second
one but we actually have to check every
single split that is possible
to save you from doing all of the math
i've written all of the overall
impurities for the different splits the
first two at the top are the ones that
we calculated together
just like with categorical variables
where we picked whatever split gave us
the lowest genie impurity we do the same
here
in this case that's that 0.34
so we would make our split here for the
variable income in thousands let's go
ahead and make that tree
so we'd have income in thousands and
we'd have greater than 81.6
because that's the average between those
two
and we have yes
and we have no
for the yes side
we have
three people with cats
zero without and for the left hand side
we have three people with cats
four without
and if you do the math by hand yourself
you can see that you get an overall
impurity of 0.343
if you're still struggling with the gd
impurity calculation i highly recommend
you try and do this by hand right now
so now we know a little bit more about
what it's like to make a split in a
decision tree but decision trees often
have more than one split in fact they
look a little bit more like the
structure you see on your screen so how
do we decide how to build a tree further
well there's some basic rules first
whenever we try out a split we calculate
the genie impurity or the entropy for
each node
then we choose whichever split gives us
the lowest impurity
as we saw before when we were working
with the three categorical variables
whether or not someone had a child had
the lowest genie impurity of all three
splits so we chose that as our root node
so for example this would be the start
of our tree with this being the root
node but because our decision tree can
have more than one split we need to
decide where and when to go further
let's take a look at that right hand
node again
remember we noted that this node has a
perfect genie impurity it's zero it
couldn't possibly be improved by further
splitting
so we determine that this is going to be
a leaf node
in decision trees as we're building them
we keep trying out new splits so for
instance on this side we might split on
another variable like whether or not
they had a childhood pet every time we
try out a split we need to find the
overall genie impurity but we also need
to check whether or not it improves the
impurity compared to the parent node
the reason why we will never make a
split on the right hand node is because
it's impossible to improve it
any way we split this node would either
maintain the same genie impurity or
actually make it worse
so again the rules for adding a split to
a decision tree are make the split
calculate the impurity or the entropy
and then compare it to all of the other
options as well as to the parent node
if the parent node the one we're
splitting from
actually has the lowest impurity
we're not going to add anything it's
already as pure as possible we couldn't
possibly improve it by adding a further
however if one of the candidate
variables that we can split on improves
the genie impurity or the entropy then
we go ahead and add that split and so on
and so forth until all of our nodes have
leaf nodes since we know that having
child as our root node has the best
possible outcome from all of our
different categorical variables let's
have that be the root of our tree
so let's go ahead and try and make our
tree bigger
we know that using child out of all the
categorical variables is the best option
it lowers the genie impurity the most
compared to all the other splits but
let's go ahead and try and add more
for the right hand node we know that the
impurity is zero so we're actually going
to make this a leaf node because it
isn't possible that any split that we
could possibly make could improve the
genie impurity at most it would keep it
the same and at worst it would actually
make it worse
so this is going to be a leaf node on
the right hand side
on the left hand side however our genie
impurity was 0.49
so there's definitely some room for
improvement
let's try and use pets as the next split
so again because we're in the left hand
node we're only looking at people who
have children
so it's going to be this row
this row this row this row and then
these three rows at the end
so let's look at people who did or did
not have pets as a child
of the people that have children
so we're going to say
pets and we'll say yes
no
so for people that did have pets as a
child there's only two of them
one had a cat and one did not for people
that have children again because we're
in this left-hand node
and did not have pets as a child
two have cats
and three
did not
so we need to go ahead and calculate the
impurity of this split
well using our formula it's going to be
1 minus 1 over 2 squared minus 1 over 2
squared equals
0.5
and then for the right hand node it's
going to be one minus two over five
squared minus three over five
and that is going to be zero point four
eight
now of course we need to weight these
together
to do that we are going to take
the number of data points in the
left-hand node so that would be 2
out of 7
is 0.5
plus how many are in the right hand node
well it's 5 out of 7. so we say 5
out of 7 times the impurity for that
node which is 0.48
all together with a little rounding we
get 0.49
as our overall impurity for this split
now notice that does not improve our
parent node both of these the parent
node as well as the subsequent pet split
gave us an overall impurity of 0.49
which means we're not going to make this
split it doesn't actually improve
anything
the other split that we could possibly
make is on whether or not people work
from home
so we have yes and we have no
for people who have children because
again just working with this left node
and work from home
we have two who have cats and one who
does not
for people who have children but did not
work from home
we have one person with a cat and three
who do not have a cat
to calculate the impurity we use our
formula for the left hand node we're
gonna have one minus
two over three squared minus one over
three squared
and that's going to be 0.45
and for the right hand node we're going
to have
1 minus 1 over 4 squared minus 3 over 4
squared which is going to equal to 0.375
now of course we have to weight these
together so for the left-hand node we
have 3 of our 7 data points
times the impurity for that node 0.45
plus on the right hand node we have
four of our seven data points
times the impurity for that node or
0.375 altogether this gives us an
overall impurity of 0.41
now we have to see does this improve the
impurity compared to the parent node and
in this case it does our impurity for
the parent node is 0.49 and that goes
down to 0.41 for the child node when we
added work from home
this means that adding this split
improves our ability to predict whether
or not someone has a cat and you can
kind of see that's the case visually
because in our nodes that we just
created each have more of one category
than another
so when we build decision trees we just
follow that rule over and over for every
split we figure out which variable
improves the genie impurity the most
we choose whichever variable does that
or if the parent node has the lowest
impurity we just leave it as a leaf node
and we continue on and on basically
until we have all leaf nodes
one thing about decision trees that is
really cool is something called variable
or feature importance
variable or feature importance answers
the question of how much does a feature
reduce node impurity basically how
important is this feature to the
decision tree
we calculate variable or future
importance by looking at how much every
split on that variable decreases the
entropy or the impurity weighted by how
many data points actually get to that
because of course even if it reduces the
entropy or the impurity a lot if only
two data points are going there it's not
that important
once we do that for the entire tree we
have the feature or the variable
importance the higher the importance the
more important a variable was to the
one really cool application i've seen of
variable or feature importance in
decision trees and related models is for
variable selection say you have a
hundred variables but you only want ten
using variable or feature importance as
a way to choose the top ten most
important variables when predicting an
outcome is a really cool way to do that
and last but not least i want to just
mention regression trees
so far we've been using decision trees
as a classification model meaning that
it's predicting a categorical outcome
the decision trees can also be used for
regression or predicting a continuous
value
decision trees and regression trees are
very similar the main difference is that
regression trees are predicting a
continuous outcome for instance we could
use whether or not something is able to
fly as a way to predict the density
of their bones
we have leaf nodes and decision trees
instead of predicting the most common
value like we do in a decision tree or
the mode
we use the average
say we're predicting housing prices and
we have 10 data points in a leaf node
in order to come up with the value that
we're going to use as our prediction
from the model we would take all 10 of
those houses and take the average of
their prices and that would be what our
model would predict for any data point
that makes it to that leaf node another
difference would be how we measure the
success of the tree
remember we've been using things like
genie impurity or entropy as a way to
measure how well our tree is doing and
how well individual splits improve our
decision tree for a continuous value in
a regression tree we can use metrics
that we're used to like a mean squared
error a mean absolute error or something
else that measures the distance between
our guess and the actual value
so both decision trees and regression
trees are very similar they just differ
in the output of the model and the way
that we measure how well our model is
predicting things
all right that's all i have for you i
will see you next time

 
hello and welcome to your naive bayes
lecture naive bayes is an algorithm used
for classification
and this is a really fun one to lecture
about because we're going to split our
lecture into two parts one part about
the bayes part of naive bayes and one
part about the naive part let's start
with the base part
the base part refers to bayes theorem
which is used in naive bayes you can see
the general theorem for bayes theorem in
this faded gray color
basically it's a way to figure out the
probability of an event given that some
other event has happened for instance we
could think of the probability that
you've taken drugs given that you got a
positive drug test and we could
calculate that so that would be p
drugs
given positive on a drug test
that you can calculate by looking at the
probability of getting a positive given
that you've done drugs
times the probability that you've done
divided by
the probability of getting a positive on
your drug test
bayes theorem is really useful in many
areas of statistics especially bayesian
statistics but here we're just using it
to calculate our conditional probability
when we have a classification problem we
basically want to know how likely it is
that a data point is in different
categories that it could be in for
instance if we're looking at whether or
not someone is registered to vote or not
we want to know what is the probability
someone's registered to vote and what is
the probability that they're not
registered to vote that would help us
decide whether or not to classify them
as registered or not in order to
calculate something like these
probabilities we use bayes rule in other
words for every category that we could
possibly classify a data point as we
want to calculate the probability of
that category
given all of the values of the
predictors that they have for instance
we could say what is the probability
that you're registered to vote given
your age your occupation and your
political affiliation we would then want
to compare that to the probability of
you not being registered to vote given
your age occupation and political
affiliation
basically we want to predict the
probability of being in a specific
category given the various values you
have for your different predictors
but we don't know that so the way that
we calculate that is using bayes theorem
to do that we calculate the probability
of getting those different values for
the predictors given that you're in a
certain category
times the probability of being in that
category overall
now you may have noticed in the last
slide that i totally ignored the
denominators and that is for good reason
let's look at an example
say we're trying to predict whether or
not someone had a heart attack based on
the fact that they smoke are diabetic
and are obese
well when we calculate the scores we're
really comparing two categories the
probability that they had a heart attack
given those values and the probability
that they didn't have a heart attack
given those values
when you look at everything written out
here and here you can see that these two
values have the same
denominator because all we're really
doing is looking to see which value has
the higher probability having a heart
attack or not
we're just comparing two numbers to see
which one is bigger and because when
we're comparing two numbers to see which
one is bigger it doesn't really matter
if we divide or multiply by a positive
number it's just the same one's going to
be bigger
so to make our lives a little bit easier
math-wise we go ahead and just cancel
out those denominators and completely
ignore them because whichever
probability was going to be bigger well
without the denominator that score is
going to be bigger anyway
i call it a score because when you don't
divide by the denominator it's
technically not a probability so we're
going to call them scores but they're
proportional to probabilities and
whichever one is bigger is going to
indicate which category is the most
likely for our data point so for naive
bayes we calculate multiple scores one
per possible outcome category in this
case we're going to calculate the
probability that someone has a heart
attack given that they smoke are
diabetic and are obese
to do this we're going to use the
probability of seeing someone who smokes
is diabetic or is obese given that
they've had a heart attack which in
other words is of all the people who
have had heart attacks how common is it
to see people who smoke are diabetic and
are obese
and then times the probability of a
heart attack thankfully heart attacks
are rare so even if someone seems like
they might be a very likely candidate
for a heart attack we don't want to be
over predicting heart attacks so we
multiply by the rate of heart attacks in
the general data set just to make sure
that our predictions are kind of in line
with what's happening in the data
similarly we calculate a score for the
probability of not having a heart attack
given that someone is a smoker diabetic
and obese
we do that by calculating first the
probability of seeing someone who is a
smoker diabetic and obese given that
they didn't have a heart attack in other
words of all the people who didn't have
a heart attack how often do we see
people who are smokers diabetic and
obese
times the probability of not having a
heart attack
we'll end up with two different numbers
we'll call this s sub heart
and this one s sub not heart
in other words we'll end up with a score
indicating how likely it is that someone
would have a heart attack given that
they smoke are diabetic and obese and
then a score representing how likely it
is that someone won't have a heart
attack given that there's smoker
diabetic and obese
whichever of these scores is bigger is
going to be the category that we
classify our data point as so if the
score for heart attack is bigger we
would say this person is going to have a
heart attack all right that was the base
part of naive bayes because we used
bayes theorem let's talk a little bit
about the naive part of naive bayes the
naive part of the naive bayes algorithm
refers to the fact that within
categories we assume that predictors are
independent let me say that again
within a category we assume that the
different predictors we use like smoking
being diabetic or being obese are
independent meaning that they're not
related
now the reason this is a naive
assumption is because we know that
pretty much always this assumption is
not true take the example we just looked
at being a smoker being diabetic and
being obese are all dependent on each
other the increase of one is likely to
increase the probability of being
another or for instance if you are poor
and have very little extra time and
resources it may be more likely that you
smoke to kind of reduce your stress and
that also means you don't have time to
prepare healthy meals or afford a
healthy food
and so then you might be more likely to
be obese so in the real world we know
that these things are all related they
affect each other they're not
independent
but
when we use the naive bayes algorithm
the way that we calculate probabilities
assumes that all of these predictors are
independent and we know that's not true
so it's a naive assumption to make
because we know it's not true
however
this
greatly improves the computational
efficiency of the naive bayes algorithm
and it turns out that in real life
applications it doesn't really make the
naive bayes algorithm that much less
effective
so basically we're making an assumption
that we know is incorrect to help
improve the computational efficiency of
our model and it tends not to have that
big of an impact on whether or not our
model does well so let's talk a little
bit about the independence of those
probabilities really quick and i'm going
to give you a very simple example
say we're just looking at whether or not
someone is a smoker and whether they're
diabetic
to calculate the probability of someone
being both a smoker and diabetic
represented here
we would need to use this formula
we take the probability of being a
smoker and multiply it by the
probability of being diabetic given that
you're a smoker however when we assume
that these two things are independent
the probability calculation gets a lot
easier
in this case we would say that if we can
assume that they're independent
then the probability of being a smoker
and the probability of being diabetic
would just be the probability of being a
smoker
times the probability of being a
diabetic because they're independent
basically in probability terms the
probability of being a diabetic given
that you're a smoker is equal to the
probability of being diabetic because
they're unrelated they don't affect each
other
this probability is a lot easier to
compute and i just want to show you an
example of what this would look like so
let's calculate the probability two
different ways
all right so in my class i get a lot of
business majors so let's look at the
probability of being in cpsc 392 and
being a business major
well because i have a data table here i
can actually just calculate this
probability straight up so let's do that
just so we can check our work in the
data we have 20 students and six of them
are both business majors and in cpsc 392
so our probability is 6 over 20 or 0.3
all right now let's use our formulas
first let's use the one that does not
assume independence
this says that the probability of being
in 392 and being a business student
would be equal to
the probability of being in 392
times
the probability of being a business
student given that you're in 392.
well if we count them up we can see that
the probability of being in 392 is 10
over 20
and the probability of being a business
student given that you're in 392
is 6 over 10
because out of the 10 people in 392
six of them are business majors when we
multiply this together the tens cross
out and we get 6 over 20 which is the
correct answer that we saw before now
let's calculate that same probability
assuming
that these things are independent which
we know is not true in real life so when
we make the naive assumption that these
two things are independent we are
basically saying that the probability of
being in 392 and being a business
student is the same thing as the
probability of being in 392
business student
so notice we don't have to calculate
this conditional probability we just
need the probability of being a business
student and the probability of being in
cpsc 392.
when we calculate these things well we
know already that 10 out of 20 is the
times 8 out of 20 is the probability one
two three four five six seven eight of
when we multiply those things together
we get 0.5 times 0.4
or 0.2 we can see that the value of 0.2
that we got this way assuming
independence is not correct
that's because these two variables are
actually not independent they depend and
affect each other this means that when
we assume that things are independent
when they're not we might get the
incorrect probability however
not having to calculate all of these
conditional probabilities especially as
we have more and more predictors
ends up being incredibly computationally
efficient hence we sacrifice a little
bit of numerical accuracy for
computational efficiency and like i said
in the beginning it ends up not really
mattering that much and naive bayes is
still a very effective algorithm so to
review the bayes part of naive bayes
refers to the fact that we use bayes
theorem minus the denominators in order
to calculate scores to compare the
likelihood of a data point being in
different categories of the outcome
the naive part of naive bayes refers to
the fact that we assume within
categories that the predictors are
independent we know that's almost never
the case however it improves
computational efficiency and it turns
out to not affect the algorithm too much
so let's do some naive bayes one really
famous example of when naive bayes is
used is to classify emails as ham or
spam as a real email or as a junk mail
one way we could do that is by looking
at whether or not certain words are in
an email
here i'm going to look at four words
viagra love dollar and buy so let's do
some calculations
we know that we have to use bayes
theorem without the denominator of
course so for each category we're going
to calculate the probability of
category
given our
predictors which is going to equal
the probability of the predictors
given the category
the probability of the category
so let's go ahead and do that
so when i say
the probability of a predictor given a
i'm basically saying how likely is it to
see this combination of predictors
in a certain category given that it's in
a certain category
so let's go ahead and do that for an
email
say our email is
hi
i would love to buy you lunch let's go
out sometime love tom so if we look at
that email we can easily see that it
does not have the word viagra
it does have the word love it does not
have the word dollar and it does have
the word buy
so anytime a word is present we're going
to indicate that with a 1 and any time
it's not present we're going to indicate
that with a 0. all right let me erase
that so for a ham email let's figure out
what its score is well first let's
calculate this part
the probability of seeing our predictor
values given the category in other words
what's the probability of seeing an
email without the word viagra with the
word love without the word dollar but
with the word by given that it's a ham
well we luckily from our training data
have the probabilities of these
different words occurring
viagra appears in three percent love and
36 percent dollar and two percent and
buy in two percent of our emails that
are real ham email let's go ahead and
calculate our score the probability that
a ham email has the word viagra is three
percent
so
our email did not have the word viagra
so to calculate the probability of an
email not having viagra in it is 1 minus
0.03 because if 0.03 is the probability
of having the word viagra 1 minus that
is the probability of not having the
word viagra then we need to look at the
probability of having the word love
because our email had the word love
so to calculate the probability of
having the word love well that's easy
that's just
0.36 we have that in our table
then we know that our email did not have
the word dollar the probability that the
word dollar was in a real email is two
percent so the probability that it
wasn't is one minus two percent
finally we're going to account for the
fact that our email had the word buy the
probability of a real email having the
word buy is two percent so we're going
to multiply by 0.02
now we're almost done but we can't
forget
about this part of the equation we need
to multiply by the probability of an
email being a ham email in general
in this data set i happen to know that
it's 50 50. there's a 50 chance of being
a ham email and a 50 chance of being a
spam email so we are going to as our
last step multiply by
0.5 if there was a 70 chance we would
multiply by 0.7 or an 80 chance by 0.8
but in this data set there are half ham
half spam emails so we're multiplying by
0.5 when we multiply all that together
we get 0.0034
and i'll go ahead and round there all
right now let's do the same thing but
calculating a score for it being a spam
email so first we're going to do the
same thing that we did before but now
we're looking at the probabilities with
regards to being a spam email
so let's go ahead and erase this
all right so the first thing we know is
that our email did not have the word
viagra the probability of seeing the
word viagra in a spam email is 32
meaning that the probability of not
seeing the word viagra would be one
minus 32 percent
then we're going to multiply it by the
probability of seeing the word love
which in this case is 0.05
then our email did not have the word
dollar so the probability of seeing the
word dollar is 83
in a spam email so the probability of
not seeing the word dollar like we
didn't is one minus 0.83
then we're going to multiply that by the
probability of seeing the word buy in a
spam email well here it's 0.74 so we'll
say times 0.74
now again like last time we need to
finally not forget this part of the
equation so we're going to multiply by
0.5 and that is because in this data set
there are 50 percent ham emails 50
spammy ml so that's where that number
comes from when we multiply all this
together we get about
0.0021 and i'll go ahead and round there
so notice these two things are not
probabilities we'll call them scores but
we have one per category
we are going to choose the category that
has the highest score
in this case that is the ham category
because 0.003
is a little bit bigger than 0.002
so in this case we would classify this
email as a hamill
just to review we used bayes theorem
well the numerator to calculate scores
for each possible category
we did that by using the probabilities
of our different predictors given that
they were in certain categories for
instance this top row is the probability
of seeing these words given that you're
a ham email
and the bottom row is the probability of
seeing these words given that it's a
spam email
we use these probabilities in order to
calculate our scores there's two parts
to calculating the score first is the
probability of seeing our different
predictors given that we're in a
and the second is the probability of
being in that category
together when we multiply that all out
we get our score and whichever one is
higher is the category that we classify
our data point as so to review for naive
bayes the two important things to
remember are that it's naive and that
it's bayes naive because it assumes
independence of your predictors when
that's usually not true although it does
make things more computationally
efficient
and bay is because we use bayes theorem
minus the denominator to calculate
scores comparing how likely it is that a
data point is in different outcome
categories now in our examples all of
our predictors were categorical but you
can actually use continuous values as
well we'll talk a little bit more about
that in the next lecture
all right that's all i have for you i
will see you next time

 
hello and welcome to your all the stuff
you need to know python lecture so today
we're going to cover some of the new
stuff in python that we're going to need
in order to work with this class mostly
that's going to be things from the
pandas and the numpy packages briefly
pandas is a package that is built in
python to work with data specifically
with data frames and a data frame is
just a way of storing data it's like an
excel spreadsheet but maybe a little bit
simpler you have rows where data
represents a person or a cell or
whatever unit you're measuring and a
column represents a feature or a
characteristic of a data like height
weight something like that pandas not
only gives us the data frame but also
different ways of working with it
on the other hand numpy is a way to
implement arrays in python we'll talk
about this at the end but arrays are
likely but a little bit different so
obviously in a data science class one of
the most important things we're going to
be doing is working with data and so one
of the first things we have to do is
make a data frame all right we're going
to run our imports and then let's go
over a couple ways that we can make a
data frame so the first way that we can
make a data frame is to have a bunch of
lists that each represent the columns in
a potential future data frame here we
have a column for name of student age of
student and finally the major of that
student
so one thing we can do is we can make a
dictionary where the keys in the
dictionary are the names of the columns
and the values of the dictionary are
lists representing the actual data in
the column so for instance i can say
name as a key
and put name which is our list
here
and then i can say age
age
and finally
major
now the keys don't have to be named the
same thing as the variables but here it
ended up working out that way and now
that we have a dictionary we can finally
use pandas to actually convert this into
a data frame so i'm going to say df
that's going to be my data frame
variable equals
pd that's the pandas package dot data
frame
2 df
and that's going to tell me okay take
this dictionary turn it into a data
frame and once we have that let's go
ahead and look at our data frame so i'm
going to say df
and run
beautiful so you can see the outputted
data frame that we have here now
normally our data frames are not going
to be this short and so we'll typically
use this function called the dot head
method
and this will basically give us just the
top couple of rows of our data frame so
that if we have a data frame with a
hundred rows we're not printing all of
that out inside of our notebook another
common way you might get data into a
data frame is with a list of lists so
you can see here that we have a big list
and inside it we have sub lists that
each represent a row of our data frame
in this case it's the exact same data
that we loaded above
so in order to do this we're going to
say df2 this is going to be my new
variable equals pd
dot data
frame same function but instead of
giving it a dictionary we are going to
give it our list of lists
now because we have a list of lists
instead of a dictionary pandas doesn't
know what the column name should be so
we're going to go ahead and provide
those by saying columns
equals and then a list of all of the
column names here it'll be name age and
now that we have that let's go ahead and
print out our data frame
so you can see we got the exact same
data frame as above just with a list of
lists instead of a dictionary another
way to load in data files when you're
running on your computer rather than on
google co-lab like we are now is to load
in data frames using a file path now i
can't do this right now because i'm
running on google co-lab and that means
that i don't have access to all the
files on my computer but if you were you
can go ahead and load in data straight
from your computer to do that we would
say df3 equals pd dot and now we're
going to use a new function called read
underscore csv
and then we're going to give it the file
path to the file on our computer so i'm
going to say path df
now again this isn't going to work and
i'm not going to run it here because i
don't have my local files i'm running on
the cloud not on my local computer but
if you happen to be running on your
local computer either through jupyter
labs or vs code you can load in files
directly from your computer using this
method yet another way we can load in
files is probably the most common one
we're going to use loading it from a url
so if your data is hosted online maybe
somewhere like github you can copy the
url to your file and read it straight
from there so here we have the url to a
data file
penguins.csv that i would like to load
and we can go ahead and load that
straight from the url so i'm going to
say df4 equals pd.read
csv so we're going to use that same
function but instead of giving it a file
path i'm going to give it a url
so now that we've loaded that
let's go ahead and call head now that
we're loading from a larger data file we
don't want to print out hundreds of rows
so let's go ahead and run that
and you can see we have now imported and
loaded our penguin csv data frame and
the dot head function has just shown us
the first couple of rows even though
there definitely are more rows now one
thing i quickly want to cover related to
that url loading is how to actually get
that url
so i am on our course web page and i am
going to go to the data folder and i'm
going to scroll down until i see the
penguins data frame that i want to load
i'm going to click on that and you can
see that it shows you this very pretty
form of your data frame that you can
scroll around and see but this is not
the url that we want to use instead we
are first going to click raw at the top
right and it's going to load this page
for us
this is the url that we are going to
copy and paste to use when we're loading
in data
if you try and use this url it won't
work and the way that you can tell is
this url does not have the word raw in
it when we click on that raw button you
can see it adds raw to the beginning of
the url that signals to us that this is
the correct url with just the file and
that it will load into pandas all right
last but not least let's talk about how
to load in files that you have on your
local computer into google collab so
that you can use them
so we can't give a file path to our
computer because google collab doesn't
know where anything is on our computer
and it doesn't have access to that but
we can give it access to files that it
needs
if you click this little folder button
to the left on google colab you have a
little menu that has an upload button
when you click this upload button
you have the opportunity to choose a
file that you would like to use so i'm
going to upload penguins.csv
to google colab
now that penguin csv is uploaded i can
just load it as if it's a file on my
computer so i'm going to say df5 equals
pd dot read underscore csv
penguins
dot csv
and then we'll say df5 dot head
and go ahead and run that so now that
this file exists and is uploaded to
google cola we can just read it in that
way as well now that we have our data
frames loaded we probably want to learn
some information about them one of the
main things that we'll do very often is
count the number of rows and columns in
our data frame the easiest way to get
both these pieces of information is with
the dot shape attribute
so if we have a data frame and we use
df4 and we call dot shape
this will print out the rows comma the
columns of your data frame so here when
we run it we can see that there are 345
rows and nine columns in this data frame
if we only wanted the number of rows we
could for instance say zero that's just
going to give us the item at the zeroth
index
and it'll just give us the number of
rows we would use the index 1 if we
wanted the number of columns the next
thing that you might want to do is know
what columns you have in your data frame
to do this we say the name of the data
frame dot columns this is going to tell
us what are the names of all of the
columns in our data frame so for
instance here we have species island
bill length build up
these list all the columns that we have
in our penguin data frame next we might
want to know a little bit more detailed
information about our columns there's a
couple ways to do this one is df4.info
this is going to give us some
information about our data frame
so here you can see the names of the
columns you can see how many non-null so
non-missing values we have and then of
course the type that each column is so
for instance species is an object which
basically means a text string
we have some floats and of course an in
another thing we might want to do is use
df4.describe
which is going to give us a little bit
more information about our continuous
columns so here you can see all of the
continuous columns and you have
information like the count the mean the
standard deviation and some of the
quartiles that represent like the 25th
50th percent 75 etc now we talked a
little bit about the head function
before as a way to just grab the first
couple of rows of a data frame well we
can actually control exactly how many
rows we want so let's say df4.head
by default when we run this it only
gives us the first five rows of the data
frame but say we want the first 12 well
we can just give the argument 12 and
it'll print out 12 rows for us
similar to the hud function if you ever
for some reason want to see the bottom
of your data frame you can use the tail
function just like this
next let's talk about the group by
function
so our penguin data frame has a couple
of columns like the species or the
island that are basically categorical
variables they group the penguins either
into similar species or by whether or
not they're on the same island
sometimes we'll want to summarize
something for each of those groups
separately so for instance let's look at
the bill length and the build depth for
penguins of different species
so i'm going to say the name of our data
frame dot and this group by function
which takes in the argument of which
column or columns you would like to
group by
so i would like to group our penguins by
species because i want to have a
separate summary for every penguin
species
so once i have that group by function i
can grab some columns or do some data
summaries so let's go ahead and let's
grab
the bill
length
mm
column
and the bill
depth
mm column
now if you want to get the average bill
length and build depth we're going to
have to use the dot mean function
the dot mean function as the name
implies is going to grab the mean for
both of these columns so when i run this
you can see that it's outputting a table
for me there is a row for every species
a deli chin strap and gentoo and then
there's a column for build length and
build depth
these numbers are the means for these
different penguin measurements sometimes
especially when you're working with data
that is
real world messy data you're going to
need to check that all of the data is
actually
there and to do that we are going to
check for missing values the way that we
do that is we say the name of our data
frame dot is
null
dot sum
and this is going to grab us the number
of missing values per column in our data
set
so for instance in our data set uh
sex is missing 11 values which means for
11 of the rows we don't know the sex of
the penguin
bill length build depth flipper length
and body mass are all missing two values
so because that's not a ton of missing
data remember we have 344 rows of data
i'm just gonna go ahead and drop any row
that is missing a value
to do that we're gonna say df4 equals
df4 dot drop
n a
and this is going to just say okay if
any row is missing a value drop it
so let's go ahead and run that and then
we'll also rerun this is null
function so that
we can see that once we've dropped all
of our missing values they no longer
exist
now you'll notice that when i dropped my
values i said the name of the data frame
equals the name of the data frame.drop n
a another way that you can do this is
you can say the name of your data frame
dot drop
and you can use the argument in place
equals true and that's just going to
automatically drop the values from your
data frame without you having to
reassign it to the variable name
another thing you're going to want to do
whenever you drop your values in either
of these cases is you're going to want
to reset your indices
so the indices for the row right from 0
1 2 all the way up to the number of rows
those do not get re-done when you drop
values so say you have a data frame and
row at index 5 is missing a value and
gets dropped your indices now look like
0 1 2 3 4
6 7 8 because we dropped that row but we
didn't renumber the indices for your
rows
in order to do that we're going to say
df4 dot reset
and again in place equals true
so when we run this this is going to go
ahead and drop our nas
and then also renumber those indices so
instead of 0 1 2 3 4
6 we'll again have 0 1 2 3 4 5 etc
so sometimes when we're working with
data that has a categorical variable
with more than two categories it'll be
listed like this so here we can see that
there are multiple categories a b and c
and the group variable just has a string
representing if it's a b or c
but often a lot of the algorithms that
we use are going to want it in a bit of
a different format so when we have a
categorical variable that has more than
two categories we're going to want to
use dummy variables so i'm going to
create this and i'm going to say dummies
equals pd dot get
dummies
and i'm going to tell it what data frame
i want to use so here i want to use the
df data frame and then i'm going to tell
it columns equals and this is going to
be which columns we want to create dummy
variables for
in this case i just want to create dummy
variables for the group column
so now that we've done that let's go
ahead and run it
what you can see is that we now have a
data frame with dummy variables dummy
variables are zeros and ones that serve
as indicators of whether or not a data
point is in a certain group
so we know that the first data point is
in group b
so it has a 1 for the group b column
that was created and a 0 for all of the
other columns
the second row is in group a so it has a
1 for group a a 0 for group b and a 0
for group c
so basically this is the same data just
represented a little bit differently
with multiple columns per variable with
ones if a data point is in that group
and zeros otherwise
now sometimes there might be multiple
variables that we need to create dummy
so in this case i've created this data
frame that has the grades someone got
and their class both of which are
categorical variables with more than two
categories and i want to create dummy
variables for both of them so i'm going
to do that using again our data frame.
excuse me
get
and i'm going to say the name of the
data frame comma
columns
and we are going to list which columns
we would like to create dummies for
in this case it's grade and class
and once we're done with that let's
print it out
so now you can see we have multiple
dummy variables we have a couple of
dummy variables for the grade that
someone got and then four dummy
variables one for each year that they
are in high school
let's talk about subsetting data so
often when we have a data frame we're
going to want to grab smaller pieces of
it maybe only a few rows or only a few
columns or a combination of a subset of
rows and columns and there's multiple
ways to do this so one thing we're going
to do a lot is grab a single column from
a data frame
so for instance let's say that i want to
grab the species column
from our penguin data frame i can do
that by saying the name of the data
frame square brackets and then the name
of whatever column it is that i want to
when i run this you can see it just
returns to me by itself the column for
the species of the penguin
an identical way to do this is df4 the
name of the data frame dot species
and this will work exactly the same way
let me comment this out and rerun
you can see it gives us the same row the
only reason that i prefer the first way
is because sometimes when variables have
spaces or non-traditional characters
this one doesn't work but the square
brackets with a string of the column
name will always work
and once we have a column we can do all
sorts of stuff to it so we can say df4
square brackets bill
link
there we go and then and again this is
just going to grab us the bill length
column but let's say i want to know the
mean bill length for all penguins in our
data set i can do that by grabbing the
column and then calling dot mean on it
that will return to us the mean
of the bill links for all of the
penguins now grabbing columns by name is
really useful because the names of
columns mean something but often we want
to grab things like rows or even
sometimes columns by their number or
their index
so for instance let's say that i want
the row at index eight well one way that
i can do that is i can say df dot i look
oh excuse me that's df4 dot i look
eight
that is going to return to us the row at
index eight
now notice that this returned to us a
little weird where it's just like
printing out kind of as text if you want
it to print out prettily like an actual
data frame we can actually run that same
code so we'll say df4.iloke
but instead of giving it just a single
number eight we're gonna give it a list
with the number 8 inside of it
when we run it this way you can see it's
the same information it just now looks
like a pretty printed out data frame
instead of what this is which is called
a series
now you could also give this multiple
numbers so we could say like df.iloc87
and that would give us the row at eight
index and seven index so you can do this
with multiple numbers but even if you
just want one row if you want it to
print out nicely you can do it this way
let's say we want a range of rows so i'm
going to say df4 dot iloke now i in ilok
stands for integer which means we're
grabbing things by the integer or the
index of them
so when we say df.i look i can say hmm i
want the rows at index 2 through 7. to
do that i'm just going to say
colon df4.iloc2colon
and that is going to grab us the rows at
indices two up to but not including
seven so you can see here we have two
three four five and six
now i look takes both rows and columns
that you want so here we've just been
sub selecting on the rows but we can
also subselect on our columns by indicee
if we want
so let's say df4 dot iloc and we'll do
the same thing that we did before 2
colon 7. we already know that that's
going to grab us the rows at indices 2
through up 2 but not including 7.
and now we can also tell it how many
columns or which columns that we want
so here you can see that ilook takes up
two arguments first which rows we want
and then second which columns we want so
we've told it which rows we want let's
grab the columns at index 2
through 4.
so this should grab us the same rows but
only the columns at indices 2 through 4.
so let's go ahead and run that and you
can see now we have all the same rows
but we only have species and island
because we have indices two and three
remember it's up to but not including
the second number
and just since it's a little complicated
let's do that again so we can say df4
dot ilok remember i and i look is
integer or index so we're going to give
it numbers and i'm going to say let's
say i want rows 10
through 21 remember it's rows comma
column so rows 10 to 21
and columns at index four
to seven
so you can see we've just grabbed a
different sub
data frame from our big data frame
now let's do some filtering of our data
so in the previous examples we've known
exactly which rows and columns that we
wanted to pull either by the name of the
column or by the index of our rows or
but sometimes we know something about
the rows that we want to pull without
actually knowing which rows those are
so for instance there are multiple
islands in this data that the penguins
come from but let's say just for now i'm
only interested in looking at the
penguins from the torgersen island well
i don't know the indices of those rows
at least not off the top of my head but
i do know the condition they need to
meet in order for me to select them
so to do that we are going to filter our
data frame
to do so we are going to use boolean
vectors so i'm going to create a
variable called torg which is going to
store booleans indicating which rows
that we would like to grab it's going to
be a true for rows that we would like to
grab and false for rows we don't want to
and the way that we're going to get
these booleans is we're going to pull
the column of island so i'm going to say
df4
island
and then i'm going to use a boolean
operator in this case the equals
operator to check whether the different
elements in this column df island
is actually torguson or not so i'm going
to say equals to tour gerson
and
let's actually print that out so you can
just see what that looks like so again
we're grabbing the column island and for
each of those values we're checking
whether or not it's equal to torgason so
when i print this out you can see that i
get a vector of boolean values again
true when the penguin is from the island
torgerson and false otherwise
now that we have this vector or rather
array of booleans we can use that to
subset our data frame
to do that i'm going to say
capital
dot lok now this looks a lot like our
ilok function but unlike ilok which
remember i is index or integer we can
put things like booleans or strings so
i'm going to say df.loc
torg
and then i'm going to grab the head of
that because i don't want all the values
to print out
and you can see that we have now grabbed
only penguins that are from the island
torguson and we can do that with other
columns too so for instance let's make a
variable called adele and we'll say df4
square bracket species so now i want to
filter based on species
and let's say i only want the adelie
penguins so i'm going to say okay this
column species which of them are equal
to a deli
now again i'm going to have a boolean
array so let's print that out you can
see a bunch of trues and falses
and again i'm going to say df4 dot loc
again not iloc but loc
and we're going to say adele
then let's print the head of it
and you can see that we've grabbed only
penguins that are of the adelie species
and we don't just have to filter on
categorical variables we can also filter
on numeric ones so i'm going to create
this variable called short boy because i
want to grab the penguins that have
short bill links so i'm going to say df4
bill
mm and again i'm grabbing that name of
the column and then i want to check if
their bill length is less than
39 millimeters
so once i do that short boy
i am again going to get an array of
booleans true when the penguin's bill
length is shorter than 39 and false
otherwise
and then i can use my trusty.loc
and say short boy
and there you go we have a data frame
with just the short build penguins
but if that isn't enough let's filter on
more than one thing so i'm going to copy
and paste our short boy
variable and i'm going to create a new
one called short boy 2 because instead
of just filtering on the length of our
penguin bills i also want to filter on
the depth
so i want two conditions i want their
bill length to be less than 39 and i
want their build depth to be less than
18.
to do that we can use uh the operator
and and i'm going to explain these
parentheses in a second but what we're
going to do is we're going to use the
ampersand operator and we're going to
put on one side one of our boolean
conditions and on the other the other so
we're going to say df4 dot bill
mm is less than 18.
so notice on either side of this
ampersand there are fully fledged
boolean comparisons right we have a
boolean operator on this side and a
boolean operator on this side the reason
we wrap them in parentheses is so that
it knows how to actually process this it
says process this boolean thing on the
left side and this boolean thing on the
right side
now these individually will both give us
true and falses
with the ampersand operator we're
actually going to get true when both of
them are true and false otherwise
whether both of them are false or just
one so in essence we're going to get a
true or false array where it's only true
when the penguin has both a bill length
less than 39 and a build depth less than
so let's just print that out short boy
too
and you can see that's exactly what it
gives us so we can pull out our
trusty.loc
short boy2
and here we have a list of only the
penguins who have short bill lengths and
short build depths now dot lock like i
look can give you both rows and columns
that you want
so let's go ahead and say df4 dot lock
and we're going to give it that short
boy 2 so that's saying which rows we
want the penguins who have short bills
but we can also look at a specific
column or columns
so here let's grab the body mass so i'm
going to say the name of the column
because again dot loc unlike i look can
take booleans it can take strings all
sorts of things so we're going to say
body
mass in grams
and then i'm going to grab the mean
actually let me show you that first so
i'm going to run this
and you can see we've grabbed all the
body masses of the short build boys
now i want the average or the mean so
i'm going to use that dot mean function
and when i run that we're going to get
the mean body weight for all of the
penguins that qualify as those short
build penguins finally sometimes when we
want to check two conditions we don't
want and we want or
so let's go up and scroll up and let's
copy
our code for checking for short build
penguins where they have a length less
than 39 millimeters and a depth less
than 18.
now let's say instead of and we want ore
so penguins whose bills are either short
in length or short in depth instead of
an ampersand the only thing that we're
going to change is we're going to use
the pipe operator which represents or so
if the left side is true or the right
side is true then we're going to grab
that
row so let's go ahead and run that and
you can see we now have a lot more
penguins because we have any of them
that have short build links or short
build depths
sometimes we may also want to grab data
based on the type of each column one
common thing that we might want to do is
grab all of the numeric columns
so here we've seen this before with dot
info where you can see the data type of
each column so say we want to grab any
of these columns that have numeric data
types
well i happen to have a list of all the
different numeric data types that could
happen in this data frame and we're
going to use that to filter which
columns we select
to do this i'm going to create a new
data frame i'm going to call it df4 dot
numeric
and we are going to say the name of the
data frame dot select
d
this is basically a function that allows
us to choose columns based on what type
they are
so we're going to say include
equals num and again num is that list of
all of the types that we would like to
include
so then let's print it out
and you can see now we've gotten rid of
any of the columns that are non-numeric
like the island or the species of the
penguin
sometimes we might alternatively want to
grab columns by their name i'm just
going to show you one example of this so
i'm going to say bill
vars equals and i'm going to use list
comprehension to do this so i'm going to
say column
name
for column
name in
df4 dot columns remember that's going to
grab us a list of the different columns
if
name dot starts
with
now starts with is just a string method
you may have covered it in cpsc230
and it's going to check if that string
starts with the string bill
in our case we know that we'll grab bill
length and build depth because both of
those column names start with the string
bill so let's go ahead and check our
work so we'll say bill vars
oh and it should be columns
okay so we're going to go ahead and see
okay this gives us a list of all of the
column names that start with bill
well if we want to use that to then
subset our data frame we can say df4
square bracket
and inside put that list of the multiple
columns that we want to select
when we run this you can see it'll give
us a smaller data frame with all of the
rows but only the two columns that start
with bill
now there are different string methods
you can use but this is just an example
of how you can use the strings of the
names in order to select your columns
so far we've just been working with the
data as it was given to us but we can
also manipulate the data for instance
let's say that we want to add a new
column to our data frame for instance i
want to look at the ratio between the
bill length and the build depth of our
penguins so i want to add a column
called bill ratio now this is really
similar to adding an entry to a
dictionary we're going to say the name
of our data frame square brackets and
then the name of the column
whatever we want to call it so i'm going
to call mine bill ratio and then we set
it equal to
whatever we want that column to be equal
to in this case i want it to be df4
uh square bracket
and then divided by df4 square brackets
oops bill depth
so this is going to take the first
column divided by the second column and
reassign all of this to our column bill
ratio which will now give us the ratio
for each penguin of their build length
to their build depth so let's go ahead
and say df4
dot head
and you can see now that we've run this
we have a new column called bill of
ratio all right enough pandas let's talk
a little bit about numpy now numpy is a
python package that allows us to work
with arrays and arrays are pretty
similar to lists but they're different
in a couple key ways
first let's create an array and i've
done this using np.array
and then a list which we're very used to
from cpsc230
so i ran this and now we have a list and
an array that have the same numbers now
in some ways they're very similar so for
instance we know if we say x list square
bracket 2 that'll give us the item at
index 2 from the list which means 0 1 2
the number 3.
and when we run this you can see that's
exactly what it is well similarly we can
say x which is our array square bracket
2 and that will give us the item at
index 2 which is again 0 1 2 the number
3.
now here's where things get a little bit
different let's try some multiplication
now if you go back and remember back
from 230 when we multiply a list like
this by an integer like 3 what's going
to happen well
it's actually concatenation we take
three copies of the list and squish them
or concatenate them together in a single
list
however that's not quite what happens
with arrays
with arrays when we say
x times three instead of getting three
copies concatenated together we get each
element of the list multiplied by three
arrays do a lot of element-wise
operations which means when we multiply
by three it goes to each element in the
array and multiplies it by three
now i'm going to define
two new well one new list in one new
array
and let's look at what happens with
addition
so again from 230 we know when we add
two lists so x list plus y list
we get the concatenation of those two
lists so addition is concatenation so it
makes sense that multiplication was also
concatenation where we take those lists
and we shove them together
all right future chelsea here i made a
mistake when recording so i'm inserting
this clip instead so we just talked
about the fact that when we have two
lists we get concatenation when we add
them together meaning that we take the
elements in list x list and the elements
in list y list and we shove them
together in a single list
with arrays however just like with
multiplication we do element-wise
operations this means that for each
element in x and y
we add the elements together so for
instance the elements at index 0 we take
those from x and those from y add them
together
elements at index 1 in x and y add them
together etc and you can see those
results here
so arrays and lists are a little bit
different and i want to give you an
example of why arrays are so useful
so let's calculate a distance we know
that the distance between two vectors
when we're using something like
euclidean distance is calculated using
this formula
basically for the vector x and the
vector y we take each axis however many
there are and we say what is the x
position minus the y position square
that add all of those up together and
then take the square root of the whole
thing
well we'd have to use a lot of like for
loops and stuff if we had these vectors
as lists but if we have them as arrays
it's a lot simpler
so the first step is that we need to
take the difference between every
element in x and every element in y
to do that because these are arrays i'm
just going to say x minus y and that's
going to give us the difference so those
x minus y's for all of these for each
element in the arrays
the next step after we have that
difference is to square those
differences so we're going to say
x minus y
squared
and when we do that you can see it's
just the results from here
squared because it did element wise
taking it to the power of 2. now that we
have that we actually want to take the
sum of all of these to do that we're
going to use the np.sum function and say
so this is taking all of these numbers
adding them together and giving that
result to us
and then finally the last step is going
to be to have a square root so actually
let me copy and paste it this time we
can then say
mp.sqrt for square root and just take
the square root of this number
and this
5.567 etc is the distance between
these x and y vectors
now that was all for demonstration
purposes to show you why element-wise
operations that we get with arrays are
really useful but i just want to really
quickly show you a shortcut for
calculating distances because we will do
that a lot in this class so just
remember this for when this comes up
next time we're going to say np numpy
dot lin aug norm
excuse me dot linal
dot norm
and we're going to say x minus y so the
np.linaug.norm
function actually does all of these
steps these three for us so when we give
it x minus y it does the rest of the
calculations for us so again just
remember you can use
np.lynag.norm to calculate distances you
just give it the first vector minus the
second vector and it'll calculate the
distance between them so when we run
this you can see we get the exact same
number because we basically just did the
exact same operation
all right last but not least i promise
i'm almost done so numpy also has a sub
module called random and it's incredibly
useful for generating random data
so i'm going to say mp.random.normal
and i'm going to say loc which is the
center of the normal distribution we're
going to sample from equals 0 scale
equals one and again that's the standard
deviation of the normal distribution and
then we're going to say size equals 10
which means we want to sample 10 random
values from a normal distribution with a
mean of 0 and a standard deviation of 1.
so when i run this you can see that we
get 10 numbers randomly sampled and if i
run this again we're going to get
different numbers because this is
randomly generating the numbers it'll be
different every time but i can also
sample from different distributions so
for instance i can say np dot random dot
uniform
and this will sample from a flat or a
uniform distribution so here we're going
to say the low point let's say 0 so this
is the minimum
high equals 1 that's the upper bound
and then again size equals 10.
so we run that we are going to get 10
randomly sampled values from a uniform
distribution between 0 and 1.
now the next one is actually super
useful so say i need to pick on one of
you in class
i can use the
np.random.choice function so i'm going
to say
mp.random.choice
and it takes in an argument that
contains a collection like a list of
items to choose from so here i'm going
to say names and then i'm going to
run
so you can see this selected a single
name from that list for me and every
time i run it it's going to give me a
different name
now we can use the same function to
generate some data for us so for
instance say we're playing a game in
this class and i need to separate you
into two groups group a and group b i
can actually use the random choice
function to do that so i'm going to say
and i'm going to say okay choose from
either group a
or group b
but now i'm going to say choose and
we'll say let's pretend there's 100 of
you so size equals 100 and then replace
equals true
this replace argument basically tells
the function that if you select a you
can select a again if you select b you
can select b again
and so when we run this you can see that
we get a hundred random assignments so
for instance if we were playing this
game i could be like a b b b and assign
u2 group that way now i mentioned that
because these are randomly drawn every
time we run them we're going to get
slightly different values however
sometimes we don't want that to happen
we want to get the same values maybe you
want to make sure you're getting the
same values as me
we can do that using the np.random
dot seed function
and inside this function we put
any integer that we want people often
use things like 42 or 1234 or 8675309
any integer is totally fine and if
someone else puts that seed in their
computer that means that the randomly
generated numbers that you get are going
to be the same as the ones that they get
so for instance let's say 1 2
3 9 3 is going to be our seed and i'm
going to say mp.random
dot normal
i spell it right yep
0 1 1 so again this is going to be a
mean of 0 a standard deviation of 1 and
only one draw from that distribution
and when i run this i get this number
and if you used the same random seed
that i did you should also get that
number so again the
np.random.seed function allows us to
generate random results that are
reproducible across different people
across different notebooks or different
environments alright that's all that i
have for you i will see you next time

 
hello and welcome to
your second major part of your linear
regression lecture
so today i want to talk a little bit
about bias
and variance which are things that get
data scientists
very excited if you talk to them about
it so
before i do that i want to go over a
couple of concepts
the first being the difference between
data we have now
and future data because when you think
about building a predictive model
we of course want it to perform well on
the data that we have
but typically we're designing these
models with the idea in mind that it
will
also be able to perform well on future
data
data that we've never seen before and we
can think of these two
sets of data as just data we've seen and
data we haven't seen they're coming from
the same population from the same source
the only difference at least we'll
assume right now is that
one is a random sample and another is
another same random sample
when we fit models to our data we try
and pick a model that fits as
best as we can but of course typically
that still means that the model is
incorrect
some time so you can see for example
this model that we have where we fit a
line or
linear regression to the relationship
between income
and happiness and while it does a good
job of describing the relationship
it's not perfect we
do have the option technically of
fitting a
much more complex model to the data
and this actually describes it much
better
in the sense that there's almost no
error in this model
it's perfectly able to predict every
single data point
in our data set and if you were just to
compare these two
on the data that we have now it would
seem like the more complicated model on
the left is doing
a lot better there's virtually no error
but we also want to think about data
that we may have
in the future this yellow sample
represents data from the same population
as the blue sample
but data we've never seen and data that
we didn't use to actually
train our model when you look at the two
models that we fit
to the blue data the data that we have
now you can see
that they're not stable in the sense
that on the left hand side with our more
complicated model
that had virtually zero error in our
sample data
now actually has a ton of data a ton of
error on data that we've never seen
before
whereas on the right hand side you can
see that the simpler model
does have a similar amount of error
compared to
when we were looking at data from our
sample that was used to train the model
if we focus on the more complex model
this illustrates really well the concept
of variance this model has a
very high variability of what the error
will be like between the training set
data that was used to build the model
and a testing set which is essentially
data that we've never seen before
on the other hand we also have the
concept well illustrated by the right
hand side
called bias and bias essentially means
that we have a model that is very
simple and that is unable to perfectly
capture
the relationship between our two
variables if we scroll up to this first
slide where we saw the data we have now
and the supposed future data on the same
plot you can see
that a relationship between these two
might be best modeled by this
curved line here and no matter how hard
we try
that really simple linear model is not
going to be able to
accurately represent the relationship
between
income and happiness this is bias
it's unable to really represent the true
relationship between the variables
when we're building good models we want
to have a balance between
bias and variance we don't want models
that are
so simple that they can't really capture
the true relationship between the
variables that we're modeling
but on the flip side we also don't want
to have models that are so complex
that they do what we call over fit like
we saw in the previous example where the
model is so complex and perfectly fit to
the data we trained it on
that it doesn't perform as well on data
that we tested on
data that's not been seen before and
wasn't used to
train the model the bias variance
trade-off as you might hear it referred
to
basically operates on this idea that
there's a sweet spot in the middle where
we have models that are not
too simple that they can't capture
relationships but are not so complex
that they're prone to overfitting
and by the way when we have something
that's called underfit
that refers to those high bias models
where they're really not able to capture
the relationship
so we're looking for a balance between
an underfit and an overfit model
hopefully that's something that's
perfectly fit throughout the course
we're going to try
and do a couple things to make sure that
we have a model that has a good balance
of bias and variance but for now what
we're going to do
is learn about some methods to see
whether we're having this high variance
model thing happening
if we want our models to do well on data
that it's never seen before
then we should really make sure that
it's doing well
on data we've never seen before but the
problem is is that we're not time
travelers and we can't actually go
grab future unseen data when we're
training our models so
we have to come up with a way to
estimate how
well our model will do on unseen data
without actually being able to reach
into the future and grab that data
ourselves
the way that we do that is with types of
model validation
the first thing we can do is something
called
a train test split and essentially the
concept with
all of these techniques we're going to
talk about is that you take the data
that you do have
and you section off part of it to
pretend that it's future data and you
don't use it to train your model so
it basically acts as unseen future data
because it wasn't used to train your
model
the simplest way to do this is to just
make
one split and take part of your data set
and
put it away and don't look at it until
you're done building your model
once you've built the model using the
rest of the data that's available to
train the model
you'll use that separate sectioned off
data set we call it the testing set
in order to evaluate how well your model
does on unseen data
if there's a really huge difference
between how well the model performs on
your training data
versus your testing data that indicates
that you maybe have an overfit
model it's a little too attuned to the
one data set that it was fit to
and doesn't generalize well to unseen
so going back to the train test split
essentially what happens is just what i
said
you choose a percentage of your data
that you want to withhold for your
testing set
and you don't touch it look at it or
consider it until
you have built your model so you can see
like multiple percentages that are used
by different people
often splits are something like 80 in
the training 20
left over for testing maybe 90 10 or 70
30. what you need to think about when
making a split for your trained test
split
is one do i have enough data in my
training set
to accurately build a model if you only
have
one data point in your training set it's
probably not enough to build a good
similarly you want to make sure that you
have enough data in your testing set to
get an
accurate measure of how well your model
is doing on unseen data
it's really important to make sure you
balance those two things and
typically people use things like 80 20
in order to do so
but it really depends on the size of
your data set and the complexity of your
model so again
just to review in a trained test split
what we're doing is we're taking the
sample of data we have now
and we're making a split you can say
okay i'm going to use
80 of it to train my model and i'm going
to use the remaining
portion when i'm done training my model
to guesstimate how
well it'll do on unseen data one problem
that we can have
when doing a simple train test split
even though
technically it is helping us achieve our
goal of estimating how well our model
performs on unseen data
is that we're sort of throwing away
information
we're only ever using our test set as a
way to estimate we're never
letting that information be a part of
the model building process
so enter cross-fold validation in fact
we call this k-fold cross-validation
often
k is just a variable which can be
something like 5 or 10
and it represents the number of groups
equal size
groups that you'll split your data into
so if we just look at
one of these columns right here you can
see that
similarly to before we've taken our data
and we've split it into chunks we split
it into
10 chunks in this case so we're doing 10
10-fold cross-validation
and what we do is we take k minus one so
in this case
nine of those chunks and we use them to
train our model
and then we use the last remaining chunk
or fold
to estimate how well our model does on
unseen data
but the beauty of k-fold
cross-validation is that we repeat this
process
k times each time having one of our
folds be
the test set which means that over the
course of the k
models that we build every single data
point at some point in this process
gets to be a part of the testing set
which
hopefully gives us a little bit more of
an accurate measure of how well the
model is doing
and what we'll do often in k fold across
validation is
we'll take the error or the measure of
model fit like in our case of linear
regression and r squared or a mean
squared error
we'll take them from all the k tests
that we've done
and average them together to see how
well this type of model
is doing in general across all the k
models
now this is a really great way to make
sure that we're using all of the
information in our model effectively
however it is computationally expensive
because instead of fitting one model
you're fitting k models
and in the case of linear regression or
some of the simpler models that we'll
use in this class
that doesn't really matter it is not a
noticeable
time difference or memory load on your
computer
but as you get to more complicated
models that take a lot longer to run
it is something that you need to
consider when deciding how to validate
your model
can i really wait for k of these models
to fit
if your model takes one second or even
five minutes to fit
no big deal but when you get to larger
models maybe even like a neural network
you're probably not going to want to
wait days just in order to do k-fold
cross-validation
so it is important to keep that in mind
when choosing your model validation
metric
when you take k-fold cross-validation to
its nth degree
you get something called leave one out
and this is essentially the case where
we're doing kfl cross validation but
each fold is equal to only
one data point which means that we use
all but one data point to train our
model and then we
use uh the one remaining data point to
test
how effective that model is whether it's
how close it is to the predicted value
or whether the correct
category was predicted now
this is a really effective way to build
hopefully very accurate models because
you're using all but one of your data
points to build each model
but the problem with this is often
computational efficiency
as i said it's not really that efficient
fit one model per data point
in your data set and again with simple
models like ours it really doesn't
matter but
when you're starting to think about okay
which measure of
model validation would i like to use
then you do need to consider
like how many data points do i have and
do i have enough time
slash computational resources to fit
that many models
i will say that oftentimes the models we
it's not going to be a problem but just
something to think about
so again just to review leave one out
is just k-fold cross-validation where
each fold instead of being a group of
data points is just one data point
now a question that i get a lot is how
do you decide
i just gave you three ways of doing
model validation and you're
probably going to only want to do one of
them well the two things that you really
need to think about are how big is your
set and what is the computational
expense of your model
in this class it's probably going to be
a toss-up i would say that often you're
going to do something like k-fold or
leave one out because you don't have
that much data and so computational
expense of your model is not really
important
but like i said as you go on to build
bigger better and more complicated
the amount of time it takes to fit a
model is going to be really important
and maybe the deciding factor between
choosing between leave one out
and k fold or even k-fold and just doing
a simple train test split
you also want to make sure that you have
enough data if you're gonna do something
like a train test split
to both fit your model using the
training set and
evaluate your model using the testing
set there's no hard rules about what you
should do and when
but when you're selecting a model
validation those are the kinds of things
that i would like to hear from you and
your reasoning as to why you selected
of the three methods awesome i will see
you for your next
python lecture

 
hello and welcome to your linear
regression 2 in python lecture so of
course first what i'm going to do
run all of our imports
and then load our data
so in the conceptual lecture we talked a
lot about model validation which is
basically a way to see how our models
might do
on unseen data so we're going to explore
that a little more we have some more
spotify data a little different than the
beyonce just different artists
and let's just really quickly look at
how big this data is so we can do that
using the name of the data frame
which is m
dot shape
so this data has 2
53 rows and 14 variables so keep in mind
that number of rows as we move through
some of these exercises
so the first type of validation that we
talked about is a train test split and
this is the most conceptually and
computationally simple method of model
validation that we talked about
basically what you do is you just take a
little portion of your data put it to
the side and don't look at it and that
is your test set the rest of the data we
basically treat like we've treated all
of our data before we can use it to
z-score we can use it to
like train our models etc and then once
we're done with that whole process the
test set can be used in order for us to
see how our model might do on data it's
never seen before so i'm going to use
the following variables in order to
predict danceability in our data set
so what i'm going to do is i'm going to
first do our train test split
so remember train test split outputs
four things it outputs the training set
predictors the testing set predictors
the training set outcome and the testing
set outcome so we usually put these four
things on the left hand side and then we
use the train test split
function from sklearn now remember we
have to
tell train test split at least three
things
first we have to tell it what the
predictors are so in our case we're
going to grab from the data set m we're
going to say m bracket predictors
because this is going to grab the
predictor columns from our data frame
then we need to tell it the outcome so
in this case it's going to be m dance
ability
um so it's the thing what we're
predicting and then the last thing that
we want to tell it is what kind of split
do we want to do so in this case let's
do an 80 20 split that means 80 of our
data is going to be in the training set
and 20 is going to be in the testing set
to do that we can say test size oops
underscore size equals 0.2
okay so when we do this this is going to
create our train and our test set for us
and let's just double check that it did
what we wanted so first let's take our x
train and x test and print their shapes
so let's run okay so we know that
originally we had about
2500 rows and now that we've split with
20 in the test set 80 in the training
set our training set is about 2 000 rows
and our testing set is about 500 and if
you do the math in your head that seems
about right
both of them however have all 10 columns
because they need them
all right let's just check what these
look like so we'll say x train dot head
run okay so this is what our training
data looks like and it looks exactly
like our data from above but obviously
only the columns that we care about now
all right so just like we did in the
last uh linear regression python lecture
we are going to create a model so i'm
going to say model equals
linear regression
and i am going to fit that model i'm
going to say model
dot
fit
x train
y train again because we need to give it
predictors and then actual values
so i'm going to go ahead and run that
and then as usual as we've been doing
this entire time we can grab predictions
from this model so for instance i'm
going to say why pred equals model dot
predict x
train
um so this is giving us and actually
let's just put
this out
this is giving us the predictions that
the model makes for the training set
but now we have a testing set and often
we're really interested in knowing how
our data does or excuse me how our model
does on unseen data because remember
we've learned that it's possible for the
model to do really well on data it's
seen before and really poorly on data
it's never seen before so i'm actually
going to change this to x test so now
these are the predictions for our test
set
so now we have our predictions we can
also calculate different metrics for how
our model is doing in this case let's
look at r squared now we've used r
squared with r2 score before um also you
can do something and i'm going to warn
you to be careful but you can also do
this we can say model the name of our
model dot score
and give it x
test
y
test so this is actually going to
calculate your r squared for u as well
and it's going to take in the predictors
and the predicted value when we run this
we can see that our r squared is about
uh
30.6 percent
now the reason i want you to be careful
is the dot score function is super easy
super useful but as we learn more
algorithms going throughout the class
dot score returns a different metric
for every single model and so you need
to make sure that if you're using dot
score that you know for sure what metric
it's using is it using mean squared
error mean absolute error is it using r
squared is it using in the future when
we learn different models accuracy or
precision or something like that you
want to make sure you know what dot
score is calculating because depending
on what type of model you have it'll
calculate something different
my recommendation is if you're ever not
sure just ask for what you want use r2
underscore score or use mean squared
error or whatever you want because you
know for certain you're going to get the
right thing in that case whereas dot
score is going to change depending on
the model
so either make sure you look it up or
just ask for what you want in this case
i know that this is calculating r
squared so i'm going to use it to
calculate r squared and this is my test
r squared let's look at our training set
r squared we can say model dot score x
y train
and run okay so interestingly our model
is doing better on the training data
than it is on the testing data that
means that it might be a little bit over
fit right it's doing a little bit better
on data it's seen than data it hasn't
seen before
another way that we can assess our model
a little bit is by making the true
versus predicted graph so i'm going to
call this true versus pred equals pd
data
frame um and i am going to store the
predicted values
oops there we go
y pride and then the actual values
or i guess i can just call this true
um which would be stored in y
okay so let's look at what this data
frame looks like first versus
pred
so you can see we basically have a data
frame of the predicted value versus what
the true value is and we can use this to
plot and basically see how off our data
is so for instance we can say ggplot
um
what do we call this true versus pred
and then aes x equals true
y equals pred
predicted is what i called it
and then plus jam
point
and run
so you can see in this case uh we
actually have quite a spread of errors
but we see this roughly linear trend
right on average our model is doing
pretty okay but there's a pretty wide
deviation right we might be pretty off
from what the actual value is as is
shown by like how spread out these
points are but at least we see some
relationship which is reflected in the
fact that our r squared is 0.37 it's not
like zero
so now we've built a model using train
test split let's really quickly look at
k-fold so remember k-fold
cross-validation is similar to train
test split but we're doing it multiple
times in k fold we split our data into k
groups and then we build k models each
time we build a model a different one of
our groups or folds is going to be the
test set and the rest of it is going to
be the training set again this has the
advantage of at some point every single
data point is going to be in our test
set and in our training set so we're not
really like ignoring any of our data so
to set up a k-fold the first thing we
need to do is set up a k-fold object
this is basically going to be
responsible for splitting our data for
us so we're going to say kf equals and
then in sklearn it's k-fold
and then it takes um two arguments that
we want to
use all the time
one is n splits which is going to say
how many folds or groups or splits do we
want let's do 10 here because we have a
lot of data we have like what 2500
points so 10 group should be fine
um and then the other thing that we want
to just turn on is shuffle equals true
so basically when we're doing k-fold by
itself we want to make sure that it like
randomizes the rows before it splits it
into the groups because sometimes and
you never know especially if it's not
your data you never know when someone's
like accidentally listed like all of the
really high values first and all the
really low values later um and so
shuffle just randomizes the data before
it splits it so now that we have our
k-fold object let's just
run that
let's make a little setup so we need to
like with train test split have our x
and our y
so our x data is just going to be the
predictor variables like usual so we'll
say m
predictors
and we'll say y m
dance the ability
now because we're building multiple
models we're going to want some
lists to store things in so we're going
to build 10 models because we're doing
10-fold cross-validation so i'm going to
create an mse list which is going to
store the mean squared error for each of
our models and then i'm going to create
an r squared list which is going to
store the r squared for each of our 10
models
last but not least we need to prepare a
model
so we're going to say the name of the
model a linear regression
and this is just creating an empty model
for us to use next all right so let's
run this
now is the big part of k-fold so the
k-fold object is basically going to
return the indices of which rows are in
our training and our test set so what we
need to do is loop through those right
we're going to have 10 different models
and this for loop is going to run each
of those 10 models so we're going to say
for train comma test in kf oops kf dot
split
x so we're going to give it our x data
and then inside this loop it's basically
going to be everything that we did with
our train test split
but just inside a for loop because we
want it to happen 10 times one for each
fold in our data
so the first thing that we need to do is
we need to create our train and test
data sets right so we know which indices
should be in the train and test set but
we haven't actually stored them yet so
i'm going to copy and paste this here
and then i'm going to walk through this
through you
so we have our x train set which we're
going to from x grab using iloc the
training data
x test similarly is going to be from x
grab the test data
y train is going to similarly be the
training data from y and y test is going
to be the testing data from y
so now that we have our data split we
can build our model or train it
so we already built a model up here so
we're going to use that we're going to
say lr.fit and again we need the data
we're going to train it on and the
outcome we're going to train it on so we
don't want to use our test set at all
here so we're going to say x train y
train because we're training our model
and we only want to use the training set
now that we've trained our model we can
grab our metrics so for instance i'm
going to say um
mean
squared
error
and again mean squared error takes the
actual values so this is y
test so just
hold on with me for a second and then
we're going to get the predicted values
which is going to be
dot predict x test
so notice
i'm using y test and x test here that's
because in this specific instance i'm
interested in just the out of sample
performance i want to know how my model
did on data it's never seen before
however sometimes you might want to grab
both the training and the test set for
instance if we want to look at
overfitting or something like that in
this case i'm just going to do the test
set but in general when you're doing
this any metric that you want to look at
you should be grabbing inside the for
loop so this is computing the mean
squared error and let me copy and paste
that for the r2 score
to score
but i actually want to store them in
these lists right i'm not just going to
have one metric i'm going to have one
per model that i fit so i'm going to say
mse.append so i'm going to append the
current mse to the list msc
and then same for r squared
beautiful so when we run this loop it is
going to do all of this for us 10 times
once for each fold so let's go ahead and
and then once we're done let's look at
our msc
so for instance you can see that for our
10 different models we got various
different mscs ranging from i guess .013
to about 0.17 looks like the highest
now we can look at these and this is
really useful sometimes to see all of
them but sometimes we just want the mean
like the average mean squared error so
we can say mp.mean
msc
and so we can see that on average our
mean squared error was about 0.157
and then same thing for the r squared
scores we can look at all of them yeah
it looks like they range from about 24
to 39-ish
we often want to know the mean so we can
say mp.mean r2
so our average r squared score for the
out of sample performance so the test
set is about 34.8 percent so again not
great not horrible so you can see
there's a couple extra steps to doing a
k fold right we need to actually do a
for loop we need to have these splits
but a lot of what we're doing is very
similar right we're grabbing our
training and test set we're fitting a
model and then we're looking at how the
model did using various metrics
another form of cross validation that we
talked about is leave one outcross
validation which is essentially k-fold
where k is equal to the number of data
points we have so each data point is its
own fold which means we're fitting our
model and all but one data point and
then testing it on that leftover one
the setup looks a lot like kful though
first we're going to create a leave one
out object using leave one out from sk
and then we're gonna have an mse list
which is going to be our
list of mses and then let's just create
a model lr equals linear
regression
now you might be wondering why did i not
have an r squared list here well when
you only have one data point in your
test set you actually can't calculate
the tests that are squared so that's why
i'm leaving it out here so when you do
leave one out you can't calculate r
squared for each
fold because it's just a single data
point now the for loop though is going
to look really similar so i'm actually
going to copy and paste
and scroll down
so this is going to be almost exactly
the same as k-fold except
when we're using our for loop for train
test in kf we're actually going to say
for train test in leave one out dot
everything else is going to be the same
but again we don't need our r squared
here
okay so let's go ahead and run
and notice this is taking a while when
you see a little asterisk connects to
your cell it means that it's running but
it's not done yet
um this is because we're running a lot
of models remember we originally had
2500
at rows which means that when we do
leave one out we're fitting 2500 models
okay it just finished that was in real
time and that wasn't that long it's not
too computationally expensive for a
simple model like linear regression and
a small data set with only 2 500 rows
but just be aware it's totally okay if
it takes a little while it's running a
ton of models
um so now we should have a bunch of mscs
let's actually just print them out
there's gonna be so many
oh my gosh okay so we have so many
different mean squared errors uh let's
just print out the mean of them np dot
mean msc
okay and actually similarly to above our
average mean squared error is about
0.015
when we do things like train test split
or k-fold those are randomly splitting
our data which means that we might get
different splits every time we run it or
if you run this you might get a
different split than i do when i run
this so i just want to point this out
this will become a little more important
later but just so you know
when you do train test split or when you
do k-fold there's an argument you can
use called random state and you can set
it equal to any integer that you want
i've chosen 42 people often use 1 2 3 or
something like that um this is similar
to the numpy.random seed this basically
guarantees that every time you run this
code you're going to get the exact same
split so if you ever have a situation
where that's important or like on your
homework if you think you're going to be
running your code a bunch you can always
add this argument in to make sure that
you always get the same split and then
it's not changing every time you run it
all right let's talk a little bit about
everyone's favorite subject data leakage
in the last lecture or the conceptual
part of the lecture we talked a little
bit about the importance of the test set
as being unseen data we're basically
trying to predict if we get new data in
the future how will our model do on data
that has never been seen before and we
use our test set as kind of a proxy for
that so it's very important that our
test set is truly unseen data we don't
want to leak any information from the
test set into our model and one way that
can happen is through z-scoring so
typically what we've done is if we want
to z-score something say we have our x
and y data we would say z equals
standard
scalar
and then z
dot fit
x
and then something like x c equals z dot
trans
form
now here's where the problem lies
now that we're going to do a train test
split if we were to z-score our data
beforehand and use all of our data then
we're actually leaking information from
the test set into the model because when
we do this and we calculate the means
and the standard deviations we're using
information from every single data point
in x and right now because we haven't
split it yet x contains both our
training set and our test set which
means information from our test set is
leaking into the model because it's
information is being used to calculate
the mean and standard deviation and then
we're using that in our model and this
goes for everything z-score is just
calculating a mean or a standard
deviation basically anything you do with
your data that takes information from
all of your data if you do it before the
train test split or the k fold or the
leave one out you're risking leaking
information from that testing set into
so what we would do instead is not
z-score here so we can go ahead and run
this
what we would do is we would wait to
z-score until after we had split our
so we're going to say z equals standard
scalar this part's the same
and then what we would normally do right
is z dot fit but what are we going to
fit on well we don't want the model to
see the test set at all so instead we're
just going to fit on x train
now we actually don't want to fit a x
train for all of our variables right
because some of these like key and mode
are actually not continuous variables so
i'm going to use this content to just
indicate which variables we want to
z-score so i'm going to say z-score dot
fit x train content
and then like normal we can say xz
uh train
equals z dot transform
content
so what you can see here is that we've
calculated the means and standard
deviations using only the training set
and then we've transformed the training
but we still need our test set to be
z-scored right because if our model is
used to z-score data coming in it has to
have z-score data coming in when we use
the test set too
so what we're going to do is we're going
to say xe test equals z-dot transform
x-test content
so you can see we're basically using
transform for both train and test but we
are only fitting
on train basically you never want to fit
anything on your test set whether it's
your z-score or your model your test set
should really go pretty much untouched
until your model is done being built so
just to review to prevent data leakage
what we want to do is make sure that any
computation that involves all of the
data like a z-score or like taking a
mean or like taking the standard
deviation
is done after we do the train test split
whether that's train test split k-fold
or leave one out once we know what's in
our test set we can make sure to avoid
it and only fit on the training set
and then use transform on both the
training and the test set
so let's go ahead and run this and you
can just see let's actually just print
it out xc train
so you can see we have all of our z
scores they are just now separate for
the train and the test set
so i know i sound like a broken record
but i just want to repeat here it is
really important that if you're doing
any computation z-scoring or otherwise
like taking a mean or standard deviation
or anything like that if that's
happening on all of your data or you're
doing it before you did your train test
split you're likely leaking information
from your test set into your model and
that can give you inaccurate estimates
of how well your model will do on unseen
data because technically the test set
which is supposed to be unseen data has
no longer unseen you've kind of peaked
at it so to speak
and this process looks similar in both k
fold and leave a now i'm just going to
show you k fold because k fold and
levanat are super similar but it works
the same way
so we have our x equals
dot predictors
y equals m
dance ability
and then we're going to build our model
lr
[Music]
regression there we go
um and then we're gonna build with k
fold kf equals k
fold n
splits equals let's do five-fold
cross-validation shuffle equals true
i'll say mse equals empty and r squared
equals empty again remember you can't do
r squared with leave a noun and we're
gonna say for train test in kf dot
split and we're going to split on
x so say x there we go
so everything else is actually going to
look really similar so i'm going to copy
and paste this part from our last
example so we're going to set up our
train and our test set so now that we
have our train and our test set set up
we can safely z-score because we know
what's going to be in our test set so
i'm going to say z equals standard
um and then i'm basically going to do
what i did above here
um the only difference is i'm going to
store them back in their data frame so
i'll show you what you mean so i'm going
to do z dot fit
content right so we're going to only
z-score our continuous variables
and then i've only fit my z-score object
on my training set right the means and
standard deviations that we calculated
only come from the training set but now
that i've done that i can safely
transform both my test set and my train
say z dot
transform x train
so this is transforming them and then
i'm just going to copy and paste that
test content now what i'm going to do a
little bit differently here is i'm
actually going to take these and store
them back in the original x train and x
test right now i'm calculating the
values but i'm not storing them anywhere
say x
equals so it's basically saying and
we've done this before replace the
original raw continuous variables with
their z scored counterparts and then i'm
going to do that again for test x
beautiful so now x train and x test have
our continuous variable z scored and our
non-continuous variables we've just left
them alone
so now we can do what we've done before
which is build our model so we'll say
lr.fit x train y
and then we will do our metrics so we'll
say for instance mse dot append
um lr oh no no we want um
squared error
and then we have our actual values
test and our predicted values which
would be lr.predict x
and again we can let me actually put it
here and then calc your commented out
so say we wanted to also get the
training values here we could make it
like another list like msc train equals
um so for instance we could do that and
then just change these to
um and again that's really helpful for
instance if you're looking to
calculate or like see if things are over
um and then we also want our r squared
so we're gonna say r2 dot append from
this list up here our two dot append r2
score
y test lr dot predict x test
beautiful okay so let's go ahead and run
okay that's all done and so now we've
actually stored our um mse our msc for
the training set and our r squared
scores so we can look at the means of
those so let's see np dot mean mse
um let's run that
okay and mp.mean mse train
so we can see that the mscs are pretty
similar for train and test uh
test is a little higher but like not
concerningly so so i'm not concerned
that there's overfitting here um but
let's look at our r squared mp.r
dot mean r2
beautiful so now we have all of our
metrics so we know that there's not a
ton of overfitting we know that our mean
squared error is approximately 0.157
and our mean squared or excuse me our r
squared is about 35 which means our
model is explaining about 35 percent of
the variance which is certainly better
than 0 which is just guessing the mean
but it's not super close to one so again
our model is doing okay but not great
now again just to review we always want
to make sure that we're z scoring after
we know what's in our test set so for
train test split that means after doing
the actual train test split
and k fold and leave one out that means
it's happening inside the for loop and
we want to make sure that we're always
fitting on the training set and we're
never fitting on the testing set because
we want to make sure the testing set
remains unseen
any computation whether it's z-scoring
or whether you're just taking a mean or
whether you're taking a standard
deviation by itself
any computation that involves all of the
data we want to be very careful when we
have a trained test split or k-fold or
any of these because we want to make
sure that test set is remaining unseen
so just as a rule of thumb
never fit on the test set whether that's
the test set by itself or all of the
data which includes the test set make
sure you're always fitting on just your
training set
alright that's all i have for you i will
see you next time

 
hello and welcome to your second
data visualization lecture now that you
know
the basics of ggplot plot9
i really want to talk about those things
that i mentioned in the very first
lecture
about how to do effective data
visualization
because like i said it's not really
about you
knowing how to make a box plot or
knowing how to get rid of a background
it's about you genuinely thinking about
the message that you're trying to
communicate
and making sure that your graph supports
that
i think it's really important to
remember throughout this entire lecture
that there's no one-size-fits-all rules
i wish i could give you rules that when
you have this type of data for this type
of audience that you have to do
x y and z but it's just so
context dependent that i can't give you
those hard and fast rules
what i can do is walk you through an
example of
all of the different questions that i
think are helpful to ask
when doing effective data visualization
and at least give you a taste of what
that could look like
so first i'm going to load my data
because of course the most important
part of a data visualization is that you
have
data now this is just a review of what i
already had on my slides
these are some questions that i want you
to ask yourself
when you're making data visualizations
the first thing is what is the message
that i'm trying
to communicate now in this specific
graph that i'm going to make we're going
to use the penguin data
and we're going to answer the question
are penguin species
different in bill length let's just go
ahead and make the default gg plot graph
for this
to see what the answer is so as before
we're going to say ggplot
penguins a e s
x equals oops not capital species
y equals bill length
m now as before we actually made a
similar plot in the last lecture we're
going to add geom
uh box plot and hopefully this will help
us
see what the answer to our question and
the thing that we'll be trying to
communicate actually
is beautiful
okay so as we saw in the last lecture
the
chin strap penguins have the highest
bill length with gentoo penguins some to
wear in the middle and again i should
really learn how to pronounce this the
adelaide
adelie penguins with the lowest bill
length
overall now this is just the default
graph that ggplot gave us
let's think about as we go through all
these questions
what are things that we can change or
improve about this graph
to make the answer to our question which
is
which penguins have or like do the
penguins have different build links
more understandable from this graph
now i'm just going to go and copy and
paste what we have here
but i want to say okay well what are
things that we can maybe
add to our graph to make it a little
more understandable
one thing that we looked at before is
adding color
to distinguish the different groups now
i can either do this
in the ggplot function or in the geom
box plot function i'm going to do it in
the in the
general ggplot function but i'm going to
say fill
equals species and this is going to give
a different color for each of our
species
let's go ahead and run this beautiful so
same graph
just now there's color and i think that
the color really helps communicate that
these are different species or different
groups that we're contrasting
because the colors that we're using are
very different we can see that the
species are very different
and that the message that we're trying
to convey is
there are different species and their
build links are different
so adding the color really helps with
next another thing that we could add to
our graph
is a title right now you would have to
be aware of the exact
context of the question we're asking and
the data we're working with
to really grasp what's going on in this
graph
but adding a title can be really helpful
for people who are coming in
who don't know a lot about this data set
or don't even know that we're talking
about penguins
one of the ways that we can add this is
by using
the gigi title function
so we can go ahead and add that and say
gg title
and then give it a title that is
thoughtful and descriptive
and really supports the message that
we're trying to communicate
in this case i'm going to choose
something like do penguins
do peng how about do penguins species
have different bill links
and this really gives the reader of the
an idea about what we're actually doing
oh i forgot to add the fill so we'll say
fill equals species
and let's go ahead and run that
beautiful so now we've added something
to our graph
to help it be a little bit clearer in
terms of the message that we're trying
to send
now let's go ahead and add maybe at
least one more thing
so when we're making our graph the title
really helped but another thing that
could really help
in terms of changing or adding things is
relabeling
our x and y axes right now they're just
named what the column names in our data
frame are
but that's not one pretty and two
something that would be easily
understandable i want someone to be able
to glance at this graph
and understand the message that i'm
trying to convey
and changing the x and y axis labels
will just
really help support that message so
let's go ahead and add them
we can add x and y labels or at least
change x and y labels
by using the labs function and actually
we could have set our title inside the
labs function too
but we'll get to that later for now i'm
going to say
x equals and i'm going to capitalize
species because it kind of helps
draw the eye to that word more and then
in terms of the y-axis
i'm gonna have that say bill length
in millimeters so now let's go ahead and
run this
and you can see again very similar graph
we're just making incremental changes
here
but the x and y axis labels are a lot
more
clear they draw the eye more and i think
they communicate our message better
now one point i really want to make
because i think it is so
important and in my eyes kind of
exciting
is that the context of the audience that
you're using
or that you're talking to really might
change the way that you're effectively
communicating this information
now right now i have this beautiful
graph that has
species as the x-axis label
but let's imagine that i am presenting
to a group of people that is very
familiar with penguins
knows instantly by looking at it that
gen 2 chinstrap
and adoleive are species of penguins
one thing that i could do is get rid of
the x-axis
label and i did that by giving the
x-axis label an empty string
now you can see that we've removed some
ink
from our graph if we're talking to an
audience who
would instantly recognize these words as
species of penguin
we don't necessarily need to use up the
on our graph to tell them that it's a
different species
this is just a really great example that
the rules of data visualization
aren't static it depends on your message
and it depends on who you're presenting
to
because if you're presenting to say a
group of kindergartners who are really
excited to learn about penguins
they're not going to know that gen 2 is
a type of penguin unless you told them
and so you may want to include that
x-axis species label
whereas with experts your message is
actually maybe hindered by that label
because you're adding extra visual
information that
doesn't give them any actual useful
information that supports the message
that you're creating
now that we've talked about adding
things to help make the message that
we're trying to
do more clear let's talk about how we
can remove
things to make our graph more efficient
effective
and clear if you've ever heard that
really silly rule of every time you get
ready you should
look at yourself in the mirror and take
off two accessories that you have on
that's sort of the way that i want you
to think about graphs i
always want you to be asking why is this
element of my graph here
is it useful is it supporting the
message that i'm trying to convey
so let's think about what we can do with
the current graph that we have
in order to make it more effective in
the message that it's communicating
one thing that we already removed is the
species label
let's assume we're presenting to experts
so we don't need that label
taking it away actually made the message
of our graph
a lot more clear at least to that
audience now one of my favorite ways to
start
removing things in order to increase the
clarity and efficiency
of my graph is to add one of those
themes that i was talking about
my absolute favorite to use as a default
is theme
underscore minimal because it just gets
rid of a lot of that visual clutter
that exists in the default gg graphs so
let's go ahead
and add the theme but in essence remove
some visual information from our graph
and see how that improves the clarity
and the effectiveness of the message
that we're trying to communicate which
again just to remind you
is the fact that penguin species have
different build
lengths
awesome i think that's already a huge
improvement
if you see from the graph above there's
a lot less visual clutter
and actually we've increased the
contrast
of the graph before the contrast between
the light gray background and the colors
was a little bit less stark than the
contrast between bright colors and a
white background
i think this really helped improve our
graph and doesn't waste
ink in the sense that we are not putting
visual color or visual information
in a place where it doesn't really serve
any purpose
cool now let's think about what other
things we can get rid of
and to be honest depending on the
context literally
almost any part of your graph is
something that in some context it's
appropriate to get rid of
but i just want to talk about some that
are pretty common at least in the graphs
that i've seen
one thing that we can think about is the
grid lines that we have in the back
now gridlines are sometimes useful it
helps go from the x-axis label excuse me
the y-axis labels and the x-axis labels
and draws across the entire graph so
that we can keep constant track of what
our values
are but in this case on the x-axis at
least
the values are just categories they're
species of penguins and we don't really
need a grid line for that
and so getting rid of the x-axis
grid lines i think will actually add
clarity to our graph
because instead of having to look at
lines that don't actually give us any
information
we'll just have white space or less
visual clutter
the way that we do this is with the
theme function and this is slightly
different than the theme
minimal that we just added but we'll say
plus theme
and then we'll say panel underscore
grid under score major
underscore x equals element
underscore blank now let me explain
every part of that to you just so that
you understand but
i am kind of operating on the assumption
that you're going to have to look up
some of the ggplot plot 9 documentation
when you're building your own graphs
but to give you an idea of what this is
saying it's saying
panel which is like the background of
the graph the grid which is our grid
lines
the major uh grid lines so the bigger
thicker ones and on the x-axis
so it's saying let's get rid of them
element blank means get rid of them i
don't want to see them
so when we do this we're basically
saying get rid of all the major grid
lines in our graph
so let's go ahead and see what that
looks like
oh beautiful i think it looks much
better already
in fact i think that that really cleared
up some of the visual space so that you
can just focus on the message which is
different species of penguins have
different buildings
let's think about what else we could
remove here is a time
when you really have to focus on the
message because
different messages require different
elements
of your graph in this case let's pretend
that the exact message that i want to
is just the penguins have different
buildings
i don't necessarily care that my
audience knows exactly in millimeters
what the different buildings are for
each species
i just want them to understand that
between the species there are
differences in
bill length so i can actually remove a
lot of different things from my graph
maybe the first and most obvious one is
this legend that says species over here
since we have x-axis labels we don't
actually
need this legend here it's communicating
redundant information
that no one actually needs to see twice
and it's just taking up ink so let's go
ahead and remove that
i'm going to copy the plot that we have
from before and then show you how to do
okay so the way that we're going to do
this and actually
a lot of what we're going to do in this
section of the graph
is we're going to go back to the theme
function
and we're going to say legend underscore
position
equals the string none which essentially
tells ggplot that i don't want to see
this legend over here
ugh better already i think it's less
clutter
less visual information and i'm not
actually losing
any information that legend didn't
really tell me anything that i didn't
already know
now the fact that i only want people to
get the message that the different
penguin species have different buildings
means that i can actually probably get
rid of my
y-axis gridlines as well now i want to
caution you
don't get rid of everything if it's
important to you
that your audience knows what the
approximate median
or range of billings is for each species
you absolutely should leave these grid
lines in
and leave the labels of 40 50 and 60
millimeters in
because it's important and it's a way
that you're communicating your specific
message
but my message is just that different
species have different build links
so i don't actually care that my
audience knows exactly what that bill
length is
so i'm going to go ahead and get rid of
both the major and they're a little bit
faint so i don't know if you can see
them but the minor
grid lines and i'm actually also going
to get rid of these numbers on the left
hand side
since i don't care what the actual value
is i just want to
visually show that there's a difference
they're not really necessary here
and this is the point at which it gets
kind of hard
i want to make a really good graph and i
want it to be effective
but i have to be really careful that i'm
not removing things that actually
would support the message i'm trying to
communicate in this case i've really
thought
deeply about it and i realized all i
care is there is a difference
not exactly what the values are so i'm
going to go ahead and get rid of that
the way that i'm going to do that is
really similar to how i got rid of the
other grid lines so i'm actually just
going to copy and paste that
twice and instead of major i'm going to
put
minor because i also want to get rid of
the minor y
axis and then i'm going to get rid of
this x
change that to a y so if you review here
this is saying
get rid of the major x the minor y
and the major y grid lines
similarly i want to get rid of these
labels that i have and the way that i do
is by saying axis underscore text
underscore y so that's saying the text
on the axis of the y-axis
get rid of it so i'm going to say
another element underscore
blank okay so now i've gotten rid of
so many things in my graph let's see how
it looks
ah beautiful i love this we're using
so much less ink than we were previously
in fact if i just scroll up you can see
how visually cluttered our earlier
graphs were
compared to this tapered down very
tailored graph
that specifically communicates the
message that we want to communicate
which is there's a difference between
the species in terms of buildings
okay so now that we've added and removed
things in order to make our message very
clear
let's think a little bit about
accessibility
now accessibility comes in so many
different shapes and sizes and
all of the different ways that you can
make your graph accessible
are incredibly important i can't cover
all of them here
i'm sure there's even ones that i'm not
aware of but i want to cover some common
ones that you can always be asking
yourself
to make sure that your graphs are
accessible
let's go ahead and copy the graph we
made before
and let's think about people who are
color blind
now the color blind uh often have
trouble distinguishing between different
colors there's actually different types
of color blindness so
you might want to be cognizant of that
but i want to show you
this color blind friendly palette that
someone made
and if you google color blind friendly
color
palette so many different options will
pop up
that people have research-based chosen
colors
that people with different types of
color blindness can distinguish
now this is a really great color palette
because the difference between all the
is more easily seen by people who have
colorblindness
so while the colors that i used are
actually really bright and easy to
distinguish at least for us
i want to make sure that my palette that
i'm using is colorblind
friendly so i'm going to go ahead and
grab these
hex strings and copy them
into my code so that i can use those
for my graph i'll see you in one second
once i've copy and pasted those
awesome okay let me walk over what i did
with
the colors for you so you can see that i
added something here
i added a function called scale fill
manual
and this is basically saying for that
fill aesthetic that i've added to my
allow me to specifically choose manually
the colors that i want you to use
and i've given it a list of all of the
different hex codes
from this website that had the
colorblind friendly color palette
so this basically tells it instead of
this red green and blue use the
colorblind friendly
colors forgot a parenthesis now let's go
ahead and see what that looks like
beautiful so we have the same graph as
above but now we're using colors that at
least according to this color
plait palette are easily distinguishable
but for people who are colorblind
now another thing that we have to think
about when thinking about accessibility
is visual impairment if you look at the
plots that we're using
they're not horrible but the default
font sizes for everything
and line sizes are incredibly
small if you have any type of visual
impairment i mean
ask your grandparents to try and read
some of your graphs
it's going to be really hard to read
especially the species of penguin but
pretty much anything on our graph
one thing we can do to keep
accessibility in mind is
make sure that the fonts and the sizes
that we're using
are appropriate for the context of our
graph in this case
i think we have a lot of room to
increase the size of the fonts to make
sure that one they're easily noticeable
and two for people with any visual
impairment that they're still
able to read them now that we've talked
about colorblindness let's talk a little
bit about
visual impairment and i want to break
down more specifics because there's
different kinds of visual impairment and
different ways that we can address it
one thing that we might want to think
about is contrast
are we putting different colors that are
very similar
on top of or next to each other and
expecting people to know
that they're different objects or
different groups or something like that
increasing contrast is a really good way
to communicate messages when part of
your message is
these things are different one of the
main ways that we can see this
is in the background and we actually did
a really good job of this just by adding
our theme minimal earlier in the process
the white background is a really great
contrast to the darker more saturated
that we use to fill our box plots
so i'm not going to add anything here
because we've already done an a plus job
with contrast but
it's always something to be thinking
about when you're making graphs
the next thing that we can be thinking
about is shapes
so if you have some kind of color
blindness
or any other real visual impairment
that makes it really difficult to see
the difference between the colors that
we've used
in our fill for our box plots
one thing you can do is add a quote
redundant
information to your graph for instance
because one of the ways that we're using
color is to distinguish the different
species of penguins
we can add another element that has a
non-color way of communicating that
we could fill our box plots with a
pattern
although i'm not sure that that's
actually available in plot9 although it
is in ggplot and r
another thing that we could do is add
raw data points
and have different shapes for the data
points
per species the way that we would do
that is by adding another geom
geom point like we did in the previous
and then inside g on point we'd say aes
shape equals species and this will allow
us to have the raw data plotted
on our box plot and for the shape to be
different for each of our species
now even though if you look at this and
see oh
well i feel like that's redundant i
don't want to use up ink
well sure and it's definitely a balance
you have to think about
but in this case i think it's a really
great way to add
information that is actually not
redundant to some
people in order to better think about
accessibility and make sure that your
message which is species are different
is clearly understandable to everyone
now the last and maybe the most
common thing that i want to talk about
in terms of accessibility
again this isn't meant to be exhaustive
but one thing i often see
is the fact that the default graphs in
ggplot or matplotlib or
really any plotting package makes
everything
so small it makes lines really thin
it makes fonts really small and that's
not a great way to
make sure we're supporting accessibility
so one thing that we can do is look at
our graph and go what could we
make more bold more prominent bigger
in order to help people that maybe have
mild visual impairment
if you ask like your grandparents or
someone you know that has some kind of
visual impairment you can even ask them
and say can you read this is it clear to
you
and then change those things to make
sure you're including a wide range of
people
in the group of people that can really
appreciate the message that you're
trying to sign with your graph
like most things the way that we're
going to change this is inside the theme
and the way that we're going to change
this is we're going to say axis
underscore text underscore x
so this is saying change the axis text
on the x axis
and we're going to say element text
because it's a text element
and we're going to say size equals and
i'm going to make this
size 12 font and let's just see what
that does to our graph
beautiful oh my goodness i can see the
contrast so much better
in terms of what the species are i think
this will make it easier
not only for people with visual
impairment but really anyone
to make sure that they can read the
species labels on the x-axis
i'm going to go ahead and change the
axis text as well as the title text
to also be a little bit bigger i've gone
ahead and added some extra things so
that the title
the y text and the x text
are bigger font sizes another thing that
you can think of in terms of size to
make sure your graph is accessible
is to increase the size of things like
our shapes right now they're
really tiny and it kind of makes them
hard to see
and if they're an important way that
we're trying to communicate that the
different species are different
we can increase their size in order to
make them a little more visible
we want to do this inside the individual
geom
for the point so inside gm point we're
going to add an argument called
size and we're going to set it equal to
3 and that's going to make our shapes a
little bit bigger
let's see what our graph looks like now
that we've added all of these resizing
beautiful the shapes are so much easier
to see and now so is the title
and the y and x axes i'm going to stop
here but
i think we could go on forever adding
and removing things making sure that our
message was clear
but at least for the question that we
were trying to answer or communicate
which is the
different penguin species have different
bill lengths i think we did a really
good job we thought
about accessibility we thought about
communication and we thought about who
our audience was and i think our graph
looks beautiful
and very clear now the last thing was a
little harder to touch on in this graph
but the other thing that you want to
think about in terms of accessibility
is culture different people have
different cultural contexts
and if you're using something like slang
images colors that mean something in
your culture but may not mean the same
thing in another culture
you want to make sure that you are
thinking about that and thinking about
ways that you can
overcome the deficits that might occur
when
using those culturally contextual items
one really silly example is we often see
the colors red green and blue
as stop wait and go if for some reason
you had an audience member who had
never seen a traffic light and never
gotten the association of red green and
yellow that we do you might want to
think about that because if you're
trying to use the color red to mean stop
and the other
give the color green to mean go that's
not going to be intuitive to them
as it would be from someone of the same
culture as you
these are just really important things
to think about to make sure that your
is communicating the same message to
people across all cultures that you are
talking to now i've said it once and i
will say it again
i can't teach you hard and fast rules
for being an effective
data visualizer i wish i could i would
probably make a lot of money and i could
probably teach an entire class on it
but i just can't there's no static rules
it
all depends on your message the context
that you're working in
and even a little bit on the data that
you're working with
what i want you to take away from this
is not how to make a graph
that tells you that different species of
penguins
have different buildings that's really
not going to be applicable in your daily
life or your future career
ever but what i do want you to take away
is all of the questions that we asked
ourselves throughout this process
to make sure that the data visualization
that we created was effective
i want you to ask yourself these
questions whenever you're making a data
and i will put the caveat that if you're
just trying to really quickly plot data
for you to see the data if you're
exploring a new data set
i'm not saying that you have to make
perfect data visualization all the time
what i am saying is when you're making a
data visualization that someone else is
going to see
make sure you're being thoughtful think
about the message
that you're trying to convey and think
about
how you can make that message more
effective can you make it more
accessible can you make it more clear
what can i add and what can i take away
to make sure that the people viewing
my chart are going to understand the
message that i intend for them to
understand
hopefully you enjoyed this process and i
will see you
next time

 
hello and welcome to your naive bayes
and python lecture
so first i'm going to start off with
what we should always start off with
when opening a new notebook and that is
running
all of my import statements so i'm going
to go ahead and run that
don't worry about this line this is just
to make sure that my numbers print out
instead of using scientific notation
so we are going to be talking about how
to implement
the naive bayes algorithm and so
to do that i'm going to go ahead and
load in
this spam data set which like we talked
about in the theoretical lecture
is looking at whether or not an email is
spam based on
whether certain words are present cool
so you can see
that our data set consists entirely of
binary
variables we have whether or not an
email is spam zero
one we have whether or not the word
viagra appears the word love appears the
word dollar appears
or the word by appears and we're going
to use those predictors to predict
whether or not an email is spam
okay so normally you would do this with
all of your data but obviously for the
sake of time i'm just going to do it
with one
and i'm going to make a graph to explore
our data a little bit
so i'm going to give it that spam data
frame
i'm going to say a yes x equals
factor is spam and remember
factor just tells ggplot that the
zeros and ones in this column actually
represent categories not numeric values
and then i'm just for the sake of it
gonna say fill equals
so this is going to show us for both the
spam and the non-spam emails what
proportion of them
have the word viagra in it so i'm going
to add a geom
bar and then of course for aesthetic
purposes
a theme bw cool
so when we run this you can see that we
actually have the same number of spam
and non-spam emails there is a hundred
of each in our data set
and you can see that when an email is
spam
it is much more likely to
have the word viagra in it which makes
sense because you would think that
people
sending you spam emails use that word
more than you know your personal
correspondence
so now that we've explored our data
let's go ahead and build
a model so now that we've explored our
data we're going to go ahead
and build a model and the first thing
that we have to do is
specify what our predictors and our
outcome are
so first we'll do x and x is going to be
all of the features in our model so
viagra love dollar bi
so let's select those columns
viagra love
dollar buy
and then of course we have to specify
our outcome the thing that we're
predicting so we'll say spam
is spam
cool so now that we have our x and y
variables let's
choose our method of model validation in
this case i think i'm going to use
five-fold cross validation so let's
make our k fold k fold
and splits equals five
um and then we have to set everything up
for that for loop
to do our k-fold cross-validation
so i'm going to say act because we're
going to store the accuracy remember
naive bayes is
a classification algorithm which means
it's predicting a category so we usually
use something like accuracy to see how
well it did
so i'm going to create an empty list for
that
um and then i'm going to create our
model so i'm going to say
mb for naive bayes is a
bernoulli and b
so that is the naive bayes algorithm
that we're going to use because
all of our predictors are binary
variables i'll talk a little bit more
about this at the end for now you can
just trust me
so we're going to create our model and
then we can say for train
test in kf dot
split x and then
as typical we are going to go ahead and
set up our x train
and y-train i'll be back when i've done
that okay
so now that we've set up our x-train and
our x-test
i actually realized we're going to want
to store both our
train accuracy and our test accuracy so
that act
list that we did i'm going to say act
train so that's going to be where we
store our
training accuracy and act test
that's going to be where we store our
test accuracy okay so now that we have
our train and test
inside of our k-fold method
we would normally at this point think
about z-scoring
but we only have binary variables so
we're going to skip that step
because we don't z-score binary
variables
so then the next step is to take our
model and say model.fit and
fit on our training data
so now that we have a beautifully fit
model we can go ahead and look at the
model performance
so we're going to do that really in
three ways first we are going to append
to our accuracy
training data so we're going to say ack
train dot append
and then remember we are going to
calculate an accuracy score so we're
going to say accuracy
score and we give it the real data so
that would be y
train in this case and then the
predicted data which we're going to grab
from our
model n b dot predict x
train so this is going to compare the
predictions from our model versus the
actual values for the training set
and then we can go ahead and copy and
paste this
and just change it to be for our test
set so we're going to say act
test and then we're going to say y test
and
x test and then the last way that we
sometimes
look at our performance of our model is
with
a confusion matrix so i'm going to plot
the confusion matrix
confusion matrix for our test set and so
remember that plot confusion matrix
takes the fitted model
the x test and the y
test and it'll create that beautiful
confusion matrix for you
awesome so our for loop is done and once
this for loop has done
all of its rounds so it's going to do
five models because it's five full cross
validation
we can go ahead and print out the
average accuracy for our train
and our test set the confusion matrix
will actually output every time we run
it so
it should already be there so to do that
i'm going to use this fancy formatting
i'm just going to print out test ack and
then turn
the mean accuracy into a string
cool so we have everything set up for
our model all we need to do is run it
so once we get our model accuracy and
all the confusion matrices we can now
examine
how well our model did and check for
overfitting
so you can see that the average test
accuracy
was uh 90.5
and the average train accuracy was also
90.5
and that's good news for us because when
we're looking for overfitting
we're basically looking for a case where
the train accuracy is a lot higher than
the test set accuracy
which would indicate overfitting but we
don't see that here in fact the
estimates are exactly the same so it
looks like we're on a good uh
path here the next thing we can look
through uh
is the confusion matrices and i'm just
gonna focus on this last one
but we are just trying to make sure
that there are no patterns that are
concerning to us so
this looks pretty good we can see as our
accuracy score
said that our model is doing pretty well
and it's not the case that our model is
for some reason
predicting only ones or only zeros or
anything like that so we don't see
anything weird in this confusion matrix
so i'm going to
give it the aok and move on
so the next thing that i want to do is
check a single email
by hand using our model so let's pretend
we got this email
hello i hope you're well honestly i am
doing super well
i have the urge to buy something crazy
maybe one of those giant bean bags from
the mall
a love sack well another day another
dollar
sincerely dad so we can imagine that we
received this email
and we are going to try and classify it
as spam being a spam email or ham being
a true email from your dad
and we can use the fitted model that we
got from the previous
part to do that and so we can do
this by saying okay let's convert this
actual email into
data so remember we're looking for the
words viagra
love dollar and buy and so viagra
has uh does not fortunately appear in
this email
the word love does the word dollar does
and the word by does so this is
essentially what this row of data would
look like if this email were in our data
set
and so we can take that and don't worry
too much about this this is just making
sure that we have an array
uh of our data so that we can predict
from it so we're using our old model
we're putting in the new data that we
just by hand calculated
from this email and we're gonna see what
the model thinks
the prediction is is it spam or is it
ham
so when we run this you can see that
this algorithm
thinks perhaps rightfully so that this
email
now let's actually do some of the math
by hand so we're gonna use
spam our data frame and this function
group oops group by and we're going to
say group by
is spam and then use the dot mean
function
to pull the proportion of emails that
are have love viagra etc
for our spam and our non-spam emails
and when we run this hopefully this
looks pretty familiar because this
should be similar to what you saw
in your theoretical lecture okay so now
that we have this table
let's go ahead and use these
probabilities to
predict whether this email is spam or
so first i'm going to go with spam and
i'm going to call this spam
score yep score there we go and so
remember the naive
bayes algorithm says that the first
thing that we have to do
is multiply all of our probabilities
together
so for something that is spam we're
going to be looking at this bottom row
because one is
is spam so remember our email did not
have the word viagra in it so this is
the probability that a spam email
has the word viagra so we need to say
1 minus 0.32 to get the probability of a
spam email
not having the word viagra then we're
going to multiply that by the
probability of having the word
love and we're going to say 0.05
times the probability of having the word
dollar which we did so we're going to
say 0.83
times the probability of having a spam
email have the word
buy which ours does so we're going to
say times 0.74
and the last step of the naive bayes
algorithm is to multiply by the
probability of an email being spam
just in general and we saw when we
explored our data
that we have equal numbers of emails
that are spam
and ham so we are going to go ahead and
multiply
by 0.5 because exactly half of our
emails are spam and a half are ham
so multiply by 0.5 then the next thing
we have to do is calculate our ham
score so we're going to do the same
process but we're looking now at the
probabilities that we got from
the ham or real emails so again we do
not thankfully have the word viagra so
we're going to say 1 minus
0.03 times
0.36
times 0.02 times
0.02 and then again the final step
multiplied by the probability of an
email being
ham which again is 0.5 because they're
equal
so now that we have these scores
calculated i'm going to go ahead and
print them out
so that we can see and compare them
ourselves and
remember this is exactly what the naive
bayes algorithm is doing
for us we're just now doing all of the
math by hand
so i'm going to go ahead and print it
and again don't worry about my fancy
formatting this is just because these
are very small numbers and i need to
make sure that python actually
prints all of them out so you can see
here that the score for
spam is much like magnitude
higher than the score for ham so
hopefully that makes a little more sense
intuitively now that we've done the math
by hand
why our naive bayes algorithm put this
email into the spam category because the
score that we calculated is higher for
than it is for ham okay so the last
thing that i want to talk to you about
is this fact that uh there are actually
multiple
algorithms multiple functions to use
naive bayes in sk learn
so the one that we previously used is
the bernoulli naive bayes function
but there are actually two others that
are commonly used for naive bayes
and i'll go over this in a second but
the differences between these basically
have to do with
what kind of data that our predictors
are so one type of function that you can
use for naive bayes is the gaussian
naive bayes function and that function
assumes that your variables
are continuous and have a normal
distribution
the reason we have different functions
for this is because the way we calculate
those probabilities that we use for the
naive bayes calculation
depends on like whether your variable is
continuous or categorical
the way those probabilities are
calculated are different
and that's what these different
functions do so again
gaussian naive bayes assumes that your
predictors are all
continuous and have a normal
distribution the bernoulli naive bayes
which is what we just used
assumes that your data are binary so
that means that all of your predictors
are
zeros and ones the third
type of naive bays that you can do in sk
learn is categorical naive bayes
which is actually very similar to the
bernoulli naive bayes that we just used
but
instead of forcing you to have only two
categories zero and one
you can actually have multiple
categories
so you might be asking me okay well if
gaussian assumes continuous
bernoulli assumes binary and categorical
just
generally assumes that you have
categorical variables
what if my data is mixed
well the problem with that is that you
actually can't do that with a single
in sklearn there are workarounds if you
do have a mix of predictors that are
continuous and categorical
and i've linked a little bit of a
tutorial or like answer here
so you can go ahead and check that out
if you need to use naive bayes on a
mixed
predictors data set in the future but
for the purposes of
our class we're going to stick to cases
where we either have
all continuous or all categorical
predictors
like we did in the previous example but
i'd love to show you an example with the
gaussian naive bayes of how you can use
that function
when you have continuous predictors
predicting a binary
outcome okay so the first thing that i'm
going to do
is i'm going to generate some fake data
for us
to use in our naive bayes model so you
can see here that i'm generating our
outcome this is the binary thing that
we're predicting
and then i'm generating three continuous
predictors then i'm going to go ahead
and put them in a data frame for us to
look at so you can see
that our outcome y is a binary zero one
variable and then our three predictors
are continuous
so i'm gonna set things up similar to
before first we're gonna set our x which
is going to be
df bracket x1
x2 and x3
and then our y our outcome is just going
to be
the column 5. so now we're going to set
things
up like before we're going to do our k
fold k fold
and splits equals 5.
um and then when i build my model i'm
going to call this nb2
i'm actually going to use a
gaussian naive base so we're going to
say gaussian
and b and this is instead of the
bernoulli naive bayes function that we
used
previously so like before i'm going to
store
our accuracy for the train set and the
accuracy for the test set
and then i'm going to do our 4 train
comma
test in kf
dot split x and then
as usual i am going to go ahead and
calculate my x train x test y train y
test i'll be back when i do that
okay now that i have calculated our x
train y
train etc now i'm going to actually go
ahead and
z score my variables since our
continuous variables actually can be
z-scored unlike our binary predictors
it is totally valid to z-score and we
want to do so in a way that doesn't
cause data leakage so i'm going to say z
equals standard
stand standard scalar
and then i'm going to say x train equals
z
dot fit transform
x train and the reason i'm doing it this
way is because all of our variables are
continuous and all of them are being z
scored and then x test equals z
dot transform oops
x test so now that we've z scored we can
fit our model so we'll say
nb2 dot fit x
train which now has our z scored
continuous variables
y train and then similar to before
we are going to generate all of the
accuracy scores and confusion matrices
that we need to evaluate our model
cool so you can see just like before we
appended our
oops to appended our test accuracy to
our test accuracy
list we appended our train accuracy to
our train accuracy list
and then we went ahead and we plotted
the confusion matrix for
each of the models that we're going to
create so
now that we've set this up it actually
looks super similar to what we did
before
the only difference now we're using
continuous variables so we're using the
gaussian naive bayes function
and because they're continuous we are
now allowed to and will
z-score so let's go ahead and run this
code
so you can see we have our test accuracy
on average and our train accuracy on
average
we're looking at this and we're saying
hmm is there a huge difference between
training and
test accuracy and in this case i would
say there's not a huge difference
there's a slight difference the train
accuracy is a little bit higher but it's
not a huge difference that makes me go
uh-oh
i think our model is overfitting so i'm
going to give this an a-okay
we can also again look at our confusion
matrices to make sure that there's
nothing wonky going on
and at least in these it looks like
everything's okay
we do have a little bit of more errors
compared to the other model that we did
with the bernoulli naive bayes
but it's not like there's any systematic
problems and we're actually getting
things right
a lot of the time okay so hopefully that
helped you
understand how we could use naive bayes
with continuous predictors
and just to review there are those three
functions we talked about
gaussian naive bayes which is for
continuous predictors
for newly naive bayes which is for
binary predictors
and categorical naive bayes which is for
categorical predictors
and again for our purposes we're going
to stick to cases where we have all of
one data type but
if you're interested in seeing how you
can mess around with the code to
actually include
a mix of predictors you can check out
that link that i posted here
cool well hope that was helpful and i
will see you in class
bye guys

 
hello welcome to your k-nearest
neighbors lecture so k-nearest neighbors
is another classification algorithm and
it's one of the most simple
classification algorithms I can imagine
so we're going to use data points and
features in the same way that we have
before to predict the category of a
different data point now I made this for
something else but it definitely applies
here so K nearest neighbor neighbors
algorithm essentially assumes that if
you are neighbors or you're similar to
other data points with respect to the
features that you're choosing that you
will have the same category as then so
the way that K nearest neighbors works
is it looks at all of the data points
that are near you based on your values
of different features and it says how
many people are near you are this
category and if it's category a then we
classify you as category a now this
operates on the assumption that you if
you are near to other data points in the
category that you are in that category
but in a lot of cases that is a valid
assumption to make
and so the K nearest neighbor algorithm
works pretty well in those cases so
literally this is how it works we have a
bunch of data points that have labels
and in this case we have the red class
and the yellow class now the way that K
nearest neighbors works is it chooses a
different number of features so just FYI
K nearest neighbors works in
multi-dimensional space ie it works so
when you have more than two features but
obviously it's really hard to plot those
out so in this case I'm just gonna look
at two features so say that we have two
variables these can be anything let's
see it could be the cat
height and the cat weight and the two
groups are different breeds of cats can
be whatever that's what we'll go with
for now so say that we have this data
and we have for each data point to the
height and the weight of the cat as well
as their breed well say we get a new cat
and we only know its height and its
weight and we're trying to predict its
breed well let's say that the data point
is let's say right here so this is our
new data point and we're trying to
classify it based on its height and
weight well the way the K nearest
neighbors works is we look at the K K is
just a variable here it can be any
number the K nearest neighbors to this
data point so for instance if K equals
let's say three we would look for the
three nearest neighbors well that is
this one I think this one and probably
this one so we would look at the three
nearest neighbors and we would say what
category or what breed are these
neighbors well they're all the yellow
category so we would predict that this
data point is in the yellow group now we
can increase K we can say let's say K
equals five so now instead of just
looking at the three nearest neighbors
we look at the five nearest neighbors
which also includes these two points so
now that we're looking at the five
nearest neighbors to this data point we
look at the category of all these data
points and see which one is most common
now we don't have just one category we
actually have one of the red group and
four of the yellow group so what we do
is basically each data point gets a vote
and we choose whichever category has the
highest number of its members so we can
say okay well there's four yellow group
and one red group so we're going to
classify this cat as the yellow group so
we can use any number of K and what
we'll do is we'll just keep increasing
the neighborhood
the like data points around our data
point that we want to look at but no
matter what the basic principle is look
at the k-nearest data points to this
data point we're trying to classify
whichever group has the highest number
of its members in those que neighbors so
whichever of the groups is most common
in your neighborhood that's going to be
the category that we classify our
unknown data point as it's really that
simple
let's see once we do these kinds of
algorithms obviously the categories are
not always going to be so perfectly
split apart but what you can see here is
this is a map of where or excuse me what
category we would classify a point as
depending on where it is in the map so
anything that is orange in the
background if a data point we're here we
would classify it as an orange data
point any data point that is blue we can
classify as the blue category so this
you can see at the very top is working
with K equals 25 so it's looking at the
25 nearest neighbors to each data point
in order to make the classification now
one thing that's interesting and we'll
see a little bit more as we plot more of
these out together when we are using
Python to do this for us as you can see
that these there's these weird areas
that are kind of like raggedy and
basically it just shows us that in these
certain little areas it happens that you
would have more orange neighbors than
you would blue neighbors and so we'll
experiment with this a little bit but as
you increase K sometimes you can see
these boundaries getting a little bit
smoother and we'll experiment with this
a little bit in Python so this isn't the
first time that we've seen this but it's
the first major time that we've seen
this the K nearest neighbors algorithm
has something that we call a hyper
parameter and that hyper parameter is K
the number of neighbors that were
looking at and a hyper parameter in this
case is something where it's a value
that can change that we can use k equals
two we can use K equals 500 if we have
enough data points for that of course
and it's something that the the data
scientist is choosing before we run the
algorithm it's not something that the
algorithm itself estimates from the data
now hyper parameters can definitely just
be chosen based on your knowledge and
expertise as well as the limitations you
might have with your data set obviously
you can't have K equals 5,000 if your
data set only has a hundred points but
there's another way that we can estimate
what the best value of K is using our
data and this is in general called hyper
parameter tuning and it's essentially a
way to try out different values for a
hyper parameter and see which one gives
you a model that performs the best so
the way that we're gonna do this is with
an extension of the model validation
techniques that we've been learning so
so far when we do model validation we're
only using it to check whether to
estimate how our model would perform on
unseen data we haven't really used it
for anything else yet well now we are so
in the past what we've been doing is we
have a data set and in some way whether
it's a train test split or it's a k-fold
cross-validation we're dividing our data
into two sets we're dividing it into our
training test training set and our test
set and we're training our model on the
training set and then we're testing it
to estimate its out-of-sample
performance on the test set well now
with our train with with our hyper
parameter tuning we're gonna add a step
to that so say this is our data we're
gonna still do some kind of train test
split usually so we're going to have our
training data here and then our testing
data here and remember how we've talked
about in class that it's really
important that you have separation
between your train and test set you
don't want any data leakage and that's
the same here so if we're going to use
our test set if we were going to use our
test set to estimate the performance of
a model based on different values of K
so let's say we train our model on the
training set with K equals 5 and then we
test to see how it did on the test set
and then we train our model with K
equals 7 and then test to see how I did
on the test set and etc for a different
case if we do that we're letting our
test set leak into our model because
we're using it to make decisions about
the hyper parameters that are used in
our model so what we do to avoid this is
we make another split in our training
data so now we're taking our training
data and we're gonna make something
called a validation set and so this is
essentially going to take the place of
our test set when we're trying to tune
our hyper parameters so I'm going to use
this here so our test set is still going
to be completely set aside we will not
even touch our test set until we know
what our hyper parameters are and we've
trained our model so what this is gonna
look like is when we need to have our
training data we're gonna split it into
training data and validation data and
then when we're tuning our hyper
parameters we're gonna train our model
with different values for hyper
parameters on the training set and then
we are going to estimate its performance
on the validation set so notice we're
not touching our test set yet so for
instance I could try k equals 5 and then
I could try k equals 7 and then I could
try k equals 10 usually we'll try more
but for the sake of this lecture I am
going to build three different models
one with k equals 5 one with k equals 7
and one with k equals 10 and then I'm
going to test that
on the validation set this is going to
give me an estimate of how well these
models would perform so I am going to
get some kind of level of accuracy for
each of them so say the accuracy for 0.5
is 0.65 for K equals 7 it's 0.72 and for
K equals 10 it's 0.45 so these numbers
from my validation set will give me an
idea of which of my hyper parameter
values is the best so in this case I
would pick K equals 7 because it gave me
the highest accuracy so then what I
would do is okay I've chosen my hyper
parameters now I'm gonna train my model
using K equals 7 and now now that I've
made all the decisions with regards to
my model I'm going to finally look at my
test set and test my model on my test
set and this is going to again do what
model validation is done for us before
which is give us an idea of how our
model will perform on unseen data so
just to review the reason why we do this
is because if you use the test set as a
way to choose a hyper parameter you're
actually leaking information about the
test set back into our model and
remember the whole purpose of model
validation is to check how our model
would do on unseen data and if you're
having your test set leak into your
model it's no longer really unseen data
and you're probably gonna overestimate
how well your model is doing the way we
get around that is by essentially
creating like a fake test set from our
training data which we call the
validation set and that allows us to
create different models using the
training data and estimate their
performance based on the validation set
and use that to choose our hyper
parameters so that we don't even have to
touch our test set until we have our
final model okay I know that was a lot
hopefully the next lecture where we do
this all in Python will help give you a
little bit more information about what
we're gonna do

 
hello and welcome to your logistic
regression lecture so we just finished
learning about linear regression and now
we're going to see how we can basically
use linear regression to predict a
categorical outcome that would be
logistic regression now linear
regression had a continuous outcome that
meant that we were predicting a number
like 0.25 or 67.6 or some other type of
continuous value logistic regression on
the other hand predicts a categorical
variable both of these models can have
categorical or continuous predictors so
inputs to the model what determines the
difference is the outputs of the model
again a linear regression is predicting
continuous values and logistic
regression categorical values and
throughout this lecture I want to
convince you that logistic regression is
just linear regression in Disguise
because basically with logistic
regression we use linear regression to
predict a categorical variable and that
might seem a little weird so let's walk
through how that's actually possible
because again a linear regression
predicts a continuous variable and a
logistic regression predicts a
categorical one so let's logically walk
through the steps of how we can get from
one to the other now before we start I
want to make it clear you're never going
to actually be doing these computations
when building a model I'm talking you
through the theoretical reasons for how
we can use a linear regression model to
predict a categorical value so when you
get raw data to input all those zeros
and ones for your outcome you're not
actually going to be transforming them
this math is done All For You by the
model with that disclaimer out of the
way let's talk a little bit about the
math so linear models like a linear
regression model can't predict a
categorical variable but that's what we
want so what's the closest thing to that
categorical variable that we could
predict that's at least a little more
continuous well how about a probability
instead of predicting just a zero or one
directly we could predict the
probability that a data point is in
category one we can then use those
probabilities to get those zeros and
ones for instance a common threshold
would be 50 if you have a predicted
probability above 50 you'll be a one if
you have one below fifty percent you'll
be a zero although fifty percent doesn't
actually have to be the cutoff that you
use you can use anything from zero to a
hundred percent as you're cut off all
right now we've settled on probabilities
but there's something wrong with
probabilities they're bounded values
they can only be between zero and one
and sometimes linear models can struggle
to predict bounded values that have hard
limits on either side like our
probabilities so what could we we use
instead of probabilities well one thing
we could use is odds odds as we learned
in our all the math you need to know
lecture are just a different way of
representing probabilities the formula
for odds is p over 1 minus P so odds
altogether represent how many times more
likely it is that something will happen
as won't happen odds range from 0 to
Infinity so that's a little bit better
of a range than that bounded zero to one
range we had with probabilities zero in
odds means something will never happen
and as we get closer and closer to
Infinity we get closer and closer to an
event that will always happen so we've
gotten at least one step closer odds
have a broader range that a linear model
might be better able to handle so while
odds have a wider range that a linear
model can better handle it has a problem
with symmetry that we don't love let's
consider an example of an event that is
has a 25 chance of happening so the odds
of that event are going to be 0.25
divided by
0.75 or about
0.333 repeating now consider the
opposite so maybe we could consider the
opposite of this event so if 25 is the
chance of rain let's consider the odds
of it not raining so if there's a 25
chance of rain that means there's a 75
chance that it won't rain so the odds of
it not raining are
0.75 over
0.25 and that equals 3. now this makes
sense to us right we have about a third
and three for the two different events
but there's not symmetry in those values
they're not equidistant from zero for
one event versus its opposite and that
can be a really useful property to have
in our model in order to get that
property we are going to take the log
using log rhythms of our odds to get the
log odds so we are going to say log of
our odds
and that is going to be the thing that
our model is going to predict a
logarithm as we learned in our math
review lecture is something that can
take values between 0 and positive
infinity and transform them into values
that go all the way from negative
Infinity to positive infinity and by the
way that they do this it creates that
symmetry that we wanted so let's go
ahead and take the log of our previous
example right so we had the odds I'll
write them here
0.25 over 0.75 equals 0.333 repeating
and then we had 0.75 divided by 0.25 and
the odds for that event were 3.
now those weren't symmetric around zero
but if we take the natural log of them
the natural log of 0.333 repeating
is negative
1.09
and the natural log of 3
is 1.09 so now we have that nice
symmetry that we wanted and we have a
continuous value that our linear model
can actually predict
so this is the formula that we end up
with so that we can use a linear model
to predict a categorical outcome first
we consider probabilities rather than
predicting directly those zeros and ones
then inside this function you can see
that there is p over 1 minus P that is
our odds and then we take the log of
those odds to get the log odds and
basically what's happening is that this
whole thing is the log odds of something
happening and we're using a linear model
which should look familiar from our
linear regression lectures to predict
the log odds of something happening
as we saw in the last couple of slides
that we did when we have the law Gods we
can just do a little algebra to get back
to that probability and then threshold
it usually at 0.5 but it can be at any
number in order to get those zero and
one predictions so by using log odds as
our outcome we are actually able to use
a linear regression model to predict a
categorical outcome although indirectly
through log odds
in the end this gives us a s shaped or
sigmoid logistic curve that will predict
the probability of being in class 0
versus class 1. and the example that I
have on the screen you have the
probability of passing an exam where
people with zeros did not pass the exam
and people with ones did the Blue Line
represents the probability of passing
the exam based on the number of hours
that you studied alright now that we
know how logistic regression Works let's
talk a little bit about how to assess it
now in linear regression we had a bunch
of different metrics like the mean
squared error the mean absolute error
are squared all of which measured how
close are actual guesses were to the
real values and when we have a
categorical prediction instead of a
continuous one there are other metrics
that we might want to look at because
predicting a category is a little bit
different than predicting a continuous
value for example imagine you're in a
marketing department and you're trying
to guess whether or not someone will be
a big purchaser from your company the
important thing about your model is that
it gets the answer correct does it
actually spit out the correct answers so
that you can know who will be your big
customers because we care about the
accuracy we use slightly different
metrics to assess the performance of a
model and we'll talk about some of them
here the most common one that you would
be familiar with would be accuracy which
is just a measure of out of how many
predictions did I made how many did I
get correct but there's some better
sometimes more nuanced ways of measuring
model performance that we'll talk about
now all right I'm going to turn off my
camera for a second so that you can
actually see the whole slide and let's
review the concept of accuracy a little
bit as well as talk about confusion
matrices when we have a binary
classification meaning that there's only
two categories that a data point can
have like red registered to vote versus
not or a freshman or not a freshman goes
to Chapman doesn't go to Chapman we can
classify one of those as a positive case
or a one and the other as a negative
case or a zero
and when we have this binary
classification problem there are four
possible outcomes when our model makes a
prediction
the first outcome is that we're correct
we guess that our data point is a
positive case and it is a positive case
we can also be correct by guessing that
something is a negative case when it
actually is a negative case
and then we can be incorrect we could
predict that something is a negative
case when it's actually a positive case
or we could predict that something is a
positive case when it's actually a
negative case
so these are the four possible outcomes
you can see that on the main diagonal we
have correct cases These are times when
our model guessed correctly and then on
the off diagonal we have times when we
were incorrect with these four values we
can calculate a ton of different metrics
that measure the performance of our
classification model the first of those
is accuracy which is a pretty intuitive
concept accuracy is just how many times
you were correct out of the total number
of predictions that we made in other
words you can see up here that it's the
true positives the ones that are
positive and we predicted positive plus
the number of true negatives where it
was a negative and we guessed a negative
divided by the total number of
predictions and when we use this number
we basically get a percent of times that
we correctly guessed the category
accuracy basically answers the question
how often is the model correct but
accuracy isn't the end all be all metric
to assess the performance of our model
another thing we might care about is
called sensitivity or recall sensitivity
or recall is out of the actual positive
so out of the data with the actual
category is positive
how many did we correctly predict in
other words we're basically just
ignoring this part of the confusion
Matrix we are only looking at cases that
are actual positives and of those how
many did we get correct in other words
sensitivity or recall answers the
question how often is the model correct
for positive cases and this might be
really important to you or even more
important to you than overall accuracy
for instance imagine you're designing a
test to detect pregnancy or a disease
you might really care about the
sensitivity of your test how many people
who are actually sick or pregnant did
you say are sick or pregnant similarly
there's another metric called
specificity specificity measures how
often we are correct for negative cases
so just like in the previous example
we're ignoring an entire row of our
confusion Matrix
we're just saying four people that are
actually negative how many of them did
you correctly predict as negative for
example let's do another medical analogy
say we have a cancer screening test that
people get at their annual checkups yes
we want it to detect cancer but we also
don't want it to tell everyone that they
have cancer if they don't specificity
would allow you to tell how often you
are telling people without cancer that
they don't have cancer if we had poor
specificity we might be unnecessarily
alarming people saying that they have
cancer when they actually don't another
metric that we might care about is
Precision Precision asks how many of the
people that we predicted are positive
actually are think of a scenario where
you have a model that just predicts that
everyone is positive it would have a
really good sensitivity out of the
people who are actually positive every
single one would be predicted as a
positive live but that isn't the full
story Precision basically says of
everyone that we predicted was positive
how many were actually positive
next we'll talk about an F1 score now an
F1 score doesn't have as much of an
intuitive interpretation like our other
metrics but as you can see from the
formula It's a combination of precision
and recall so if we ever need a single
metric that combines both precision and
recall F1 scores are a great way to get
that Precision measures how often
predicted positives actually are
positive and recall predicts how often
we correctly predict actual positives
combined it's a really useful metric to
assess how wall our model is performing
all right enough with Standalone metrics
let's really quickly talk about the ROC
curve an Roc curve is a useful
visualization that tells us how well our
model does at distinguishing between
positive and negative cases
an Roc curve is just a graph and on the
x-axis it has one minus the specificity
or in other words the false positive
rate in other words how often are we
accidentally classifying things as
positive when they're truly negative and
on the y-axis we have the true positive
rate
in other words how many of our positives
are we correctly predicting as positive
now I mentioned before that we usually
use a threshold of 0.5 for logistic
regression meaning that anything with a
predicted probability above 0.5 will get
classified as a one and anything below
will get classified as a zero but like I
said we don't have to use that threshold
and the ROC curve kind of takes that to
an extreme the ROC curve basically says
as we try different thresholds how does
that affect the true positive rate and
the false positive rate
in an Ideal World we would have a really
high true positive rate and a really low
false positive rate basically a perfect
model would be able to have a hundred
percent true positives with zero percent
false positives now I'm going to give
you a couple of lines for reference the
first one is going to be a straight
diagonal line across the ROC graph this
represents a model that basically sucks
it's not really able to distinguish
between positive cases and negative
cases so we kind of treat this straight
diagonal line as a baseline for a model
that just randomly guesses like a coin
flip would have this type of Roc curve
so we ideally would like our model to be
better than that and remember we talked
about the case that if something is
really good or actually perfect then it
will have a very high true positive rate
and a very low false positive rate so a
perfect model would have an Roc curve
that basically looks
like that
now in real life we rarely get models
that are exact coin flips or completely
perfect and so we often see something
more in the middle and we might have say
a line like this
this means it's definitely better you
can see there's a big distance between
it and the diagonal line so this model
is definitely a little bit better
but it's not perfect and any curve that
we're looking at with an Roc curve the
closer it hugs the top left corner of
the graph like this red line the better
it is at distinguishing between positive
cases and negative cases now now that we
know how to read an Roc curve let's talk
a little bit about how they're formed
all right so in the top right corner I
have a distribution of cases we have the
negatives in red and the positives in
Blue on the right
on the x-axis is the predicted
probability for all of those cases you
can see that on average positive cases
do have a higher predicted probability
and negative cases have a lower
predictive probability although they do
overlap a little bit so our model is not
able to perfectly decide between the
positive and the negative cases all
right so how does that actually relate
to the ROC curve well remember that an
Roc curve is just a way of measuring how
different thresholds affect the false
positive rate
and the true positive rate
so let's start with a really extreme
threshold again 0.5 is traditional but
we could choose anything between 0 and
1. so I'm going to choose something
right here this is probably about like
0.9 as our threshold so anything above
0.9 is going to get classified as a
positive and anything below it will get
classified as a negative
now what does this do to our false
positive and our true positive rate well
it's actually pretty good for our false
positive rate almost nothing gets
classified as a positive that is
actually a negative so our false
positive rate is going to be very low
and let's look at our true positive rate
well out of our actual positives barely
any are getting classified as positive
cases which means even though our false
positive rate is low our true positive
rate is also low so we're going to put a
dot right here next let's choose a new
threshold maybe I'll choose oh say right
here
so I have a new threshold
and let's see how that affects the true
positives and the false positives I'm
going to erase these
so now that I have my threshold here and
everything to the right is going to be
classified as positive every tuning to
the left is going to be classified as
negative I am actually getting a couple
not many but a couple of false positives
how so let's say it's probably like
right here
however my true positive rate is a lot
better instead of having almost no true
positives be classified as positive we
now have oh I would guess about 50
percent
of art positives being classified as
positive okay so let's put that on our
graph so let's say we have about 50 true
positives because about 50 of that blue
distribution is to the right of our
threshold
um and we still have a pretty low false
so our DOT is going to go here
all right let's select yet another
threshold let's put one oh right here
that's probably about 0.4
so I'm going to erase this
all right now that we have our new
threshold let's see how it affects our
false positives and our true positives
well our false positives again have
increased so that's probably about let's
say here
but our true positives have uh done
incredibly well most of the blue
threshold meaning that they will
correctly get classified as positive
cases so we're having a pretty high true
positive and a pretty high and a pretty
mid-level false positive rate all right
one last threshold for a last threshold
let's put it oh say about here maybe
that's about 0.15
now that our threshold is here and
everything above the threshold is
classified as positive and everything
below the threshold is classified as
negative our true positive rate is
incredibly good pretty much everything
in this blue distribution is getting
classified correctly as a positive so
our true positives are pretty high I'd
say close to a hundred percent
however our false positives are also
quite High notice that almost the entire
pink distribution of negatives is going
to be classified accidentally as a
positive that means that our false
positive rate is very high
so now that we've tried out a bunch of
different thresholds we can draw our Roc
curve now I'm not a great drawer so I'm
just roughly going to do this a computer
will of course do this much better but
you can see that this is doing a lot
better than our straight diagonal
of a basically a random or a coin flip
model would do so our model is doing
better than that but it's not perfect
right a perfect curve would hug that
axis and go oh and then like this
so basically you can see with this curve
that as we moved our threshold to the
left we were basically trading some
false positives for a better true
positive rate and there's a certain
point at which that kind of stops being
worth it we want the model that is the
closest to the top left corner but in
general we can see from the curve that
our model is doing better than a random
coin flip model but not as good as a
model that was perfect for this reason
we can use Roc curves to assess
different models and compare them for
instance say that we had a model that
was just a little bit better than ours
let's say it's Roc curve was more like
this
we could use those Roc curves to say oh
the model represented in green does a
better job at distinguishing between
positive cases and negative cases we can
tell because the ROC curve more closely
hugs that top left corner now this graph
is really useful but when we want a
summary metric or if we're running
dozens of different models it can be a
little tough to look at that many Roc
curves so one other thing that we do is
instead of just using the entire plot or
in addition to using the entire plot we
look at a single number the area under
the ROC curve in other words the rocaut
the area under the curve of The Roc
curve when we have a perfect model like
the one in red the area under the curve
is going to be one or very close to it
because all of the area under this line
is under that curve a model that's
pretty random about a coin flip is going
to have an area under the curve of 50
because you can see from this graph that
about 50 percent of the area of this
graph is under that curve better models
will have a higher AUC closer to one and
you can think of the rocaut as the
probability that a randomly selected
positive case will have a higher
predicted probability or score than a
randomly selected negative case in other
words it's a measure of how well your
model can distinguish between positive
and negative cases to review a pretty
mediocre coin flip model is going to
have an AUC around 0.5 and a perfect
model would have an AUC around one most
of our models are going to land
somewhere in between with higher scores
meaning better ability to distinguish
between positive and negative cases now
that we've talked about metrics we often
use to assess our model's performance
let's talk about something related loss
functions remember from our linear
regression lecture that loss functions
are a way of assessing our model
performance where lower is better and
higher is worse these are the metrics
that we use to actually fit or train our
model in linear regression one of the
loss functions that we talked about is a
sum of squared errors or basically for
every data point how far away was our
model's prediction from the actual value
square that distance and add them all
together
we fit our linear regression by
minimizing that value when the distance
between our predictions and the actual
values is as low as possible that is the
best model that we have and the one we
end up using for logistic regression we
use something called The Log loss which
is this formula right here
now these numbers might not make a ton
of sense but I think a graph might help
a little bit all right here's a graph of
our log loss loss function now let's
take a step back for a second and think
about what this means
if we have a positive case represented
by a one we want our predicted
probabilities to be very high because
that means that that case will likely be
classified as a positive case so high
probabilities are good and low
probabilities are bad because if we have
a low predicted probability for a
positive case that means it's probably
going to get misclassified as a negative
well when we have a positive case you
can see here represented in green our
loss function looks like this loss is
high or bad when our predicted
probabilities are low and losses low or
good when our predicted probabilities
are high which matches what we just
talked about when our case is positive
we want our predicted probabilities to
be high and when they're low that means
that our model is not doing that great
of a job on the other hand when we're
dealing with a negative case
our model is performing well when it
predicts a small probability and it's
performing quite poorly when it predicts
a high probability because that means
that it would get incorrectly classified
as a positive and when we look at the
cases in yellow which represent cases
where the data point is a negative case
you can see that's exactly what our loss
function does loss is low or good when
the predicted probability is low and
it's likely to get classified as a
negative case and it is high
when the predicted probability is high
because that would mean that our data
point that is actually negative is
likely to get misclassified as a
positive data point
so while the math looks a little bit
scary this graph can help us understand
what the loss function is actually doing
now you might be wondering why am I
talking about this loss function well I
want to talk about something very
quickly called gradient descent
now this is a pretty math heavy topic
that is covered extensively in 393 but
I'm going to try and cover it with as
little math as possible here if you've
taken a stats class before and learned
linear regression you probably know that
there's a direct way to solve for which
coefficients are best for your linear
regression model
however for logistic regression and
actually for pretty much all of the
machine learning models we use in this
class it's not always easy to find and
solve for that exact solution
so instead we get approximate Solutions
using optimization methods
one of the most common basic methods of
that is gradient descent now this is
where derivatives come in say we have a
model that has two parameters maybe an
intercept and a slope or two slopes or
whatever you want them to be this graph
represents the values of a loss function
at different values of those two
parameters remember we want our loss to
be low so ideally we'd like to find the
lowest place on this graph and then say
whatever those two parameter values are
at that lowest place that is our best
solution
but even as you can see in this very
simple image sometimes these spaces are
weirdly shaped and they're not really
easy to solve for it's not like we can
just solve for the exact lowest place
and put our parameters there so one
thing that we can do is use gradient
descent gradient descent is an iterative
method which basically means that we
make a guess at the correct answer and
then we slowly improve our guess and
improve it again and improve it again
and improve it again until we get
something that is acceptable think of
this complicated graph as a mountain and
valleys and say you're on a hike and you
are a little bit lost because it is
completely dark but you need to get to
the valley so that you can find someone
who can take you back to your campsite
well because it's dark you can't just
look around and locate the lowest spot
but what could you do well one thing you
could do is kind of feel your way around
you and say what is this steepest
downward direction that I could head and
take one step in that direction and
that's exactly the idea behind gradient
descent a gradient which is just
basically a bunch of derivatives tells
you how the function or how those
mountains are changing and we can use
those numbers to tell us which direction
should I walk if I want to go down the
most steeply well you can see this line
on this graph is doing exactly that so
if we started here
we might take one step in this direction
because that's the steepest downward
Direction and then we have to reassess
we say okay now which is the steepest
downward Direction well it's this way so
we're going to take one step this way
okay now which is the steepest well here
so I'm going to go this way okay now
well now it's here and now it's here and
now it's here here here here here here
and eventually hopefully we will end up
in the uh bottom of the graph or at the
lowest value now with gradient descent
it's not guaranteed that you'll get to
the lowest possible value but you can
see here that you'll get to quite a low
one and this is an example of how a lot
of our algorithms can be solved instead
of having a formula to directly know the
correct answer we can iteratively guess
and improve our answer until we get to
one that is acceptable enough in this
case that's when the loss function is as
low as possible one way we can decide to
stop our gradient descent is when the
derivative is very close to zero or
we're getting to kind of a flat surface
that indicates that we're probably in a
minimum now while I won't drill you on
any of the math that goes into gradient
descent you can again take 393 for that
I do want you to be familiar with the
idea of iterative optimization where we
start with a guess and iteratively
improve it also with the idea of
derivatives that we can use derivatives
as a way to find the minimum value of a
function by moving towards that minimum
value by looking for the steepest
negative derivative and finally that
when we have Minima or Maxima it's
usually at places where our derivatives
are at or near zero all right that's all
I have for you I will see you next time

 
hello and welcome to your first
linear regression lecture so i'm very
excited to talk about linear regression
because it's the first
of the kind of more machine learning
algorithms that we're learning but first
let's talk about what linear regression
is
you may have heard of it before it is
one of the more simple
and more common machine learning or
predictive models
oh and by the way before i start i
actually do want to point out
so i mentioned this at the very
beginning of our course but i
have these things in the top right
corner these little color-coded
categories
that can help you understand a little
bit where we are in the course and how
the algorithm that we're currently
learning about
fits into the grand scheme of all of the
things that we're learning so this is a
predictive model so you can see in the
top right corner i have that little
predict
green circle on the top and this will
continue throughout the course so
all of the slides will have the little
circle in the top to tell you kind of
where does what we're learning fit into
the greater
data science arena
okay back to what i was saying what is
linear regression
linear regression is a way to use
a line or if we have multiple variables
a plane to describe the relationship
between one or more variables and some
outcome that we're interested in
predicting for instance you can see
in the graph that i have on the right
hand side of the screen
that this might be a regression model
between someone's income
and the amount that they pay in rent in
this case we're interested in
predicting how much someone pays in rent
based on their outcome
and the way we predict is by building a
model
of what the relationship between rent
and income is using a straight line that
green line on the screen
linear regression can use multiple
variables to predict our one outcome
in fact we can use both continuous
variables and
categorical variables so income is a
continuous variable but we could also
use something like are you registered to
vote
did your parents have high education
levels
etc now a lot of models that we use in
fact
pretty much every single model that we
use has
assumptions and assumptions just mean
things that we assume are true when we
build our model
so i want to go over a couple of the
assumptions of linear regression with
you
because it's really important when
you're building a model to think about
well what are the things that i'm
assuming are true and
is that a safe assumption to make in
whatever context you're making it
i will say as a brief caveat sometimes
people when they're using linear
regression in the context
of machine learning will sort of brush
over these assumptions
and they won't think about them and it
kind of goes back to
that dichotomy that i talked about
earlier
between prediction and inference when
you're building a model and you only
care about prediction you really only
care that your model is outputting
something accurate
and if you violate assumptions but your
model is still accurate
sometimes people just don't care
now i would argue that it's still
important to think about
because for instance if you violated an
assumption
and even if your model is correct most
of the time but in a few
key important cases it's very wrong
that's not great and we'll talk more
about that later
but i just want to point out that while
some people might not talk about the
assumptions or they might ignore them
completely
i do think these are things that are
important to think about okay
now that i've said all of that i'll try
and rush through some of these
assumptions
so the first one maybe is pretty obvious
the assumption in a linear regression
model is that the relationship between
your predictors
and your outcome is linear as in
linear regression so for instance you
can see
on the screen a relationship that is uh
maybe not linear
you can see that the relationship might
be better described by some kind of
curved line
but we are assuming when we fit a linear
that the relationship is linear and if
that's not the case
maybe there's another type of model
that's more appropriate or
maybe we need to deeply think about what
the implications are
if we're wrong about the relationship
being linear
and then there are two other related
assumptions and these assumptions have
to do with the
errors of our model and we'll get into
this
in very much detail in a second but the
errors of your model are basically how
wrong your model
is in other words what's the difference
between
the true value and what your model
predicts the value will be
one assumption is that of
homoscedasticity
which is fun to say a little hard to say
but all it means is that the variation
of the errors
across all possible predicted values
are the same to simplify that even
further essentially it means that
no matter what your predicted value is
the error is the same
it's not the case that in one part of
your model
that the the error is a lot bigger or a
lot smaller than any other area of your
and if this is violated it essentially
means that your model is
better at predicting in certain places
than in
others regular linear regression assumes
that
it is the same across all values
similarly
we also assume normality of the errors
and if you remember back to the all you
need to know math
lecture that we did a normal
distribution is really
notable in the sense that most of the
values are in the middle
and then as you get further and further
from the mean or the middle value it
gets
less and less likely to have values
there and so you can essentially say
okay well if i take any any given point
on my model
the the distribution of the errors
should be
approximately well that was a horrible
normal distribution
it should be approximately normal it's
kind of hard to draw this in 3d but
essentially
most of the errors are going to be very
small
kind of like your model was pretty
correct and then as you get
bigger and bigger errors there should be
fewer and fewer of them
so again just to review for these uh
error assumptions you're assuming
homoscedasticity and normality of errors
now that i have front loaded you with
all the assumptions and all of the being
careful
let's talk a little bit about how linear
regression actually works
so if you remember back to math class
when you learned the formula for a line
y equals mx plus b that's
what linear regression is essentially y
is our outcome and any of our x values
are the predictors that we're using
and we also have an intercept which
we'll talk about how to interpret that
in a bit
but i just want to point out too that
even though y equals mx plus b
is kind of the typical line remember we
can have more than one predictor so for
instance we could have y
equals mx plus nz plus b in which case
we'd have two predictors
and we can actually continue adding
predictors basically forever
so the slope of a line
tells you the relationship between your
predictor variable
and an outcome it basically tells you as
your predictor variable
increases what happens to the outcome
and the intercept tells you what happens
if
all of your predictors had values that
were
zero so for instance in the
case that you can see on the screen with
the graph we're predicting the number of
cats someone has
by their shoe size and the intercept
would say
okay if someone's shoe size were zero so
when all the predictors are zero
what would i think that the number of
cats that person would have and it looks
like on our screen maybe like
point seven catfish um
and i gave this example to point out
that the intercept when all the
predictors are zero
sometimes is a reasonable value that we
want to know about
and sometimes like in this case it
really isn't
i mean a zero shoe size isn't a thing
so it's not really useful or meaningful
for us to know what number of cats
someone would have if they had a zero
shoe size but
the intercept is there to mathematically
help out our model
and i just want to point out that
sometimes it's meaningful and sometimes
it's not and
you have to decide in any given context
whether the intercept is something that
you care about or not
let's look at a really simple example of
where we're trying to predict weight
by height and you can see based on the
scatter plot on the left side of the
screen
that we have a scatter plot
of all of the data points and we have
our regression line shown in blue
and this essentially tells us okay if
anyone's height is
at a given point if you go up to the
line and go over to weight
that's what the model predicts that
their weight is
and if you look at the coefficients at
this side of the screen let me scroll up
so you can see it
this is how we would interpret those
coefficients so
remember the intercept tells us when our
predictor variables are all zero what
would our predicted
outcome be in this case that means that
if someone had a height
of zero centimeters we would expect
their weight to be
negative 82.2887 kilograms
now i told you before sometimes the
intercept doesn't make sense and
here that's exactly the case because no
one can be zero
centimeters tall the height coefficient
however is a lot more interesting
and meaningful to us here the height
coefficient tells us as
height changes what happens to weight
and in this case as height increases by
one unit in this case centimeters
we expect the predicted value for weight
to increase by 0.9786
so for every one increase in centimeters
of height
our model will predict that that person
weighs 0.9786
kilograms more as i mentioned before
though we
can add a lot more variables to our
model so in this case we are still
predicting weight by height
but we're adding a variable a
categorical variable
for diet and diet would be either meat
eater
or vegetarian in this case so
again the interpretation of the
coefficients which i'll scroll up to
here the intercept is when all of our
variables are zero
what do we predict the weight is of
someone now with height again
zero centimeters doesn't really make
sense uh with categorical variables a
zero basically means it's the category
that isn't shown
in our table so this means that a zero
in the case of diet means that the
person is a meat-eater
so our intercept term tells us when
someone's height
is zero centimeters and they're a
mean-eater we expect them to have a
negative 72.0358 kilogram weight which
again doesn't really make sense i'll
stop giving you silly examples of this
but just so you know what that does mean
the coefficient for diet which is
remember a categorical variable
tells us if someone is a vegetarian
since that's the category you can see
that shown here
our predicted weight for them is going
to be negative
7.62 compared to someone who is a meat
so for instance if someone was a
vegetarian we would say that their
predicted weight is 7.6222
kilograms lower than someone who has a
diet that includes meat
again we've gone over this before but
the coefficient for height says
that for every increase in one unit of
our predictor height
we expect the weight the predicted
weight to go up by
0.94 approximately
again these two things are very similar
but remember the slight difference
between categorical
versus the continuous interpretation
okay same model
now we're adding age and at this point
it's pretty hard to show you a graph
because there's so many different
variables but let's really quickly run
through the interpretation of the
coefficients
again intercept when all of our
predictors are zero so age is zero
height is zero and diet is meat eater
we expect that someone's weight would be
negative 57 kilograms
again not meaningful for diet
it's if someone is a vegetarian we
expect their weight to be eight
about kilograms lower than if they were
a meat eater
for height for every increase in one
unit
in this case centimeters of height we
expect a 0.89
increase in their predicted weight
similarly for age
as every increase in one unit of age
in this case years we expect a 0.13
decrease in the predicted weight and
this is really interesting because
we can classify variables as having a
positive or a negative relationship with
the outcome
if a variable has a positive
relationship
it means that as that variable increases
so does the outcome
this is the case for height here if a
variable has a negative relationship
then we say
as that variable increases our predicted
outcome
decreases so they're moving in opposite
directions
again just to review we can have a
positive relationship like height
or a negative relationship like age
now i'm going to take a brief detour to
talk about
who is the goat the greatest of all time
uh harden our basketball player or
alonso our baseball player
hardin had 378 three-pointers i believe
this is when
2018 whereas alonso had
53 home runs and both of those are
incredibly impressive but we need to
decide
who is the goat here but how do we
compare them
one's a basketball player one's a
baseball player
it's not really comparable to compare
three pointers to home runs so
what could we do one thing we can do is
compare each player
to all of their peers other professional
sports players in their sport
for instance for hardin for basketball
we can see this dotted line which
represents his performance and this
distribution which represents the
performance of a bunch of other
basketball players
and we can see that he's incredibly good
right he's
one of the best of the best and when we
look at this graph this at least gives
us a better concept of okay he's he's
very good he's at the very top of his
game
now let's look at alonzo in comparison
to other baseball players
when we look at this graph you can also
see okay he's also
very good compared to other baseball
players he's one of the best
and most baseball players have around 35
home runs
and this guy has over 50. now this helps
a little bit because we know that both
of them
are just extraordinary compared to their
peers
for instance if we saw that alonzo was
kind of in the middle of the pack
we could easily decide who the goat is
right now
right here but because they're both
extraordinary we need to do a little bit
more to figure out who's the goat
the thing that we're gonna do is
something called z
scoring people often refer to this as
normalizing or
standardizing you might see those terms
around but what we're going to use
specifically here is the z-score
now a z-score does two operations to
help take
things that are on very different scales
and put them on similar skills
the first thing is to subtract the mean
so for instance for alonzo and home runs
this means that from every score in our
distribution of
baseball players we would take their
score and subtract
the mean number of home runs that people
scored
this results in a score where zero
indicates that someone got the exact
average score values that are positive
indicate that someone did above average
and values that are negative indicate
that someone did below average
we can do the same thing to basketball
and then we can do the second step
the second step is to divide that newly
subtracted score by the standard
deviation
of all of the scores and this does
something really cool
because remember a standard deviation is
the typical amount
that data points vary from the mean and
from each other
and when we divide by the standard
deviation it puts these two scores that
were originally on
very different scales on at least a
scale that's similar
so you can see at the graph here we've
now z
scored the variables for home runs and
three pointers
and this allows us to at least a little
bit better compare the performance of
alonzo versus hardin
so here we can see with the z-scored
variables that while both alonso
and hardin are miles above their peers
hardin shown by the black line is
a little bit better compared to his
peers than alonso
is which makes me think that we should
declare him
the goat now you may wonder chelsea
why did you just take a long time to
talk about sports and z
scores weren't we talking about linear
regression
yes we were and i'm glad you brought it
up when
we have models that have multiple
variables like continuous variables in
our model before we had
height and age when we look at the
coefficients we said
right a one unit increase in our
predictor whether it's height or age
is going to result in a coefficient
change in the outcome but height and age
are on such different scales like
a similar to alonzo and hardin like how
do you compare
a one year increase with a one
centimeter increase in height
it's very difficult and so sometimes
when we want to compare
two variables that are on different
scales we use z-scores
we z-score our continuous variables
before we put them in the model
so that the coefficients are now a
little more comparable
for instance this is the result of the
coefficients for a model where we
z scored both height and age
and this allows us to more accurately
compare both height and age together
because now the interpretation of this
is that a one unit which now is one
standard deviation because a z-score
increase of one in a z-score is an
increase in one standard deviation
so one standard deviation increase in
height is going to lead to about a 13
and a half
kilogram increase in the predicted
weight of someone
whereas for age a one unit increase in
this case a standard deviation
age
is going to result in a negative 2.5
decrease in the predicted weight for
someone
now that they're on a similar scale we
can compare and say okay maybe
height has a stronger relationship
with weight than age does
and that's one of the reasons why we use
z-scores in our model
for comparison but there's one other
reason why we use z-scores
and this is not as much an
interpretation reason
as a computational reason when
you have variables that are on very
different scales
it can sometimes be hard for your
algorithm to find
the best solution because there's such
like a wide broad range
of values that it has to explore when
you z-score your variables
you're putting all of your continuous
variables on at least
more similar scales and that helps your
algorithm
converge faster so basically it finds
you an answer
more quickly and we love that we think
that that's
excellent and so that's another reason
why we z score variables
apart from the fact that it makes them
easier to compare to each other
now that we've talked a little bit more
about the interpretation of linear
models
maybe we should talk a little bit about
how we get them
i know that typically you just press a
button on a computer and one pops up for
you but
let's talk about how your computer is
doing that
so if you have a scatter plot of data
and to make things simple we just have
one predictor
and a predicted value for the scatter
plot there's a ton of straight lines
that at least
look pretty good in our opinion so how
do we choose which one
is the best one which should be our
optimal solution
well the way that we decide is
we look at how wrong our model is
about what the true values are we often
call these
errors or residuals because it's how
wrong our model is when predicting the
outcome value
so you can see here that the errors or
residuals are represented by these
maroon lines which go from the real data
point
down to the regression line basically
offering a
distance of how incorrect our model was
we want our model to be as accurate as
possible
so the way that we choose our model is
essentially which of the lines that we
could possibly choose
has the smallest error the smallest
distance
between our points and our model now we
actually don't
just add up all of the distances for
mathematical reasons that i won't go
into
we actually square each error and that's
why you see those transparent
squares on the screen because when we
square
each error you're essentially taking the
area of a square
that has sides that are the same length
as the distance between
the data point and the regression line
so we call this when we add up all the
areas of all the squares for every
single data point
we call this the sum of squared errors
and we want our regression line to have
the
smallest value for the sum of squared
errors as
possible we also could calculate
something called the mean squared error
which is the sum of squared errors
divided by the number of data points so
basically on average how much squared
error is there per data point
these are basically the same number it
doesn't really matter in context of
choosing the best line but
just so that you know technically we
have these two different calculations
so we've selected our line that has the
lowest sum of squared errors or mean
squared error
possible just to give you an idea of
what that looks like
we can look at these graphs here on the
graph on the left
our model is very accurate the data
points
the real values represented by the
points are very close to the model the
model is pretty spot on all the time
at the right hand side you can see a
model that has a higher sum of squared
errors or mean squared error
because the model is not
right all the time i mean it's not right
all the time in the other one but this
is very spread out we are pretty wrong
in certain places around our model
so in this case our line on the left
would have
a lower sum of squared and mean squared
error
compared to the one on the right so now
that i've talked a little bit about
sum of squared errors and mean squared
error i want to put it into context that
will help us understand
how we fit models in general not just
for linear regression
so the mean squared error can be
considered something that we call
a loss function and
the loss function is essentially a
measure of
how well your model is doing
in the case of mean squared error a good
model will have accurate predictions so
a low
mean squared error means our model is
doing great
a really high mean squared error means
our model is not doing so great
and so we call this a loss function
because it's a metric of how well our
model is doing and in this case
just fyi it'd be 1 over n
times the sum of our predicted values a
little hat means predicted value
minus the actual value squared
and that's how we calculate mean squared
and that's how we assess whether or not
our model is doing well
now one question i get a lot is what's a
good mean squared error
and i wish i could give you a rule of
thumb but remember
mean squared error is in the units of
whatever your outcome is so
if we are predicting weight it's going
to be in kilograms if we're predicting
height it's going to be in centimeters
if you're predicting credit score it's
going to be in
whatever a credit score unit is and so
there's no one benchmark for a good mean
it's all dependent on the context of
your outcome variable
so to get around the fact that mean
squared error is in
different units for every single model
that you make another way we can measure
the performance of our model
is using something called r squared now
you may have heard of r squared before
but i just really quickly want to go
over what it is
so in this scatter plot we have two
variables we can have like an
x and a y and say we're using
x to predict y well one way we could
predict things
if we had no information is to every
time
we see a variable we predict
that the y value for that variable is
whatever the mean is for our entire
sample
this is actually a pretty smart choice
if we know
nothing else about our data points
because
the mean basically gives you like oh
like a good enough guess
whereas if we just predicted randomly
we'd often be
pretty wrong you can think of this as
just a baseline model if we know nothing
else
the mean is a pretty good prediction
but of course as with any model there's
and this is what we would call the total
variance of the model
right so it's how different are our data
from the mean but given that we actually
know
the values for x we can create a
regression model
that predicts y based on x
and this model that results is hopefully
a little bit better than just guessing
the mean all the time
we can measure how well our model is
basically with the sum of squared errors
or the mean squared error
how different is our real data points
compared to the predicted values
that the model gives us now this error
again
is just variation around our regression
it's essentially a measure of how
accurate our regression model is
well r squared is basically a way of
comparing that baseline model where
all you do is predict the mean to a
little bit more of a nuanced model our
regression model that uses a separate
variable
x to predict our outcome variable y
and r squared in just
in like words is the percent of
variation
explained by our model the way we
actually calculate that is
if we treat the baseline model of
predicting the mean
as like the baseline it's basically
saying
how much does the prediction improve
when we look at a regression model
versus just that baseline mean model
and so we essentially say take the
variation that we found with the mean
subtract the error that we see from the
and divide it by the total variation the
dividing by the total variation thing
just make sure that r squared is always
between 0 and 1.
and remember r squared is the percent of
explained by our model so the more
variation our model explains the better
it's performing so an r squared close to
1 or one hundred percent if you're doing
it in percentages
is very good an r squared of zero
is very bad an r squared of zero means
that our model
is doing no better than just predicting
the mean
every single time now i want to take
this as a moment to say
technically r squared should not have
values that are negative
but remember we're using r squared as a
measure of how does our model do
compared to a baseline just
guess the mean and when we get to
later data sets that we're looking at
you might sometimes see
a negative r squared score and remember
if a 0 means it does just as well as the
mean but no better
a negative r squared score means your
model is actually doing
worse than just predicting the mean now
once we see this in real life or in real
data
we'll talk more deeply about what that
means but just keep that in mind because
if you've learned about r
squared in a statistics class or
something like that
a negative value might seem impossible
and you may feel like you did something
wrong but i just want to tell you
no you didn't and we will talk more
about what that means when we get there
the last topic i want to talk about
today is a bit of a warning
and that warning is don't extrapolate
so let's say that this graph on the
represents a model that i've built i've
built my model
using x to predict y and i built a
pretty well fitting model represented by
the yellow line
but notice that the range of x values
that i have
in the data set i used to build my model
only part of the entire range of x i
only have values in this little
orange part of the x-axis
well let's say that i want to predict
what the y value is when my x value
is way over here represented by this on
the x axis and the straight line
well technically i can and my model
again represented by the yellow line
will give me a value and that value is
represented by this circle up here
but here's the problem when i built my
i had no information about values of
x that were this large
and that can be problematic because
since i have no information
i can't assume that the relationship
that i saw in my data
is going to continue forever in fact
this in the made-up data that i created
is the real relationship between
x and y you can see that the yellow line
very accurately describes the
relationship in the data we saw
shown by the black points that are solid
does not continue for larger values of x
represented by the faded black dots and
given this new information of what the
values of x are like when they're this
big
our prediction looks pretty silly a
better prediction would probably be a
value
more around here and you can see that
that's a
pretty large difference and this is why
i'm warning you
don't extrapolate if you have a
model that has a certain range for the
predictors
don't try and use that same model to
for values of your predictors that are
way beyond what you
ever saw in your sample if you don't
have information about something
it's probably not smart to use your
model to try and predict it
because as you can see if we had the
full picture
we probably would have fit an entirely
different model represented by this
green line
but because we didn't have that
information we just
did what was best at the time so you
need to be really careful
when you don't have information for a
certain range of values
whether it's lower or higher that was
all i have to say for theoretical parts
but
go ahead and go to the next lecture to
learn more about how to do
linear regression in python i'll see you
next time

 
hello and welcome to your linear
regression one in python lecture
so first of course i'm going to run all
of our import statements very important
to do um and then i'm going to load a
data set so this should look really
familiar we have the height the weight
the diet and the age for different
people
now the first thing that i want to talk
about here is a little bit more about
z-scoring
and we talked about this in the goat
section of the conceptual lecture but i
want to dig into it a little bit more
and the point i'm about to make is very
important so just listen really
carefully for a second
when you z score things it does change
the scale that something is measured on
right that was the entire point of the
goat example
however it does not change the shape of
the data
or the data's relationship with other
variables and i'll show you what i mean
so i'm going to pull out our good
old-fashioned gg plot i'm going to say h
w a s
x equals there we go height we'll say
and i'm going to use something called a
density plot gm density
density there we go
and let's add a theme minimal just for
fun
and i'm going to go ahead and run this
so a density plot is sort of like a
histogram but instead of bars you have
this smooth line describing the shape of
the data and what i want you to notice
here is what the shape of the data is
basically it has a peak in the middle
it's a little bit higher and the high
end than in the low end like it's quite
low here but this is just the general
shape of the different heights that we
have in our data
i also want to make a gg plot we'll say
gg plot h-w-a-s
x equals height
y equals weight
do plus g on point and then plus theme
minimal
uh and now what i'm making is a scatter
plot on the relationship between height
and weight we know often in real life
the taller you are the heavier you tend
to be and we can see that positive
relationship here now the reason i'm
showing you these plots in the raw data
is because i want you to convince
yourself that while z scoring is going
to change the numbers we see on these
axes for instance here it's going to
change the x-axis here it will change
both the x and y-axis numbers but it's
not actually going to change the shape
of the data nor is it going to change
the relationship between two variables
okay so now that you've seen the raw
data let's z-score
so in order to z-score there's a couple
steps that we need to take in python in
order to z-score and an sklearn we're
going to use standard
scalar
and this is basically just a tool that's
going to help us actually z-score things
the first thing that this can help us do
is calculate the means and the standard
deviations that we need in order to
calculate the z-scores
to do that we're going to use the dot
fit function so we're going to say z dot
fit
and then we're going to give it data
that will allow us to fit and aka
calculate the means and standard
deviations of all the columns that we
give it
so i'm going to give it hw but if we
look at our data actually we don't even
have a printout so let's do that really
quick
if we look at ahead of our data we'll
notice that some of the columns
are continuous variables like height and
weight
and some are not like diet diet is not
even a number it's whether you eat meat
or not
and then we have variables like age
which are interval technically right
because we never say like oh i'm 60.243
but in real life we often treat interval
data as continuous data especially
things like age which tend to have a big
range
so when we look at this data set we know
that we want to z-score height and
weight and i just told you that we also
want to z-score interval data like age
we don't want to do anything to diet
right what would it even mean to z-score
meat like it doesn't really make any
sense
so
let's get rid of this
what i'm going to do is i'm going to fit
not on all of the columns of hw but only
on the ones that i want to z-score
to do that we can just give it a list of
those columns for instance i'm going to
say height
and age so this is basically saying
deviations for height weight and age but
leave diet alone we don't need to do
anything with that
okay so we've calculated the mean and
standard deviations and now that we've
done that we can use those means and
standard deviations in order to actually
convert these raw variables like all the
data in the data frame to z scored so
we'll do calc
z scores and the way we do that is with
the dot transform function so we're
going to say the dot transform
and then we're going to say which
columns we would like to transform well
we want to transform the same ones as
above so i'm going to copy and paste
this
and put that in here
so again this calculates the means and
standard deviations and then this uses
those means and standard deviations and
uses it to calculate z-scores so before
we had our raw data now let's just go
ahead and run this you can see that we
have our z-scored versions
now we don't just want to calculate
these z-scores we usually want to store
them somewhere and so we can do that by
saying hw square bracket
height
and age
and this is basically adding the left
hand side which says take all these
numbers that we just calculated and
restore them back in the columns height
weight and age in other words replace
the raw columns with their z scored
equivalents so when we do that and then
we look at hw dot head we can see that
we have the same columns as before but
height weight and age have been replaced
by their z-scored equivalents
all right now that i've done that we can
actually remake the graphs that we had
this gg plot and we'll just go ahead and
run that there
and then copy and paste this plot
and run that here
so notice the numbers on the x-axis of
this graph have changed instead of going
from i don't know 150 to 210 or
something like that this now goes from
negative two to positive two but notice
as you scroll back and forth hopefully
you have this on your screen in front of
you the shape didn't change
this is the same exact shape we just
changed the scale or in this case the x
axis that we were measuring things on
same thing goes for the scatter plot
look our x and y axes are now measured
on different scales instead of what was
it before instead of like 25 to 125 6
160 to 200 we now have these scored
variables right negative 2 approximately
to 2 negative 3 to 2 but notice that the
dots look exactly the same they're in
the exact same places and the
relationships between these two
variables didn't change the only thing
that changed was the fact that we're
measuring them on different scales
i say that here because a lot of people
think that when you z-score something
you make it normally distributed or
something like that and that's not the
case so just remember when you're z
scoring you're not changing the shape or
distribution or relationship of the
variable you're just changing the scale
that it's measured on which as we
discussed in the goat part of the
lecture is really important but again it
is not changing the shape nor the
relationship just the scale
all right let's dig into some real data
so this data is on the github and i'm
going to go ahead and run this and so
you have a bunch of books and some like
characteristics of the different books
now the first thing we want to do with
new data especially when it's real is
check and see if there are any null
values so let's see
uh ama dot is null
dot thumb
you can run okay so we have a couple of
missing um data so like for instance
weight is missing in nine of the cases
height and four within five that's not a
ton especially let's say ama.shape
yeah so there's 325 rows so it's not a
ton so i feel really comfortable in just
dropping any row that has missing data
so to do that we can say ama equals ama
dot drop
n a
and this is going to go ahead and drop
any row that has an n a so let's go
ahead and run that
beautiful
so now i'm going to take some time to do
something that typically we're not going
to do in our lectures because it just
takes a long time and it's pretty easy
to do on your own and that's to get a
sense of what all of these variables are
and i'm going to do that using gigi pots
this is something that you should
typically do anytime you're working with
a new data set that you're not really
familiar with so i'm going to make a
ggplot i'm going to say ama is the data
frame we're grabbing from aes
x equals list
price so that's one of the variables you
can see up here
and then i'm going to say plus gm
histogram
and we'll just say plus theme
okay so this is just going to give us a
little more information on what our data
is so we can see that most list prices
are below fifty dollars um in fact most
are probably below what like 25 with the
peak being maybe around 16 to 20. um but
we can see that we have some a very long
tail here of very expensive books for
some reason
um so let's go ahead and check out some
of our other variables
i'm just going to copy and paste this
here and one other thing we can look at
is amazon price so i'm going to change
that
cool you can see actually practically
the same pattern but books tend to be
cheaper there's not as much in like the
20 to 40 dollar range as there is
here um what else can we look at we can
look at num pages the number of pages
and books num
pages
run
okay so most books have
300 pages a couple of really long ones
probably the russian novels um over in
the 700s very interesting nothing too
crazy sticking out here but it's always
important to like look at your new data
um okay what else can we look at the
height of the book
right
okay yeah pretty typical most books are
the same height but we have some really
short books maybe like kids books or
like those tiny little books that you
see um like at gas stations or like
souvenir shops some really tall books um
okay
we can also look i think get
width
yeah
so we'll do width
same thing actually books uh tend to be
not as wide and there's some stragglers
again russian novels very thick very
long
um okay so we can also actually you know
i just realized as i was gonna go do the
next one thickness that thick and width
is different thick
um because
thickness is what i was talking about
right i guess that would be like how
wide the side of the book is like how
many pages there are again a couple
inches makes sense width i guess would
be like how wide the book is but still
we have mostly books that are less than
six inches um but some that are like
eight maybe like i feel like i've seen a
lot of children's books like where's
waldo or something like that wide um so
that makes sense let's see what else do
we have okay weight in ounces so we'll
do
in ounces
[Music]
okay and again around 10 ounces maybe
some stragglers 30 ounces that's
actually a quite heavy book
interestingly some books are less than
an ounce which
i don't even know what kind of book that
could be but hey good for them
so again we typically aren't going to do
this in lectures when we have new
data frames because you can do this on
your own and it's pretty time consuming
as you can see but it's always good when
you're working with unfamiliar data to
just kind of explore it a little bit see
if there's anything that's surprising
for instance i was a little surprised at
books that weigh
less than a full ounce but you just want
to be aware of your data see if there's
anything that looks weird because of
course there could always be like a
mistake in the data and you just want to
be aware of like what's in your data
all right
now for the part that i'm sure you're
looking forward to now that we've
thoroughly explored our data and talked
a little bit about z-scoring let's
combine those things to use z-scoring
and model building
so when we have a linear regression
model we have some variables that are
predictors and something that is our
outcome the predictor is a variable that
we have that we're going to use to
predict whatever our outcome is
so for instance in this case i want my
predictors to be well
pretty much everything the list price
the number of pages the weight the
thickness the height and the width of a
book and what i'm going to do is i'm
going to predict the amazon price so my
predictors are all of these variables my
outcome that i'm predicting is the
amazon price
so i'm going to go ahead and set up my
x's which are going to be my predictors
and my ys which is going to be my
outcome
so x is going to be all of these columns
so i'm going to say ama square bracket
predictors
and this is basically just saying from
the ama the amazon data frame grab me
these columns
then i'm going to store in y just to the
column that i am predicting which in
this case is amazon price so i'm going
to say ama
amazon
price
all right so i'm going to go ahead and
run this and this is just helpful we
don't have to do this technically but
it's really helpful to have your
predictors o as an x and your predicted
or your outcome variable and why just to
help you keep track of it
so now that we have our x's and our y's
separated we can z-score all of our
this is going to be really useful when
we look at the coefficients of the model
right because when all of our variables
are on a more similar scale because
we've z-scored them it's easier to
compare the impact they have on our
outcome now just to note you totally can
z-score the outcome of your model as
well we don't often do that in this
class for one main reason
usually the data sets that we're working
with are something that we're like
vaguely familiar with for instance like
amazon price you know most of us have
bought books off of amazon so we kind of
like intuitively know what their prices
should be
and so when we make predictions it can
sometimes be useful to have it in like
raw units just because we understand
them intuitively whereas z-scores can be
a little harder to understand
intuitively so just as a little warning
you totally can z-score your outcomes
often we're not going to but it's
totally a valid thing that you can do
all right with that said let's create
our z score object the goal standard
and then we need to fit and then
transform so i'm going to say z dot fit
and actually all of my x variables can
be z scored because they're all numeric
variables so i'm just going to say z dot
fit x
and then i am going to well i have two
options what i'm going to do here is i'm
going to create a new variable called xz
which is my z scored x's and i'm going
to say x z equals z dot transform
x
so in the previous example what we did
let me scroll back up to it
is
we z-scored our variables and stored
them back in the data frame
absolutely something we can do sometimes
we want to keep them separate or we want
both the raw and the z-scored in which
case you can just store them in a new
variable it's really up to you depends
what's most elegant and convenient in
your case
so just to review we created an empty
z-score object we fit it using x which
is basically saying for all the columns
in x calculate the means and the
standard deviations and then finally use
transform to actually calculate the
z-scores for all of the raw data
so let's go ahead and run that and now
we have to create a linear regression
model
so to do that actually pretty similar to
z-scoring i'm going to say lr model
equals
linear
regression and again this is just what
sk learn calls the linear regression
and now that we have an empty model
similar to what we did with z-scoring we
can dot fit to actually like calculate
what the model is this empty model
hasn't done anything it doesn't have any
data we need to use dot fit in order to
get it to calculate anything
in order to do that we're going to say
the name of the model lr model dot fit
and then we're going to tell it uh then
where the data is that we want to use to
fit our model so this is going to be our
x and our y
lr model or any linear regression model
takes in at least two arguments you have
to give it the x data so the predictors
comma the y data what they're predicting
so in our case we actually want to use
the z scored data so our x is going to
be x z and our y is going to be y
so let's go ahead and run that and we
have a beautifully fitted linear
regression model
so now that we have a model we probably
want to see how it did and there's a
couple of ways to do that so first what
i'm going to do is grab predictions from
the model so when you build a model
basically the whole point of the model
is for it to predict whatever it is the
outcome is so in this case amazon price
so what i'm going to do is i'm going to
grab from our model what it thinks the
price is for all of the books in our x
data set
so i'm going to just call these price
pred
and i'm going to say the name of the
model lr model
dot predict and then i'm going to give
it the data
now because we trained on z-score data
we want to make sure that we're giving
it z-score data when we predict so the
price print and actually let me just
show you price
let's grab 10 of them this is going to
be the predicted prices so what our
model thinks the price is for all of
these books in our data set
so you can see for instance it thinks
the first book in our data set should
cost about
10.91 so 10.91
okay now that we have our predictions we
can use those to calculate some metrics
that will help tell us a little bit more
about how well our model is doing um so
right when you have a prediction one
thing that you can do is measure the
error or sometimes we call them the
residuals which is basically how
different is the prediction from what
the actual value is so in our case how
off was our prediction of amazon price
compared to what we know
the actual amazon price was
so we're going to use two metrics first
we're going to use mean
squared
error
and mean absolute error
and as the names uh
imply these are both really similar
similar mean squared error takes the
error for each book so how different the
price was from our prediction uh squares
them and adds them together and then
takes the mean the mean absolute error
is taking the different errors taking
the absolute value and then taking the
average or the mean um so these two
things are basically like
really similar the difference is this is
squared this is absolute value
mean squared error is often used for
some reason because you know
mathematically it's a little bit easier
to compute
but the one downside here is that the
mean squared error is a little harder to
interpret right because it's like
squared errors so it's like
the value you get the average value that
you get is the average squared error um
that's why we sometimes like mean
absolute error because it's literally
like the average deviation
from our guess so like how far are we
off on average this will make more sense
when i show this to you in a second but
both mean squared error and mean
absolute error take two things in order
to calculate it
first we need the actual values which we
know are stored in y and then we need
the predicted values which we just
calculated and stored in price print so
i'm going to go ahead and copy and paste
and then i'm going to run them and
actually let's make sure we print them
out
so we can see them
okay so you can see the mean squared
error is 10.6 dollars squared and the
mean absolute error is 2.16 at dollars
so again this is why i sometimes mean
absolute error is a little more
comprehensible for human beings although
like i said mean squared error is pretty
common
this basically means that our model on
average was about two dollars and
sixteen cents off and the reason why
this is really helpful to us and one of
the reasons why i left the y value the
amazon price as a raw variable instead
of z scoring is because this really
helps us because if we know or like have
any concept of how much books cost we
can go hmm
is being two dollars and sixteen cents
off bad or is that pretty good and in my
opinion that's pretty bad
books often cost between what like
nine to 25
so if your guesses are on average off by
two dollars and change you're pretty far
off like it's not horrible you're
getting in the right ballpark but you're
not doing that well um so we can use
these to basically see like how well our
model is doing
another metric that we talked about is r
squared and that actually takes the same
exact uh variable so i'm going to copy
and paste this here but we're going to
use r2 squared
so when i run this you can see that our
r squared was about 0.92 which means
that our model explains about 92 percent
of the variance in the data
it means it's doing significantly better
than just guessing the mean price so our
model is actually not doing that bad
although we do know that it's about two
dollars off
so it's not doing bad but it's not doing
great we could maybe improve it although
we obviously don't have any more data to
do so here
but we have all of these metrics now to
kind of gauge how good of a job our
model is doing the r squared score
basically points to it's doing pretty
well
but practically we can see that with the
mean squared error and the mean absolute
we have a little bit of room for
improvement
and this would help us decide right if
our model is doing well but it's off by
about two dollars do we really want to
like deploy that algorithm anywhere or
is two dollars not really a big deal in
our context these are the types of
things that we want to think about when
like evaluating how well our model is
doing
all right now that you've built and
evaluated your first linear regression
model let's talk a little bit about
assumptions that the model makes
now there's a lot of assumptions for
linear regression and actually some
matter more than others in the context
of like machine learning where we're
often focused more on prediction than
inference again just to review inference
tends to be when we want to know like
the truth about the relationship between
variables you may have in a different
class seen things like p-values
confidence intervals stuff like that
that's all doing inference often in
machine learning we're more focused on
the idea that like our predictions are
accurate that's why when we calculate
like the mean squared mean absolute and
r2 score that we're like seeing how
close our predictions are
um so just a really quick caveat i'm
going to show you this because i think
it's really important but typically when
we're doing machine learning we care a
little bit less about this assumption of
normality that we're going to talk about
first than we do about some of the
others that we're going to talk about in
a second
so first i'm going to make a data frame
to help us with all of these assumptions
i'm just going to call it a sump
and i'm going to say pd.data
frame and i'm gonna make a dictionary so
the first thing that i want in this uh
data frame is the error so this is gonna
be the difference between my prediction
and the actual value so i'm gonna say y
minus price
pred and this is for each book going to
calculate the difference between the
actual price and what our model
predicted was the price
the second thing i want in this data
frame is the predicted
predicted price which is just price
so i'm gonna go ahead and run that oh
and it's lowercase y
okay i'm gonna go ahead and run this and
then i'm gonna show you really quickly
how we can check the assumption of
normality for our models again people
tend to ignore this one a little bit
more when dealing with machine learning
which is focused more on prediction but
i just want you to be aware
so we're going to use our trusty ggplot
and we're going to say the sump data
frame is where we want to grab our data
and we're going to say aes
x no we're going to say sample equals
and then we're going to use this new
function that we haven't seen before
with ggplot called qq or stat
qqq
and then of course let's plot
theme
okay so really quickly what i'm going to
do is run this we're going to make this
prettier in a second but i want you to
see a little bit about what a qq plot
looks like
okay so the qq in the stat qqq stands
for quantile quantile plot and it's
basically a way to compare your data
that you observe to some distribution
that you want to compare it to
in our case we're looking to see whether
the errors or the residuals are normally
distributed
now just a quick caveat sometimes people
make a mistake here oftentimes people
will say mistakenly that the assumption
is that the variables like your data is
normally distributed that is not
necessarily true the assumption is that
distributed but sometimes people confuse
those two things so what we're looking
for roughly in this q q plot is that our
data is following kind of this straight
sloped line and i think it'll be a
little easier to see if i add that line
in so what i'm going to do is i'm going
to add a geom
a b
line this allows you to give a line by
its slope and intercept and i'm going to
say intercept equals 0
slope equals
and this is a little uh confusing so
stick with me for a second what we're
going to do is make the slope the
standard deviation of the data so we're
going to do this this is grabbing the
standard deviation of the errors and
using that as the slope
so once we have that let's also just
make this line red we'll say color
equals red just so it stands out a
little bit more okay
let's go ahead and run so same plot but
now we have this straight line and
basically what we're looking for is that
this data doesn't deviate that much from
this line
but i can hear you already through your
screens chelsea
what does too much mean
i'm glad you asked well not really
there's not an easy answer unfortunately
there's no like good quantitative way to
say what is too much how different can
our data be from this red line before we
go oh no that's too much
so what i want to do
is i want to show you some qq plots of
normally distributed data so basically
what i'm going to do and here let me
copy and paste it here what i'm going to
do is i'm going to generate fake data
that we know is normally distributed i'm
using that mp.random not normal that we
learned about in our python pandas
lecture and i am going to make a qq plot
for what we know
is normally distributed data
so let's go ahead and run that okay so
this is one random sample of normal data
and the qq plot for that sample now it
does look a little bit closer than ours
right we see these data points are
pretty high on the left hand side we
also see at the tails so like here and
here that there's some pretty big
outliers um so it does look a little bit
different than here let's go ahead and
draw another sample right because that's
just one random sample let's look at
another one
oh what did i do
again lowercase y
okay and here's another one so we can
see that when the data is normally
distributed we do see some deviations
around the line right we know this data
is normal we made it ourselves but we
can see that like especially at the ends
you know there's sometimes some
deviations here this sometimes some
deviations in the middle um and ours
looks actually pretty good in the middle
but we do have some extreme values so we
would maybe in this case be a little
concerned that the assumption of
normality of the residuals or the errors
is violated here because of those
extreme values now i'm just showing you
what to do here in case you need to
check normality this is what you would
again the assumption of normality
affects more inferential models where
you're calculating p-values or
confidence intervals but technically it
is an assumption of linear regression so
i wanted to show it here although from
now on we'll typically skip this
assumption and move on to the next ones
which is what i'm about to do now
all right so let's talk about
homoscedasticity which is a mouthful to
say trust me when you say this in class
i do not expect you to say it correctly
because i can barely say it correctly
half the time but what we're checking
for is something called homoscedasticity
and that's contrasted with its opposite
heteroskedasticity
as i'm sure you're familiar with with
like homozygous and stuff like that homo
means the same hetero means different so
when we're talking about
homoscedasticity
what we're talking about is that the
errors are approximately distributed the
same way across the model
now i hope you wrote that down but i
want to show you what i mean
so what we're going to do is we're going
to use our a sump data frame let's say
ggplot a sump
and we're going to say on the x-axis put
the predicted value so this is what the
model thought the value was and on the
y-axis we'll put the error
so we're going to say plus geom
point
and of course plus the minimal
plus me
and let's go ahead and run them
so you can see here these are our errors
on the left hand side and on the uh
bottom we have the predicted values so
basically if our model was perfect right
we would expect all the errors to be
zero
what we're looking when we're looking
for
homoscedasticity which is the assumption
that there is homoscedasticity
we're looking for the distribution of
errors to roughly be the same as we move
our eyes from left to right across this
graph so as we go from low predicted
values to high predicted values we
expect that roughly
um the values are going to be
spread evenly right so we kind of want
this to look like a blob right like an
amorphous cloud of points now in this
case we actually don't see that the
errors are spread pretty uniformly at
the lower end but they get huge at the
upper end which means we actually have
heteroscedasticity
that means that the spread of the errors
as you move your eyes from left to right
is not the same
there's an area at least one in our
graph where the errors are spread more
widely than in another area of our golf
so you might be thinking oh no we've
just violated an assumption
what do we do well the answer in real
life is we would need to use a different
type of model in order to account for
this in this particular instance our
answer is well
this is the only model that we have
learned so far so we're going to keep
using it but it's really important that
we know that there are differences in
the schedasticity and the spread of the
errors as we go from low predicted
values to high predicted values
and we'll talk a little bit more about
this later but one thing that you want
to think about is well when we see this
pattern of errors this means that
there's certain sections of our
predicted values for which our model is
just better and instead of books imagine
if this was people imagine if we were
guessing medicine dosage or
how much money someone could get on a
loan or something that actually could
have an impact on human life well if our
model is better at predicting values for
a certain range that means that people
who aren't in that range may be more
disadvantaged by our predictions maybe
we're under predicting the amount of
medicine that someone needs maybe we're
over predicting the amount of money that
someone could have in a loan and safely
pay back
so we want to make sure that we're very
careful and paying attention to kind of
the ethical implications of violating
assumptions in our models
our models are operating on the
assumptions that we talked about like
normality residuals homoscedasticity and
when we violate that sometimes it's
going to be totally fine and sometimes
it can have a huge impact on human life
so we want to make sure that we're
paying attention and we're aware when
these assumptions are violated so that
we can make the decision of whether to
move forward or to use a different model
all right so sticking with this graph
for a second we just used it to check
for homoscedasticity
but the other thing that we can use to
check for is linearity
so we talked a little bit about in the
previous lecture that linear regression
assumes that the relationship between
the variables your predictors and your
outcome what you're predicting is linear
and we can use this graph to kind of
give us a rough idea of whether or not
that's true
so remember when we're looking for
homoscedasticity we're basically moving
our eyes from left to right and going is
the data roughly equally spread out
around this zero here again we said in
this case it's not when we're looking
for linearity we're basically looking
for any clear
non-linear pattern something that
indicates to us that there is something
wrong and the relationship between our
predictors and our outcome is not linear
so in this case even though we do see a
heteroscedasticity right it's the errors
are smaller here and bigger here i
actually don't see a huge problem with
non-linearity so we see that these are
kind of like a weird amorphous blob and
even though these errors are bigger it's
not like the um errors are like in an s
shape or a u shape which we'll see in a
second so basically what we're looking
for in this plot is are the errors
roughly roughly evenly spread across
left to right and is there any clear
non-linear pattern
now when we find a non-linear pattern
we're often going to want to figure out
what's causing it
so the reason i have this weird word
here omnibus linearity is basically this
plot can help us check for linearity
overall
but if we see any non-linearity we are
going to need to figure out which
variable might be causing it
so to do that what we can do is actually
a lot simpler we can use ggplot we'll
say ggplot ama
aes we're going to put our predictor
variable so one of our like predictors
uh let's see list price we can do
on the x-axis and then on the y-axis
we're going to put our outcome so in
this case amazon price and then we're
just going to make
a scatter plot
let's theme
there we go
okay so for instance you can see here
we can see the exact relationship
between list price and amazon price
now similar to the graph above what
we're looking for here is any clear
non-linear pattern so what we see here
is a very linear pattern right there is
a positive relationship between these
two variables and it's roughly
approximated by a straight line
so in this case at least for these two
variables we see that the assumption of
linearity has likely not been violated
so when we see non-linearity what we're
going to want to do usually is look at
all of our continuous variables that are
predictors in the model and make a plot
like this for each of those variables so
for instance i could also look at
um
num
pages and amazon price
so you can see here there's some
outliers here but the trend is roughly
linear uh it's not like there's a
completely unstraight line or like a
curvy relationship to the data even with
some outliers we have a pretty straight
line so when we find non-linearity in
this plot we often want to look
individually at our predictors to see
where that linearity comes from or
excuse me the non-linearity comes from
now just fyi if you want to skip this
step you can always check for linearity
by just looking at each individual plot
down here uh instead of looking here
first but this can kind of serve as like
a litmus test right if you see clear
non-linearity then you're going to need
to go looking for where it comes from
but if everything looks roughly okay you
can assume that the assumption of
linearity has not been violated
now that was a lot of examples and a lot
of information so what i want to do is
show you some data that does or does not
violate some of these assumptions and
what that might look like
again even when our data does not
violate our assumptions it's not going
to look perfect it's not going to be in
an exactly straight line or a perfectly
even blob for our heteroskedasticity
graph
so here's some examples of some data
that do and do not violate these
assumptions
so in this case we have some data that
has a medium relationship between the
two variables but it is a non-linear
relationship so let's go ahead and run
and this is what the raw data looks like
very non-linear again this is a really
simple case where we only have one
predictor and one outcome but you can
clearly see here this is a non-linear
relationship it's like an upside down u
so when we run linear regression on this
and make our various plots first of all
here's our first plot this
heteroscedasticity plot that we made
you can see clear
non-linearity right we can see that
there's not like an amorphous blob of
points instead there's a u-shape that
shows us that there is some
non-linearity in this data now because
there's only one predictor we know where
that comes from in the case where there
is multiple predictors we would have to
individually go through and check which
one is causing this non-linearity
however even though there's not
linearity we actually don't see a huge
violation of heteroskedastic excuse me
of homoscedasticity
because even though it's in a weird
shape the errors are roughly evenly
spread again it's not perfect it's never
going to be perfect in the real world
but we don't see huge changes in the
spread of the data points as you move
your eyes from left to right so in this
case if this were my data i would say
this very clearly
violates linearity but doesn't
necessarily
we don't have evidence that it violates
now here's some more data that has a
non-linear relationship but a very
strong relationship
here's the raw data you can see that
it's actually a sigmoid curve
we're doing a sine wave here in this
case and so you can see the relationship
is non-linear
so when we do a linear regression and
make that a homoscedasticity plot we can
see a clear
non-linear pattern right we clearly see
that there is something going on that is
not linear here so we are very
suspicious that this has violated the
assumption of linearity however it
doesn't necessarily like above violate
the assumption of homoscedasticity
because even though there's a little bit
of difference in spread we're never
going to get it perfect and as you move
your eyes from left to right the spread
isn't
completely different so in this case
like above i would say definitely
violates linearity we're not really that
concerned about it violating
all right but let's look at some data
that has a very clear heteroscedasticity
so this is going to be data that is
linear related but has
heteroscedasticity here's the raw data
let's go ahead and run our linear
regression make our plot okay so as we
move our eyes from left to right
we see a very clear change in how the
errors are spread
now again we never expect it to be
perfectly the same but this is very
clearly fanning out as we go from left
to right and this is a really common
it's not the only but it is a really
common pattern of heteroskedasticity
where we see very small errors on one
end and like fanning out ballooning
errors on the other end so in this case
we are very suspicious that this is
violating the assumption of
on the other hand the trend is
relatively linear even though the errors
are distributed differently overall
there's kind of a flat line
so we're not really concerned that this
is violating linearity because there's a
linear trend here even though we do see
all right we'll look at more examples in
lecture together but i just wanted to
give you a good idea of what you can
expect when you see
issues with heteroscedasticity or
linearity you're looking for very clear
patterns just because there's one or two
data points somewhere or something's not
quite perfect doesn't mean that an
assumption has been violated we are
looking for very clear patterns of
heteroskedasticity or non-linearity in
these plots
all right now that we've made it through
all of those lovely assumptions let's
talk a little bit about coefficients
so we can pull coefficients from the
sklearn model from the linear regression
model that we just built and usually
what we're going to do is we're going to
put it into a data frame so that we can
look at it or visualize it
to do this i'm going to store it in a
variable called coefficients i'm going
to say pd.data frame
and what i'm going to do is i'm going to
grab the coefficients from the model and
the names of the coefficients or like
which variable each coefficient goes
with so to grab the coefficients i'm
just going to call that co-f
i'm going to say the name of our model
lr model dot co f underscore that is
going to grab all of the coefficients
from our model for all of the predictor
variables and so i'm also going to grab
the names of those predictor variables
so that we can
know which variable goes with which
coefficient now because we stored these
before
right if i scroll up
we stored these originally so i'm just
using that there
so when i put these two things in let me
actually print it out so you can see it
fish
okay um you can see that we get a table
or a data frame with all of the
coefficients and then their various uh
names
now one thing that we often want to do
here but not always is add the intercept
of the model and we can do that by
saying coefficients equals
coefficients
coefficients there we go dot appends
we're adding a row
and we're going to say co-f equals lr
model so the name of our model dot
intercept
and then the name
of this is obviously going to be
so when we run this
oh and i need to put right ignore
there you go index equals true
okay so when we do this you can see that
we have the same data frame as before
we've just now added an entry for what
the intercept of the model is
now we went over this in the conceptual
lecture but just to review now that we
have our coefficients we can look at
what the relationship is so for instance
we know that all of these are z scored
so when we look at the list price we
would say a one standard deviation
increase in the list price is associated
with an 11.4 dollar
increase in the a predicted amazon price
similarly although in the opposite
direction an increase of one standard
deviation in weight is associated with a
41 or a 42 cent decrease in the
predicted amazon price and again we
could go through that with all of these
just to review the interpretation of the
intercept would be when all of these
predictor values are zero this is what
the
predicted amazon price would be
so the coefficients right are what the
model uses to predict new values so
let's say i have this new book that is
um 1245 list price is 300 pages 10
ounces uh 0.8 i think it was inches in
thickness uh 8 inches in height and 5
inches in width
we're going to store that information in
the variable new book and then what
we're going to do is we are going to
predict what we think the price of this
book would be based on all those factors
now because this is raw data and our
model is trained on z-score data the
first thing we need to do is z-score our
data so we're going to say new
book z
z dot transform
new
oops new book
and again this v is just from when we
trained our model originally let's
scroll all the way up to where we did
that this is just the same z-score
object that we used up here
and then we can use our model to predict
what we think the price of this book
would be so let's say lr
model dot predict
book c
so this is basically asking the model
what do you think the price of this book
so you can see here that this thinks
that the price of this book should be
about 8.42
now i typically would not make you do
this but i think it's helpful to do it
once by hand so what we're going to do
is we're going to recreate this value by
hand
we know that what linear regression does
is it takes these coefficients
multiplies it by the values for that
book and then adds them all together so
we're going to just do that by hand one
time so you can convince yourself that
this is what's happening when linear
regression runs dot predict but i just
want you to do it by hand this once
so what we're going to do
is we're going to take our z scored
values which we already have calculated
and we're going to add a 1 and this is
because when we have an intercept we
need to make sure to add that intercept
adding a 1 to the end is just how we add
that intercept so we're going to say new
book
enter equals np dot append
all that right there we go
new book z and one
so let's just look new book
enter and let's look at what that is so
basically we have all the z scored
values from our original book and we've
just added a one to the end
to take all of these values that our
book has
and we're going to multiply them by the
coefficients from the linear regression
so we're going to grab those coeffs
we're going to say
bracket
co-f let's just show you what those are
really quick right
co-f
you can see these are the coefficients
from our data frame we have the
coefficient for list price and then we
have the list price z score we have the
coefficient for num pages and then the
num pages z-score we have the
coefficient for weight in ounces and
then the weight and ounces z-score etc
and finally we end with uh we have the
intercept and then we have that just
multiplied by 1.
so what we're going to do is we're just
going to multiply those coefficients
by
the values from the book
so to do this let's just go ahead and
turn this into an array so we're going
to turn this into a numpy array
and we are going to multiply each of the
coefficients by
the new
uh inter
so this is going to take the z scored
values multiply them by the coefficients
and then well let's see what that looks
like so you can see each of these have
now been multiplied and then all we have
to do is take the sum of that mp.sum
and run and you can see we get the exact
same prediction although it's not
rounded that we got from our model this
is all just to prove to you that when
linear regression is creating a
prediction all it does is take the
values for that individual subject
person book whatever and multiplies it
by the coefficients and adds them all
together
all right last but not least let's do an
example so we're going to load in this
data this is some beyonce spotify data
that is on our course github and we're
going to load this in and we're going to
ask ourselves can we predict
danceability which is something that
spotify rates you know how danceable a
song is based on a bunch of different
variables
let's see what we have available to us
so we'll say b dot columns
so we can use a bunch of these different
variables to predict dance ability
so i think in this case we're going to
use loudness speechiness acousticness
instrumentalness liveness and valence
so first as usual i'm going to set up my
x and my y x is going to be b bracket
right that's saying from the b data
frame grab me these columns
and y is going to be b
dance ability
okay so now that we have our x and y
let's go ahead and z score so we're
going to say
z score equals uh standard
and then we're going to say z score dot
x right because all of these are
continuous so they can all be z scored
and then we're going to say
x z equals z score
dot
transform x
so basically here we're just scoring our
variables and then storing them
separately in x z
so now that we have that we can create
our model we're going to say bay
mod equals linear
regression
and then we can fit our model so we're
going to say bay
mod.fit and again we need two things we
need the predictors so that's going to
be our x z because we want to use z
scored variables and then our outcome y
so let's go ahead and run all of these
oh we need to run this first run run run
run run
beautiful so now we have our fitted
regression model and now that we have
that model we can use it to
tell how well the model does so the
first thing that we probably want to do
is grab some predicted values so we're
going to say dance pred equals bay
mod.predict xz
and then we can for instance calculate
like the mean
squared oops
squared error
and mean squared error we need the
actual values comma the predicted values
so we can look at what that is and then
we can also calculate the r squared
let's copy and paste this
and do r2 score
beautiful okay so we can see that our
model didn't do
great
uh the r squared is about 58
so it's not
horrible um but it's not great right so
it's only accounting for nearly 60 of
the variation which means a lot of the
variation is left unaccounted for but
we're not doing horribly right we're not
like completely off we're in the right
ballpark so i would say this model is
performing moderately well
now what if we wanted to ask ourselves
well what if we
added a new variable to our model does
that improve it
well let's see if we look up here one of
the variables we didn't use but could
use is energy so let's go ahead and copy
our predictors
and just add
energy
so we're going to do
that and then we're basically going to
redo what we've done up here so we're
going to say x equals b bracket
y equals b bracket
danceability
we'll say z equals standard
and then we'll say z dot fit and again
all of these are continuous so we can
z-score all of them
and then x c equals z dot
and then let's create another model
we'll call it
i'm feeling lazy we'll call it lr equals
linear regression
and then lr.fit our predictors xc and
our outcome y
then we can make dance
red two
equals lr dot predict xz
and of course we can calculate our r
squared and our mse
so what i'm going to do here
is i'm just going to copy and paste
change these variable names so basically
what i'm going to do is what we just did
but now with energy as an extra
predictor
so let's go ahead and run okay so
interestingly we can see that when we
added energy it actually increased our r
squared you can see it used to be about
58.8 now it's about 59.8 so it's not a
huge increase but we can see that adding
energy did increase our ability to
predict uh danceability in beyonce songs
and then last but not least maybe we are
interested in um like the effect of the
different variables like how much of an
influence they have so we can create our
coefficient data frame we'll say co-f
missions equals pd.data
frame
and we'll say uh co-f
equals lr dot co f
and names
equals predictors and actually i'm not
going to bother adding the intercepts
because i don't really care about it
right now
so we'll do this
okay so we can see the effect of all the
different variables on our prediction of
for instance we see that loudness is
positively associated with danceability
so for every one standard deviation
increase in loudness we expect our
predicted uh danceability value to go up
by about 0.2
um things like instrumentalness and
acousticness have negative relationships
with danceability and at least in my
head that kind of makes sense because if
something is acoustic then it's less
likely to be like you know a dance pop
song so it makes sense that those are
negative um valence has a positive
relationship which makes sense because
like valence is like the
emotion where like high is positive so
like danceable songs tend to be a little
more like happy and not like super
depressing songs so it makes sense to me
um so we can see all of our coefficients
if we want to we can do the
interpretation for all of them but
overall we can see what the different
effects are and maybe we want to make a
plot of that
so let's pull out good old ggplot
and we'll say ggplot uh
coefficients there you go yes and we'll
say
x equals
y equals co-f and we'll do a bar chart
so we'll say gm bar
and then stat
equals identity
there we go oh and let's actually let's
fill
names oops
ran before i could type that names as
okay so we can see this beautiful
coefficient chart although we could
change some of these things to make them
look a little bit better
that tells us a little bit about the
relationship between the variables so
for instance loudness and valence both
have a positive relationship speechiness
has a nearly zero relationship and then
these four over here which is
acousticness instrumentalness and
liveness oh and energy all have negative
relationships
all right i've turned on long enough i
will leave it there and i will see you
next time

 
hello and welcome to your decision trees
and python lecture as usual the first
thing we're going to do is go ahead and
run all of our import statements
while that's running we're going to load
some data so this data is credit card
fraud data and let's take a look at the
head of the data
you can see here that we have a fraud
variable which indicates zero it was not
a fraudulent transaction one it was and
then we have home whether or not it was
made in the area the city of the person
who owns the card amount is the amount
of the purchase and then cash back
indicates yes or no whether or not cash
back was a part of that transaction so
let's use sklearn to build a model and
again our model workflow for building
this is going to be very similar to all
the other models we've built the first
thing we're going to do is set our
predictors and our outcome here we're
going to use a home amount and cashback
to predict whether or not there's fraud
so i've set up my x and my y like this
all right so now that we have our x and
y data we can go ahead and do some type
of model validation for this example i'm
going to use an 80 20 train test split
so i'm going to say x train
x test y train
y test equals
train
test split i'm going to give it my x
variables my y's and then say test size
equals 0.2
now that i know what's in my training
set and my testing set i can go ahead
and do things like z-score so let's do
that let's say z equals standard scalar
and then fit and remember only the
training set should be used so we're
going to say z dot fit
x train and the only one of our three
columns that we're using is a predictor
that is continuous is amount so that is
the only one that i want to z-score so
i'm going to say x train amount
and then i'm going to go ahead and use
that because remember dot fit just
calculates the means and the standard
deviations i'm going to then use
transform to actually calculate my z
scores for the training in the test set
so i'm going to say x train square
bracket
amount
got my quotes correct there
equals z dot transform
x
amount all right and then let's copy and
paste
and do the same for the test set so
we're going to say x
test
all right now we can go ahead and create
our model so the first step in our
sk-learning workflow always is to create
an empty model so i'm going to call mine
tree and set it equal decision
tree
classifier
and then i'm going to fit it and
remember we only fit on the training set
so tree dot fit x train y train
beautiful let's go ahead and run our
models
and it looks like i spelled something
wrong decision tree
classifier there we go
and run again okay beautiful so now we
have a fitted model so let's go ahead
and evaluate our model
we've learned a bunch of metrics we can
use the first one i'm going to use is
accuracy so i'm going to say print
train ack
and then i'm going to use my accuracy
score so i'm going to say accuracy
score and remember we give it actual
comma predicted so we'll say y train and
then grab the predictions for the
training set we'll say tree dot predict
and then let's do the same for the
testing set
so we'll change that to test ack
y test and x test
then for speed i copy and pasted in the
other metrics but we're going to grab
the precision the recall and the roc auc
for both our training and test set as
well let's go ahead and run these
so go run
beautiful okay so let's go ahead and
look at some of these metrics so our
training accuracy is a hundred percent
which is literally the highest you could
be but our testing accuracy is only
82.5
which indicates here to me that there's
probably some overfitting going on you
can see that there's a huge
very notable difference between the
training accuracy and the testing and
accuracy so it makes it suspicious that
overfitting is happening and you can see
the same patterns in some of our other
metrics where our
training recall precision and roca you
see are all perfect while the metrics
for the test set are not as good
so decision trees actually overfit quite
a lot and when you think about how
they're made hopefully this makes a lot
of sense
if you let a decision tree keep going
over and over it might create a tree
that perfectly classifies every data
point by making every data point its own
leaf node this would certainly perform
well for the training set but probably
not for the testing set because our
decision trees get complex quite quickly
they are very prone to overfitting let's
take a closer look by looking at the
confusion matrix to grab our confusion
matrices i will say plot
confusion
matrix and i will give it our model our
x data and our y data
so let's go ahead and print both of
these out here i'll do this in a
separate tab and we'll do one for the
test as well
all right so let's run these
and you can see why our metrics look the
way they did for the training set our
confusion matrix looks perfect we made
absolutely no errors you can see that on
the half diagonal
in our testing set we did make some
errors and that makes sense because as
we were seeing in our just numeric
metrics we were not getting as good of
accuracy because we
weren't actually getting perfect
accuracy
so when our trees are very big they tend
to over fit just like this example one
thing we can do when we get a model like
this is check just how deep our tree is
if you think of a decision tree as going
from split to split to split to split
the depth of a tree is how many splits
it has at its deepest point basically
what is the most number of splits that
the tree has in a row
we can grab this number by saying tree
dot get
depth
oops nope
there you go
and we run it for this tree we can see
that our tree has a depth of nine that
means at some point in our tree there
are nine straight uh splits and that's a
pretty complicated and deep tree another
way to measure the complexity of the
tree is with the number of leaf nodes
that it has
it makes sense that the number of least
nodes would be related to the depth of
the tree the deeper the tree the more
splits you have the more leaf nodes
you'll have so we can say tree dot get
n
leaves
and that will tell us how many leaf
nodes the tree has in this case it has
37 which means when you start at the top
of the tree and work your way down there
are 37 different leaf nodes that you
could end up in this seems like a really
complex tree especially for the size of
our data so one common way to prevent
this extra complexity is to limit the
maximum depth of your decision tree when
we create a decision tree the first
splits as you know from the theoretical
lecture are going to be the broad
categories that reduce the genie
impurity or entropy the most because the
data are all together at the top of the
tree these broad splits are going to
better separate the data however as we
get to the bottom of the tree the splits
get more and more like splitting hairs
you know if you and another data point
have made it together through nine
layers of a decision tree you're
probably two data points that are very
similar and so if we made another split
to split the two of you up well that
would be a very minut tiny and probably
insignificant difference between the two
data points
because these differences at the bottom
of the tree are so
miniscule so tiny and probably a product
of our sample rather than a general
pattern in the population
these lower branches are what end up
causing the overfitting in our tree
so if we want to prevent that
overfitting we can simply chop those off
to do that we can use max depth so what
we're going to do is when we create our
tree we are going to tell it a limit for
how deep it can go
to do this let's say tree equals
decision tree
and then we're going to say max depth
equals and let's try three then we're
going to fit it again x
train y train and then let's look at the
uh metrics to see how well this tree
does so let's run that and then for ease
i've copy and pasted in all of these
metrics we can see that this limited the
amount of overfitting that's going on
now the differences between the train
and the test set performance for all of
our metrics are much less severe than we
saw before where we were basically doing
perfectly on the training set and okay
on the testing set but with a large gap
in the performance of the training and
the testing you can see if we try and
grab it using tree.get
that our tree is now a lot shallower
with a max depth of 3
instead of 9 which is our previous
unfetteredly grown tree
now technically your tree could have
fewer layers than the max depth but
typically we'll choose a number low
enough that the tree will have the max
depth number of layers another way that
we can limit the complexity of the tree
in a little bit more of a targeted way
is with the min samples per leaf
remember a leaf node is going to be
where we make a classification it's the
terminal node of the tree instead of
limiting the max depth we can basically
set a minimum number of data points that
a leaf needs to have in other words
imagine a really complicated tree where
every single data point gets its own
leaf node that tree is going to be
horrendously over fit so one thing that
we can do is set a minimum number of
samples that a leaf needs to have in
order to be a leaf node this prevents
superfluous splits because if a node has
say 10
samples and our minimum number of
samples per leaf is 10 it won't split
further even if it could improve the
genie impurity or the entropy it'll stop
splitting at that point
so to do that we're going to say tree
equals decision
there we go tree classifier we're going
to say min
samples leaf instead of max depth and
i'm going to set it to that number 10.
this means that all of our leaf nodes
must have at least 10 data points in
them
then we'll go ahead and fit we'll say
tree dot
fit x train
y train
and then for uh simplicity of time i'm
going to copy and paste in our metrics
and go ahead and run you can see that
setting min samples leaf to 10
similarly to limiting max depth reduced
the complexity of our tree and prevented
the overfitting problem we saw in the
first tree that we fit
for instance you can see that when you
set min sample's leaf to be 10 it will
also limit the depth of the tree
get depth
and run
and you can see that instead of having a
tree with nine layers deep at some point
we now have a tree that has at most four
layers or four splits deep so these two
metrics the maximum depth and the min
samples leaf both do similar things in
reducing the complexity of the tree and
we can actually use them together if we
would like there's no hard and fast rule
about what you have to do and you can
experiment around with what works best
for your data
i would say that i lean a little bit
more towards min samples leaf because
imagine say we have an imbalance in the
data and when we do our first split
there's say 90 data points in the right
hand node and 10 in the left hand node
well limiting a max depth just means
that overall in the tree there can only
be a certain max depth it doesn't really
take into account any of the imbalances
in the splits that we make
for instance i might not want to make
that many splits on that left hand node
that only has 10 data points but i might
be willing to make a couple more splits
on the right hand node that has 90
because there's just a lot more data to
be split there and i'm a little bit less
worried about over fitting when you set
the minimum number of samples needed to
make a leaf node you sort of allow for
there to be more splits when there's
more data in a node however in real life
when you're working with real data you
might have to play around with one or
more of these metrics that limit the
complexity of the tree
for example maximum depth and min
samples leaf are not the only ways that
we can limit complexity another similar
value to the min samples leaf is min
sample split which is how many samples a
node needs to have in order to make a
further split we can also set the
maximum number of leaf nodes that are
allowed or we can require a node to meet
a certain threshold in order to split
for instance normally we add a split any
time the genie impurity or entropy is
improved even just a little bit
setting min impurity decrease basically
tells the model only add a split if it
decreases the genie impurity or entropy
by a certain amount
the idea behind all of these is very
similar they reduce the complexity of
the tree just in slightly different but
related ways
so when you're fitting a tree and you
notice that there's overfitting you can
use any of these metrics that fit to
your particular situation in order to
decrease the complexity of the tree
i've tried to look for some good rules
of thumb but it kind of seems like they
don't exist so go ahead and try out
different metrics and see what works
best for your situation i would say that
max depth and min samples leaf tend to
be the easiest to understand but try
whatever works for your particular data
in your particular situation
next let's talk about how to grab future
importance in the theoretical lecture i
talked about the fact that future
importance tells us what variables have
the most influence on the prediction or
outcome that the decision tree model is
making
one cool thing that we can do with
future importances is kind of treat them
like coefficients and give us an idea of
how much impact variables have on the
outcome
another thing that i've seen done with
them before is use them to do variable
selection if you have say a thousand
predictors and you want to choose the
top 10
one thing we could do is build a
decision tree and then select the
variables that have the top 10 greatest
feature importances
to grab the feature importance we say
the name of the model uh a feature
underscore
important says and then we can go ahead
and run this gives us the future
importances for the three variables in
our model whether or not a purchase was
made in someone's hometown
the amount of the purchase and whether
or not they asked for cash back we can
see here that the amount of the purchase
by far has the biggest influence on our
decision tree with the highest feature
importance of 0.873
if for some reason computationally we
wanted a decision stump where there was
only one split we might choose to split
on the amount because it had the biggest
influence in our model
all right let's look at another example
i'm going to load in this heart data
which tells us whether or not in the
target variable someone had heart
disease
and we are going to use the predictor's
sex age
resting ecg and cholesterol in order to
predict whether or not they have that
heart event
first i'm going to grab my x and y
variables i'm going to say x equals
heart
square bracket predictors
and i'm going to say y equals heart
square bracket
target
so let's go ahead and do k fold cross
validation so to do that i'm going to
say kf equals k fold
and
splits equals let's say
four because this is actually kind of a
small data set
so let's go ahead and do that and then
i'm going to grab uh let's see the
accuracy we'll just do for simplicity
just the accuracy of each model for the
train and the test set so i need to
create empty lists x or x excuse me
train equals empty list ack test equals
empty list that way for each of our four
models that we're going to build we can
store that accuracy for the train and
the test the other thing that i want to
know about my trees is going to be the
depth that they have so i'm going to say
depth equals and then we'll record the
depth for each tree as well so let's
start our k-fold for loop we'll say
for train comma test
in kf
dot split
and now we have to set our training and
our test sets
let's say x train equals x dot i look
x test equals x dot i
dot i look
test and then
y train equals y
square bracket train
and oops there we go
my test equals that's unindented
y test equals y square bracket test
now that we know what's in our training
and our test set we can do things like
z-score so i'm going to go ahead and say
z equals standard scalar
and then i'm going to do the fit and the
transform step in one step for the
training set so i'm going to say x train
and then let's see which of our
predictors are categorical
okay looks like sex is recorded as
categorical and rest ecg is
categorical so age and cholesterol we
will z-score so we say age and
cholesterol
uh
equals
z dot fit
transform
x-train
age
so this is saying both do the fit step
and the transform step for the training
set and now that the model will be fit
or excuse me the z-score object will be
fit we'll do the same for the test set
but we will not refit we will only
transform so i'm going to say
x test age cholesterol equal z
dot trans
form not fit transform
x test
oh my gosh can't spell age there we go
and cholesterol
so now that all of our variables are z
scored we can go ahead and build and fit
our model
so let's go ahead and create a model
called tree and we'll say decision
there we go and because we know decision
trees are very prone to overfitting one
thing that we could do is just build our
model see if it's overfitting and adjust
our maximum depth or mid samples leaf
there for now i'm going to actually set
both i'm going to say min samples leaf
20
and then max
depth equals two
so hopefully those together will somehow
limit the complexity of our decision
so let's go ahead and fit our tree now
that we have our empty model say
tree.fit x
and now we're going to need to grab our
metric so we'll say ack train dot append
accuracy score
actual comma predicted so y train
and then tree dot predict x train and
then let's copy and paste and do this
for the test set
so we'll say ack underscore test
accuracy score y
test and predict with x
then let's grab our depth for each of
the trees so we'll say depth dot append
tree dot get
and finally let's go ahead and print out
confusion matrices since we're not
grabbing the other metrics for each of
our models
to do that we can say plot
matrix
x train y train that'll print the
training confusion matrix and we can
change the title of these since it's
going to print out eight of them
sometimes changing the title can help to
keep track so i'm going to say plt
dot
title
and set that equal to
and then let's copy and paste and do the
same thing for the test set
so we'll say plot confusion matrix
y
and change this to test all right once
our for loop is done running we're going
to want to print out all of our metrics
so let's print
ack
train and then
spell that correctly and then say print
mp.mean act train
do the same thing for the test
do
test back test and then finally print
out the depth so we're going to say
print depth and that'll print out the
depth of all of our trees
all right that's a lot of code let's go
ahead and run it
beautiful so we can see all of our
output let's scroll so we can see this
okay first we have the training accuracy
and then we have the testing accuracy
now our training accuracy looks a lot
higher than our testing accuracy the
depth is always two which makes sense as
we limited the max step but let's go
ahead and look at our confusion matrices
okay this is the training set confusion
matrix for our first model
hmm
and this is the testing set now remember
when we've looked at classification
models before with confusion matrices
i've told you to look for weird patterns
take a second and see if you can realize
what the weird pattern is that i'm
seeing here in the test set confusion
okay i felt a little bit like dora the
explorer trying to wait for you to give
an answer but the answer is we can see
that the top row of the confusion matrix
is all zeros and as we scroll through
the different confusion matrices we see
similar patterns in all of the test set
until we get to this middle one that
looks pretty good actually but then we
get down to the very bottom and we see
another row of all zeros meaning that
for this test set there were no data
points with the label one
now this can happen sometimes because of
the way that a data set is laid out for
instance sometimes all the positive
cases are at the top of a csv file and
all the negatives are at the bottom or
vice versa in order to just make sure
that this never happens one thing we can
do is in the k-fold function remember we
can set
shuffle
equals true this will just shuffle the
rows of a data frame before it splits it
to prevent things like this however this
is why it's really important to look at
confusion matrices and look for weird
patterns in them
if i simply looked at the training and
the testing accuracy i might just think
that this is a severe case of
overfitting not that there's anything
wrong with the way i set up my model
so let's go ahead and run with shuffle
on now and we can see that our training
and our testing confusion matrices look
a lot more like we expect them to and we
actually see a lot
more typical uh accuracy scores last but
not least let's look at regression trees
remember in the theoretical lecture we
talked a little bit about how we could
basically use the same decision tree
structure to predict a continuous value
rather than a categorical one the main
difference is one the outcome is
continuous instead of categorical
two instead of things like genie
impurity and entropy we can use more
typical metrics that we are used to with
linear regression like the mean squared
error the mean absolute error etc
as a way to measure how well our splits
and our leaf nodes are doing last but
not least for every one of our leaf
nodes in a decision tree we choose the
mode or the most common category for a
continuous outcome we use the average so
if there's 10 samples in the leaf node
we take the average of all those samples
and use that as the prediction for any
data point that makes it down to that
leaf node luckily building a regression
tree is pretty much the exact same as
building a decision tree so i went ahead
and loaded this wine data set which has
a bunch of different characteristics of
wines we are going to use the
predictor's density fixed acidity
volatile acidity and residual sugar in
order to predict the amount of alcohol
that a wine has so let's go ahead and
set up our x and y variables
x equals one square bracket predictors
y equals one square bracket l
co
home
next we need to do our model validation
i'll do train test split so we'll say x
test y train y
test equals train
split
x y
size equals 0.2
now as always let's go through the
process of z-scoring and then creating
and fitting our model so we'll say z
equals standard
scalar
and we'll say x
train and all of our predictors are
continuous so we'll just say x train
predictors
equals z dot fit
trans
form x train square bracket predictors
and then repeat for the test set but we
will not refit
we will just transform
make this x test x test
okay so now we have our z scores we can
create our model so let's do that and
we'll say tree equals decision
regressor
so instead of classifier we're going to
use regressor
and then we're going to say
uh tree.fit
x train y train
and then for simplicity i'm going to
copy and paste these in here we're going
to get the mean squared error of our
trees now remember because we are
predicting a continuous value we are
going to use the same continuous metrics
that we use back in linear regression
for example mean squared error
so let's go ahead and run this
cool and we can see that our tree is
extensively over fit the training error
is approximately zero and the testing
error is about 0.5 so there's a huge
discrepancy between the errors of the
models so just like we limited the
complexity of our decision trees we can
also do that with the
decision trees for regression so to do
that let's say actually here let's not
reinvent the wheel i'm going to copy and
paste this here
we can use the same metrics that we did
for a decision tree to limit the
complexity of the tree so for instance i
can say min
samples leaf equals 10
max depth equals 3
and then when we fit it and then copy
and paste these metrics here
we can see
that that should improve the overfitting
a little bit and you can see that it
does before we had a huge discrepancy in
the training and testing performance now
that gap is narrowing a little bit with
in fact the training set doing slightly
worse so limiting the complexity of the
tree has reduced that overfitting you
can see that using a decision tree
regressor is basically the same syntax
as the decision tree classifier the only
difference is going to be one your
outcome that you're predicting is going
to be continuous and two the types of
metrics that you pull for the model are
going to be different than for a
classification model for instance here
we use the mean squared error rather
than something like accuracy recall
precision etc all right that's all i
have for you i will see you next time

 
hello and welcome to your k-means
lecture
so k-means is our first
unsupervised machine learning algorithm
well so what is unsupervised machine
learning
so far until now everything we've done
has been supervised machine learning
supervised machine learning occurs when
we train our models and have the correct
answer to compare it to
for example in all of our prediction
models we knew what category or value a
data point had and we trained our models
using that knowledge and comparing
whether our model got the answer correct
because we had that correct answer it
was supervised
well now we're looking at algorithms
where there is no correct answer
unsupervised machine learning in other
words is any time we are doing some type
of machine learning and there is no
correct label or answer roughly this
will fall into two categories
one is going to be dimensionality
reduction where we're taking a large
amount of data and compressing it into a
small amount of data
and the other is clustering today and
for the next few lectures we'll be
talking about clustering k-means is a
very simple clustering algorithm
clustering algorithms take a bunch of
data points and put them into groups
that didn't exist before for instance
you could group customers based on their
purchases or people at restaurants based
on their orders clustering aims to put
people in groups that are similar within
group and not that similar to other
groups clustering is useful because
often we need groups for things for
instance for sending out recommendation
emails for companies or for targeting
how different therapy works for
different similar groups because the
groups didn't exist before we made them
there is no right answer to compare
whatever groups we make too and thus
it's unsupervised machine learning now
let's talk about the k-means algorithm
specifically i'm going to lay out the
steps for the k-means algorithm but
don't worry i'm going to show you
visually on the next few slides the
k-means algorithm is very simple the
first thing we do is we choose k random
points to be the center of our k
different clusters notice that we have
to choose a number of clusters before we
start once we have these k different
points spread across you can imagine
spread across our graph we then assign
every data point in our data set to
whichever cluster center it is closest
to once we do that we really evaluate
where the cluster center is because now
we have a bunch of different members in
that cluster and finally we repeat that
process of assigning data points to
their closest cluster reassigning the
center of the cluster assigning data
points to their closest cluster
reassigning the center over and over and
over until our model has converged and
we'll talk about what that means in a
second
let's look at some visuals so if we go
back we had three steps right the first
of those is just to choose k random
points to be the center of our clusters
at least originally k-means is an
iterative algorithm meaning that we
start with something and we iteratively
adjust it step by step so step one is to
just start that iteration we can
randomly choose where our different
cluster centers are going to be now i
just want to quickly take a little
tangent to tell you that in real life
when k-means algorithms are implemented
we don't actually randomly choose which
data points are going to be the center
we use some smart math to choose what
centers would be the best ones to have
for instance we might want to equally
and evenly cover the entire possible
data space however the algorithm still
works just a little bit slower even if
we assume that we randomly choose our
center so i'm going to go ahead and go
with that for now all right so step
number one we have chosen k in this case
three random points to be the center of
our clusters moving on to step two
we then look at every single data point
and assign it to the cluster that it's
closest to
you can see here that now data points
are colored by the cluster that they're
closest to you can see the ones down
below are green the ones at the top
left are red and of course the ones at
the top right tend to be blue
all we did is measure the distance
between each of our data points and each
center
and then choose whichever center it was
closest to for instance let's just take
this data point this data point has this
distance to the red center
this distance to the blue center
and this distance to the green center so
you can see why it was assigned red as
its cluster now that we have every data
point inside of a cluster we can now
recalculate where the center of the
cluster is for instance see that this
red cluster well this is not exactly in
the center of that cluster anymore
so we're going to go ahead and move our
centers to be in the center of the
cluster the way we do that is we take
the average of all the data points for
each different predictor value so you
can see for instance that this red
center is now approximately in the
middle of the red cluster for average
number of pizzas that someone orders per
week and for the average number of
toppings that they have same with the
blue and the green clusters now that
we've done step three and reassigned our
centers we can go ahead and do step two
again to do that we now reassign data
points to whatever center is closest now
remember the centers have probably moved
and so data points might be closer to
one center compared to the other so we
reassign
and then we re-center
and then we reassign
and then we re-center and we repeat
these steps over and over
so when do we stop well the answer is
convergence and how do we define
convergence going back to this earlier
slide we can see that convergence can be
measured in two ways roughly
generally convergence is the idea that
we have a stable solution of clusters
and we can realize that in two different
ways one is that cluster membership
doesn't change in other words as we
repeat steps two or three if we get to
the point where the data points aren't
switching or flipping clusters that
could be considered convergence the
second way is to look at whether or not
the center of the cluster is moving at
all in other words as we go from step
two to three two to three when we move
our cluster centers they actually aren't
moving or aren't moving very much
both of these things represent a similar
idea that the clusters are not changing
we've reached a stable solution
so to review for k-means all we do is we
choose k
random points to be the center of our
clusters at least to begin with and then
we iterate through steps two and three
over and over first we assign each data
point to the closest cluster center
then we move the center of the clusters
to reflect the new data membership
then we assign data points to the
closest cluster move the centers assign
move assign move assign move until we
hit convergence now k-means isn't
guaranteed to converge but it often does
now that we know how to perform k-means
let's talk a little bit about the
assumptions it makes about our data
k-means is a very simple very
computationally efficient algorithm
that's why it's the first one that we
learn however it makes some assumptions
about the data
the first important assumption is that
there are spherical clusters another
name for this assumption is that it
assumes spherical variance meaning that
within a cluster the data points are
about equally spread out in every single
axis direction now this doesn't mean
that the clusters have to be perfectly
spherical more that they could be
encompassed by a sphere next there's an
assumption that there's roughly the same
number of data points in every cluster
now this doesn't have to be perfect but
if you have a cluster that has like two
data points and another one with 2 000
k-means might struggle to get that
solution making these assumptions is the
trade-off we get for such computational
efficiency and simplicity in the k-means
algorithm and in the rest of this class
we'll learn about more clustering
algorithms that make different maybe
looser assumptions all right so now we
know how to do k-means and we know the
assumptions that it makes
but i mentioned earlier when we were
talking about unsupervised machine
learning that there's no correct answer
when we create clusters we have no way
of measuring how far it is from the
truth because there is no truth
so how do we measure the performance of
our clusters well one way is by looking
at two things the cohesion and the
separation of our clusters
let's talk about those concepts
separately
cohesion in clusters means that data
points within a cluster are similar to
each other a cohesive cluster will be
very tightly packed because all of the
data points are very similar to all the
other members in its cluster
non-cohesive clusters will be very
spread out and data points will be very
different from other members of its
cluster when clusters have high
separation clusters tend to be very far
apart in other words data points in one
cluster are very far away from data
points in another cluster when data has
high separation the clusters will be
very far away when it has low separation
clusters will be very close together if
not completely overlapping one way that
we can measure both cohesion and
separation together is with something
called a silhouette score a silhouette
score is nice because it measures both
cohesion and separation of the clusters
at the same time you can see the formula
for a silhouette score on the bottom
left-hand part of your screen
you can see that cohesion is represented
by a i
and separation
by bi cohesion is measured as the mean
distance between data points and all the
other points within
its cluster
on the other hand cohesion is measured
by the mean distance between a data
point and all the data points in the
closest cluster
when clusters are good they're both
cohesive and separate meaning that bi
will be large
and ai will be
small when you take a large number and
subtract a small number you remain a
large number that means that better
clusters have higher silhouette scores
we then divide the numerator by the
maximum of a i or bi whichever one is
bigger in order to scale our score
between negative one and positive one
the best possible silhouette score you
could get would be one although you
would almost never see this in real life
and the worst possible score would be
negative one
let's think a little bit about what a
negative silhouette score means let's
imagine a data point that is actually
closer to another cluster than its own
cluster maybe something like this
you can see that while this data point
is part of the red cluster on average
it's actually closer to all of the data
points in the next closest cluster the
blue cluster
in this case bi would be a lot smaller
than ai leading to a negative silhouette
score if a data point were about
equidistant from all its cluster members
compared to the closest cluster our
silhouette score would be around zero a
silhouette score is calculated
separately for every data point in our
data set and then average together to
get an overall silhouette score for our
clustering membership as a whole so
while individual points can have
negative silhouette scores it is a
little bit rare to see an overall
negative silhouette score to review a
silhouette score measures both cohesion
and separation when it's higher it means
that the clusters are more cohesive and
more separate now later on in the course
we'll learn about other metrics that you
can use to assess the performance of
models but silhouette squares actually
handle a lot of the cases that we're
going to come into contact with in this
course to finish this lecture out i just
wanted to give you a couple examples of
cool things that people have done with
k-means clustering the first one is
something called image segmentation in
this case scientists are trying to use
k-means to cluster the pixels of a
photograph in order to figure out which
parts of a leaf are just healthy leaf
and which parts of a leaf are actually
diseased you can see from the pictures
that when they have the clusters they've
actually been able to separate out parts
of the leaf versus parts of the disease
leaf next a really cool example that
i've linked here actually combines
unsupervised and supervised machine
learning in this example they took movie
reviews and clustered people into groups
based on how they felt about movies
then they used those clusters along with
a k n method to classify new users as
most similar to certain groups this is
exactly the type of algorithm and
process that's used for recommendation
systems like the ones you see on amazon
or netflix first you create groups of
similar people and then when new users
come in you figure out which group it's
closest to alright that's all i have for
you for k-means i will see you next time

 
okay welcome to your Gaussian mixture
model or expectation maximization with
Gaussian mixtures lecture in Python so
the first thing I'm gonna do run all my
packages notice that I am importing from
SK learn mixture the Gaussian mixture
function that's what we're gonna be
using today so let's look at the Burger
King data again so this has a bunch of
different Burger King menu items and a
lot of the nutritional facts for them so
let's go ahead and we're gonna do a
little bit of a simpler example first
just so you can actually like visualize
what's happening so let's look at the
features and calories and sodium
calories and sodium mg ok so we're going
to look at these two features so we're
gonna say x equals BK bracket features
and then just because we want to put
things on the same scale we're going to
z-score them so I'm going to say Z
equals standard scalar and then X
features equals Z dot fit transform X
okay so now that we've z-score
everything we can create our Gaussian
mixture model and so I'm gonna put that
in a variable called en and I'm gonna
say M equals Gaussian Gaussian helps if
you spell it right mixture and similar
to k-means it's gonna take an argument
that tells us how many clusters we want
and this is called n components there we
go and I'm just gonna say we want 3
right now why not so in the same way
that we fit the k-means algorithm we can
fit the gaussian mixture algorithm by
saying y m dot
fit and then giving it the data and so
now we have our fitted Gaussian mixture
model and week
get the most likely cluster for each
data point by using model dot fit their
predict excuse me X so this will and
actually let's go ahead and run this
really quickly so you can see what's out
putting here when you run this it will
give you a hard assignment right so we
talked about how you don't have to use
hard assignment you can you can choose
the cluster that's most likely but we do
have probabilistic assignment and the
way that you would grab that is by
saying we'll call it cluster P equals e
m dot predict predict there we go Prabha
X and let's print that out cluster P so
you can see that when you do that it'll
actually give you the probability that
the data point is in each of the
clusters so if you do want to grab that
soft assignment it's totally possible
since that's what the model outputs
anyway okay I'm gonna comment that out
but that is how you do that so now that
we have the membership we can calculate
for instance the silhouette score which
is something we also did for k-means so
here let me print it out print some
weights and print so what score X and
then we have to give it the cluster
membership so we can run this and I'll
say okay the silhouette score is zero
point three seven seven not bad not bad
at all okay and then we can also if we
add to this X so X cluster equals
cluster the reason I'm doing this is
because ggplot needs a data frame so I'm
just adding the cost of membership to
the data frame so I can plot this I can
say X yes x equals calories y equals
sodium in milligrams I think and then
color equals what did we call it cluster
so I can do a scatter plot and I can see
what my
Gian point that's what happens when you
talk in type at the same time so I can
see what my clusters actually look like
okay didn't like that what did I
misspell this time misspelled the word
cluster that's just great plus turn okay
so when we run this you can see what the
different clusters look like and one
thing you might notice is that well kind
of clustered it the way you would expect
there's the high sodium high calories
low sodium low calories the middle
cluster whatever one thing that's
interesting you can see is the kind of
shape of these clusters when we did
k-means we kind of had more well
obviously more spherical clusters
because that's what K mean as assumes
but you can see that we can get a little
bit more of a nuanced shape when we use
Gaussian mixture models and that's one
of the main reasons why we would use a
Gaussian mixture model instead of
k-means is because we don't think that
the clusters are spherical or because we
like that soft assignment and we want to
use probabilistic or soft assignment
instead of hard assignment by the way
k-means is a lot simpler computational
expense wise and so a lot of times
k-means does just fine until we just use
that because it's a lot simpler the
Gaussian mixture models does take a
little bit more computational time so
sometimes people will use k-means just
because of that honestly it's going to
depend a lot on your situation as to
when you choose to use one over the
other but one of the main deciding
factors is probably do you think that
the assumption as spherical clusters is
appropriate or not
okay so that was a simple example that
made it really easy for us to graph what
was going on there let's do one more a
little bit more I don't want to say
difficult but a complicated example
where we have three features so let's
say features equals and instead of
calories and how do we look at calories
and sodium let's look at sugar grams
protein in your hands
and fat engrams those are the main
components of food so hopefully these
will help us distinguish the different
burger king items from each other okay
so we're gonna do the same thing we did
before and we're gonna say x equals b k
bracket features then we're gonna
z-score on em or say Z equals standard
scalar X features equals seat fit
transform X Wow look at that what a
treat
okay so we can run that that's just
going to z-score all of our variables
now we can build our model we can say M
equals and go in mixture gosh the longer
these are the more I mess up on this
body n components equals three and then
we can fit our models so we'll say iam
da fit X and then print that out so then
we can grab the cluster assignments from
this model so we can say cluster equals
m dot predict X and let's just print
them out so you can see what they look
like
cool so these are the cluster
assignments we have 3 clusters 0 1 &amp; 2
this is what they look like
how exciting um so we can print out the
silhouette score silhouette score X and
cluster see what is that okay 0.41 - not
bad not bad at all if you want again we
can say m dot predict
prabha x this will give us the
probability that the data point is in
each cluster we don't really need that
here but that's how you would grab that
um so let's plot things a little bit so
I'm gonna say X cluster equals cluster
and this is just adding it to the data
frame so we can use it in GG plot um so
when we have more than two features it's
really hard to visualize but we can
visualize them at least one at a time so
we could say ax
and we'll say yes X equals let's say
sugar grams and then y equals fat we'll
do fat first fat grams and then again
let's color not a period color by
cluster and then add geom point okay so
let's do this for every combination of
two variables that we have so we'll do
sugar and fat we'll do sugar and protein
we'll do protein and fats okay so I'm
gonna say so run all below okay
so you can see that some distinct
clusters are emerging from this and it's
a little bit easier to see with these
fat protein sugar graphs and you can see
some interesting patterns that are
emerging here so for instance whatever
this purple category is the category
zero you can see that it has low to
medium fat it has very low sugar it has
very mid to low protein and we I mean we
already saw this but it has kind of like
mid to low protein and fat so these seem
like things that are kind of like your
average item it's maybe like a little
healthier sugar why so maybe these are
like their burgers or something like
that which will tend to not have as much
sugar as things like their milkshakes or
whatever else burger king happens to
sell you can see that these yellow items
are like super high protein they're
super high fat but they tend to be
pretty low sugar so maybe those are like
oh god what does Burger King serve like
bacon Nader's that Wendy's I don't know
I mean these are the things that have
like seven hamburger patties and bacon
and cheese and stuff like that because
those also have pretty low sugar but do
you have a lot of protein in fat and
then we see this blue cluster which is
probably its low protein mid to low fat
but a lot of sugar so these are probably
your dessert
it's like your apple pie or your what I
don't know honestly I need to go to
Burger King more I really don't know
thanks sir but these are like really
high sugar items so we tend to think of
those probably as more like sweet things
or bread things so you can see that we
have some pretty useful clusters here
and if we wanted to do more than three
clusters we totally can and all we would
have to do is in our code change this so
let's go ahead and run that okay so you
can see we now have four clusters so we
now basically split that cluster that
was kind of the probably regular burgers
and stuff like that into two clusters
and these two clusters have like a
really lower fat one and like a kind of
higher fat one yeah same thing so these
green ones are probably just like lower
protein and lower fat than the blue
cluster so maybe these are like the
slightly healthier sandwiches like the
ones that are like chicken instead of
ground beef or something like that I
don't know honestly I don't really
understand Burger King but you can see
how these clusters are really
interesting and the fact that we have a
lot of overlap and our clusters is a
reason why we're really grateful that we
use Gaussian mixture models because we
can now say okay well like let's just
like this particular data point it's
most likely to be in the green cluster
but it probably has a relatively high
probability of being in the blue cluster
and with Gaussian mixture models we
could actually see what that probability
is if we wanted to pull it out using the
predict prava function okay that's how
you do Gaussian mixture models in Python
hopefully that made sense and I will see
you next time

 
hello and welcome to your principal
component analysis or PCA lecture
principal component analysis is a method
of dimensionality reduction and is
technically unsupervised machine
learning while clustering is the main
type of unsupervised machine learning
that we see dimensionality reduction is
actually another large part of
unsupervised learning dimensionality
reduction refers to taking a large set
of predictors and condensing it into a
smaller set and the reason this is
useful is because when you have a model
that you need to run really quickly and
efficiently like say a prediction
algorithm that makes recommendations on
an Amazon website every time you load a
new page when you need that to be fast
having a model that works with fewer
predictors and therefore is less complex
and a little bit faster really speeds up
your ability to build the model and to
make predictions so what is PCA well
first let's take a look at this scatter
plot here we have some data that
represents students and their gpas on
the x-axis and their SAT scores on the
y-axis
now basically you have two axes of
variation students vary along the GPA
axis you know low to high GPA and on the
SAT axis but what if we could create a
new set of axes that more efficiently
explain the information in this data
notice that sat and GPA are related to
each other we can see this by that
upward trending line of points what if
we took these axes right that are
currently GPA and sat
and rotated them so that they're more
efficiently describing the information
in the graph now we have two new sets of
axes and you can think of the main dark
teal axes as a combination of sat and
GPA scores for example if we wanted to
come up with a name for it we might call
it something like General test taking
ability and notice that students vary a
lot along this axis by combining the
information in the SAT and the GPA
variables because they're related we're
able to create this new variable which
we're for now calling test taking
ability that measures a lot of the
original information in the data but
with a single variable the variable
represented by that long teal variable
we've also created a new variable
represented by the light blue axis
however we can see that students don't
really vary that much along this axis
they're much more spread out along the
long teal access so in a way we can say
that we're representing a lot of the
original information in SAT scores and
GPA with this new teal variable that we
have and this for example can be really
helpful if we wanted to say predict the
probability that a student would get
into college or would get into a certain
job and we only wanted to use a single
predictor because we wanted it to be
fast instead of using both sat and GPA
we could use this new teal variable
which is a combination of sat and GPA
instead of both of them and that's the
point of PCA PCA takes in a set of
variables and creates a new more
efficient set of variables for us and
how does it do this well the answer is
eigenvectors and eigenvalues if you
remember back from our all the math you
need to know lecture eigenvectors
basically tell you the main directions
of stretch or squish that are happening
in a matrix and the eigenvalues tell you
how how much stretch and how much squish
is happening in PCA we find the
eigenvectors of the covariance or the
correlation Matrix and it turns out that
when we do this these eigenvectors are
the new more efficient variables it does
this by taking our original axes the
original variables and rotating them to
create the new set of more efficient
variables these new principal components
are more efficient because they're
created in a way that the first
component explains the most information
from the original variables the second
the second most the third the thermos
and so on and so forth the eigenvalues
tell us how much information each of
these components is explaining for
example if one component explains 90 of
the original information that means that
ninety percent of the information from
our original set of variables is
represented by that single variable the
new component because we get these
components in order for most variation
to least we often take the first n
component say 3 5 or 10 and use those in
place of our original variables this
also means that the last few components
often don't explain that much
information at all which means we can
remove them without losing much
information often we can get 80 or 90
percent of the original information in
our data with only a handful of our
principal components meaning that we
went from a large number of variables
from our original variables to a very
small number when using principal
components before we move on on I just
want to reiterate something really
quickly that people often get wrong
because the new set of variables the
principal components are a combination
of every single one of our original
variables sort of like how you see on
this screen principal component analysis
allows us to do dimensionality reduction
without doing variable selection what I
mean by that is that when we do PCA
we're able to reduce the dimensionality
of our variables in other words we're
able to go from a large number of
variables to a smaller set of variables
however because the new set of variables
the principal components are each made
up of information from every single one
of our original variables it's not like
we could stop collecting some of the
original variables let's think of a
concrete example
say you're in marketing and you're
trying to predict whether or not someone
wants to use Smart search on your
website based on their age the average
amount of time they spend on your
website the average time between website
visits and the average number of visits
they have in a month we can put all four
of those variables into a PCA model and
see that one component explains 95 of
the original information
but because that component is going to
be a combination of their age and the
website and all of the other variables
if you want to use that component to
predict whether or not they want to use
Smart search you're still going to have
to collect all four of those variables
it just means that by the time you put
them into the model you'll only have a
single predictor in that model thus as I
said before PCA helps you do
dimensionality reduction without doing
variable selection it's not like we're
just choosing age and average time spent
on the site we're using a smaller number
of principal components each which have
a part of all four of our original
variables alright a moment ago we talked
about how PCA returns to us this new set
of variables in order from most
variation explained to least because of
this we often make plots that show us
how much variation each component
explains one example of this plot is
scree plot which you can see an example
of on the left hand side of your screen
a screen plot on the x-axis has the
number of each component from one being
the first component all the way to
however many components you have
on the y-axis is the proportion of
variance explained from 0 up to one here
you can see we'll always have a kind of
descending graph because remember we get
these components in order for most to
least so for example in this graph we
can see that the first component alone
explains about 40 percent maybe a little
bit more of the original information in
our data the second one explains just
under 20 and the third about ten percent
and so on and so forth and you can even
see that as we get to about component 10
and higher these components are adding
very little information so in reality we
could probably get rid of them and not
lose too much the other way we often
display this information is with a
cumulative variance plot cumulative
variance plot is basically like a screen
plot the only difference is that instead
of telling you the amount of information
each component explains it tells you the
cumulative amount of information that
every component from one up to this
component explains and this is because
as I mentioned before we often do things
like keep the first four components or
the first ten and so it can be useful to
know okay if I kept the first 10
components how much of the original
information would I have and that's
exactly what the cumulative variance
plot shows you
so for example here you can see that if
I retained 10 of the original 29
components I would be retaining about 95
of the original information basically
the cumulative variance plot helps us
answer the question if I kept the first
n components how much of the variation
would I retain so how do you decide how
many components to keep in some cases it
seems pretty obvious in the sense of
these components are not really doing
anything so we could probably get rid of
them and not lose too much information
but we're going to talk about two more
principled ways of choosing a number of
components
the first way is the elbow method the
elbow method looks at the screen plot
and looks for the point of inflection or
the elbow of the graph
this is basically the point at which we
start getting diminishing returns when
adding more components
on this graph I think the elbow is
probably around here so maybe about five
components this means that I would keep
the first five principal components and
discard the rest the second method of
choosing the number of components to
retain is the threshold method the
threshold method basically sets a
threshold for how much of the original
variation or variance that you want to
explain you keep the number of
components that gets you to or above
that threshold so for instance if we
look at the cumulative variance plot on
the right and we know that we want to
keep 95 percent of the original variance
well then we look and see which is the
first component to hit or cross that 95
threshold and in this case it's 10 which
means we would need to retain 10
components in order to retain 95 of the
original information both these methods
are totally valid and actually there's
other methods out there what you're
going to do probably depends a little
bit on your situation Alright Now for
Something Completely Different math
now I'm not going to make you calculate
eigenvectors and eigenvalues by hand we
have a computer to do that for us but I
do want you to look a little bit at the
math to try and get a better
understanding of PCA
so on the left hand side here I have two
correlation matrices that I have eigen
decomposed to get the eigenvalues and
the eigenvectors remember that the
eigenvectors tell us the directions of
stretch or Squish and in PCA this means
the new efficient variables that we're
creating the eigenvalues tell you how
much stretch and squish is happening
which in PCA is how much of the
variation is explained by the different
components here you can see we have a
correlation Matrix where the two
variables are not very related at all
they have a correlation of 0.1 which is
quite small notice that when this
happens we have the two eigenvectors
that are very similar this means that
the two components that we have have
relatively the same amount of
information in them on the other hand
when we have a correlation Matrix where
the variables are very very correlated
at 0.99 correlation we get eigenvalues
where the first one is almost 2 and the
second one is 0.01 this would indicate
to us that our first principal component
explains a huge amount of variation
compared to our second one and this of
course is in contrast to the first
example where they're nearly equal
all of this is to point out that
principal component analysis exploits
relationships between the variables when
variables are highly related PCA can
exploit that relatedness and create
components that efficiently explain that
shared information between our variables
this will result in components that
explain a lot of variation and then our
later components that explain very
little when variables are unrelated as
you can see here our components are not
as efficient because PCA exploits
relationships between data when there is
no relationship PCA is less able to give
us those efficient sets of variables
we'll explore this a little bit more in
the classwork but for now that's all I
want you to know about the eigenvalues
and the eigenvectors
on the right hand side you can pause to
check this out I just wanted to show you
some code to kind of prove to you that
eigen decomposing the correlation or
covariance Matrix is the same thing as
what you're getting when you do PCA on
the top here you can see where eigen
decomposing a correlation Matrix and
here you can see we're doing PCA and
with some rounding error you can see
that we pretty much get the same results
next on our math journey let's talk a
little bit more about eigenvectors
once they're scaled eigenvectors are
also called component loadings and they
tell us how much of each of our original
variables to put in each component
basically they're like weights or
coefficients that tell us you know add a
little bit of this and a little bit of
that we can grab those components from
our model which we'll do more in the
python lecture for now what I want to
show you is how we use these to get from
our original set of variables to our new
more efficient set of variables which we
also call component scores so I'll
actually Mark that down for you so these
are our new variables these are old
variables and what we do is we take our
original variables so this is a person's
score on three variables a b and c and
to get their component score for
principal component one what we do is we
take these loadings that are come from
the eigenvectors and multiply them by
these scores
to do that you can see the math here we
basically just do some matrix
multiplication and when we get that you
can see that what output is this
person's score for principal component
one of course if we wanted the this
person's score for principal component 2
we would use these weights these scores
and it would give us this value so again
in order to get our new more efficient
set of variables we use matrix
multiplication to basically re-weight
our original variables using the
eigenvectors that the PCA creates I made
this graphic for you and feel free to
pause or to find it on our GitHub if you
want to look at it a little bit longer
but this is just kind of a review of the
math that's happening so in our example
we have some original data age weight
height and heart rate for a couple of
people
we've then done PCA and created
eigenvectors or loadings that allow us
to recombine these original variables
into our more efficient set of variables
the component scores in order to do this
we need to do some matrix multiplication
which I have two examples here for you
can see that for person one for their
score on principal component one we take
the first eigenvector
and multiply it by this person's scores
and that will give us their score for
principal component one so again we're
taking the raw data multiplying it by
our eigenvectors AKA loadings and we get
out our new more efficient set of
variables we call them principal
components or component scores feel free
to pause and Ponder this a little bit
longer but for the sake of time I'm
going to move on the loadings are
eigenvectors from our PCA also can help
tell us which variables are most
strongly associated with each principal
component for example you can see that
in principle component one our first new
variable age residence employ and
savings all the load vary strongly onto
this component well it's for principle
component two things like credit cards
debt and education load really strongly
these loadings can give us a little more
insight into what our principal
components are measuring because because
the principal components are a
combination of every single one of our
original variables it can be a little
hard to interpret what they're doing in
fact if you ever need to build a bottle
where you need to know the effect of one
variable on your outcome PCA is not
going to be a very effective step in
that model building process because
every single principle component has a
little bit of every single original
variable it's hard to isolate the effect
of a single variable on the outcome for
example if I use these principal
components as predictors in a model it'd
be really hard to pull apart what the
individual effect of income is because
every principal component has a little
bit of influence from income to review
why do we do PCA well PCA is a way of
rotating the axes of our data to create
a set of more efficient variables these
variables are often so efficient that we
can just keep a handful of them and
retain almost all of the original
information in the data
this is really helpful because when
we're building models like a predictive
model or even a clustering model
sometimes having a smaller number of
predictors can speed up that model a lot
that's almost all I have for you the
last thing I just want to quickly
mention is factor analysis factor
analysis is similar to PCA but has a
slightly different goal and if you are
ever wanting interpretable new variables
or we call them factors here you may
want to look into factor analysis
whereas PCA is a lot better just for
that straight dimensionality reduction I
won't talk too much about it here but if
that sounds something that's interesting
to you feel free to look it up there's a
lot of great tutorials out there
all right now that's all I have for you
I will see you next time

 
hello and welcome to your principal
component analysis lecture in python so
we've talked about principal component
analysis as a way to do dimensionality
reduction so let's go ahead and try it
on an actual data set
as usual first thing i'm going to do run
my import statements those are very
important and then i'm going to go ahead
and run
my data in and let's go ahead and take a
look
so this data set that we're working with
is a breast cancer data set so we have a
bunch of different features and let's
scroll down to see them
that are
characteristics of a biopsy to check
whether or not someone has breast cancer
so while not all of the columns are
printing out right here in the data
frame you can see that by these little
dots in the middle here we can print out
all of the names of the columns just so
we can see all of the features that we
actually have
so we're going to only grab the columns
that we want there's one unnamed column
at the end that we don't so i'm going to
get rid of that but what we're going to
do is we are going to perform principal
component analysis on all of these
features to find out if there's some way
to combine features so that we can only
use a couple principal components rather
than the entire data set while still
explaining as much information as
possible all right so the first thing
that we want to do before we actually do
any of the principal component analysis
is to z-score our data and remember it's
really important to z-score because
principal component analysis looks at
overall variance so if our variables are
on really different scales then one
variable might count more towards the
variance than another not because it's
actually varying more but because it has
a larger scale overall so we're going to
make sure that we're z scoring before
plugging things into that principal
component analysis
so i'm going to go ahead and say z
equals standard scalar
and then i'm going to fit and i'm going
to say bc
features so that's all the features that
we want
equals and then i'm going to say
z dot fit
transform that was too many underscores
form
bc bracket
features and remember if we had
something that was not a continuous
variable we wouldn't want to z-score
that but all of these are continuous
variables so i'm good with z-scoring all
of them
all right so we're z-scoring storing
back in our data frame now we're ready
for the next step all right so let's
explore our data a little bit so we're
going to use the ggplot function and
then give it the bc because that's what
we want to use
and so we're going to say aes and we're
going to look at two of our features and
just look at a scatter plot of them
because remember the whole idea of
principle component analysis is that it
takes advantage of shared information
between our variables to create these
components so we want to see a little
bit if there are relationships between
our different features so we could spend
some time exploring this for the sake of
the lecture i won't i'm going to put
radius
mean so it's the mean radius from the
measurements and then y equals texture
mean there we go
and let's throw in a gm
point
and a plus theme minimal just to be
beautiful so let's go ahead and run this
awesome so we can see these are just two
of the many variables that we have in
our model but you can see that there is
already some shared information there
there is kind of like a big blob of
points so they're not like perfectly
correlated or anything but there is some
kind of direction and shape to this that
suggests that these two things are at
least a little bit correlated and of
course this is only two of our variables
but that gives us a good idea of what
pca might grab onto when actually doing
the analysis because it's going to grab
on to that shared information in our
data all right let's not wait any longer
so let's make our model i'm going to
call it pca and i'm going to set it
equal to pca which is a function from
sklearn and that creates our empty model
so now we need to fit it so i'm going to
say pca.fit
bc
features and remember bc features now
has our z scored data in it we don't
want to run this on the raw data
so let's go ahead and run this all right
now that we've run our principal
component model we want to grab some
information from it and in this case one
of the things we're really interested in
is how much variance each of the
components explains so we're going to
make a data frame
pd.data frame
to store all of those explained
variances so the first thing that i'm
going to do is i'm going to have a
variable called explained
underscore there we go variance and this
we can grab directly from the sklearn
model so i'm going to say pca dot
explained variance there we go ratio and
underscore so this is going to grab for
each of our different components how
much variance that specific component
accounts for
so just for our sanity the next column
is going to be the number of the
component just so that we can keep track
of which component we're looking at so
i'm going to use a range 1 2 30 and this
second number is going to change
depending on how many components you
have in your model remember the number
of components is equal to the number of
variables that went into the model
and so i'm going to have 30 but in other
cases there may be a different number
so that's our second column and our
third column we're going to grab the
cumulative variance because remember we
learned that principal component
analysis orders the principal components
from most to least variability explained
and so we may want the cumulative
variance so we might want to say how
much variance does the first 10
components account for total and the
cumulative variance will allow us to do
that so we're going to take this and i'm
going to copy it because i don't want to
retype that
so we're going to take the explained
variance and then we're going to apply
the cumulative sum function which will
grab that cumulative variance for us
okay now that we have that let's print
it out so we can see it
awesome okay so you can see this is
quite long maybe i should have just
printed out the head but what you have
is for each principal component one till
the very end we have the explained
variant so how much this component
specifically explains and then on the
right hand side we have the cumulative
variance so for instance remember that
principal component analysis is really
helpful because we can grab only a
handful of the components but hopefully
retain most of the information and
that's where the cumulative variance
column comes in real handy because let's
say i want to retain
90 of the original variance well
the first component only has 43 i say
only but that's quite a bit
together the first and second component
have a 62 percent then 71 etc and so
what we can do with this cumulative
variance column is look and see what is
the first component where the cumulative
variance exceeds our threshold of 90
and you can use any percent that you
want in this case i chose 90 so i would
say okay out of the original 30
components that i have
i'm gonna keep seven of them because
seven components retains over 90
of the original variation in the data
with so many fewer variables while i'm
on the subject i just want to point out
a common misconception remember that
principal components are all linear
combinations of the original variables
that you put into the model so it is not
the case that a single principal
component is equal to a single original
variable so when we're choosing the
first seven principal components we're
choosing the principal components we are
not choosing the first seven variables
in the model just wanted to point out
that out because that can sometimes be a
little bit of a confusion that students
have all right so we've seen the
explained variance in table form but can
often be easier to make a plot so we're
going to pull out ggplot and we're going
to use pcadf
and what we're going to do is we're
going to make something called a scree
plot and the scree plot basically takes
the information that we saw in the table
and puts it in a plot so i'm going to
say x equals pc so which of the
principal components we're looking at
and y equals explained
variance there we go
oh i think it's actually just the same
fair so we are gonna make a scatter plot
so i'm gonna say plus
gm
line
and then just to make it a little
prettier plus geom point
and then of course plus theme
minimum
okay so when we plot this we are going
to get what is called a scree plot so
you can see the explained variance is on
the left hand side and which principal
component we're looking at is on the
x-axis so one way to choose the number
of principal components that you want to
keep is with something called the elbow
method
and the elbow method basically says like
pretend this graph is an elbow keep all
of the components up to and including
the elbow of the graph and discard the
rest of them so you're basically looking
for the point of inflection on this
graph so here it's not super cut and dry
but i would say maybe this would be the
elbow of the graph and so i would keep
all the way up to this component so i
would keep this one one two three four
five
of my original components and again this
is more of an art than a science so
there are other ways to choose how many
components that you want to keep the
scree plot is really useful when you're
using the elbow method but i actually
really like plotting the cumulative
variants as well okay let's go ahead and
do that so we're gonna gg plot again pca
df we're basically making the same graph
but instead of explained variance on the
y-axis we are going to have the
cumulative variance on the y-axis
so again let's copy and paste for the
sake of time
okay so let's go ahead and run this so
you can see that this basically mirrors
the other graph that we had but instead
of the explained variance we actually
have the cumulative variance i will say
one thing that i actually think is
really helpful in this case is to add
zero components so no components to this
graph because you can see here at the
bottom we're starting off at 40
something percent which is super high it
can be really helpful to see it go from
zero all the way up so i'm going to go
ahead and add that really quickly
okay so you can see all that i've done
is added a dot for no components so
basically this just shows you kind of
the steepness of the increase to the
first component because it explains a
ton of the original variance about 40
um so now that's just visible on the
graph
so the reason why i really like this
graph is because it makes it really easy
to do the thing we did with the table
before which is to choose a number of
components based on a threshold of
original variability that we want to
retain
so say for instance that i want to
retain 75 at least of the variance well
i can see here that 75 lines so i'm
going to keep one two three four
components because four is the first
component to exceed my threshold of 75
and one thing that we can do with ggplot
which is super helpful
is add a line at the point of variance
that we want to retain so for instance i
can say plus gm
uh h line
y int
intercept equals
0.95 so say i want to retain 95 of the
variance when i run this you can see
that it'll very helpfully give me a line
right at 95 and i can count how many
components until that threshold is
exceeded in this case it looks like
about 10. so we could retain 10 of the
components instead of all 30. all right
so so far this has been really cool we
saw a huge number of variables go into
our model but we've seen through our
scree plot and other graphs that we can
basically
explain most of the original variants
with only a handful of the original
variables and that is super useful not
just to know but actually to implement
when building our models because models
especially complicated ones will run
faster when you have fewer features so
if we can condense the information of 30
features into a smaller subset of
features our models are on average gonna
run a lot faster now with some types of
models that are really simple like
linear regression on the types of data
we've been using so far it's not going
to make a hugely noticeable difference
but when you start working with big data
or more complicated models this can make
a huge difference so let's go ahead and
grab our principal components remember
pca is actually transforming our data
for us and so we want to grab that
transformation so that we can have our
data in terms of the principal
components rather than in terms of the
original variables so i'm going to grab
the first four i'm going to call it p
comps four because we're grabbing the
first four
and i'm going to use our pca model and
the dot
transform function and we are going to
transform our original data so this is
basically going to take the computations
done by the principal component analysis
model and actually apply it to grab
those principal components for us so for
each data point instead of having the
raw variables this will grab the
principal components for each data point
now this grabs all of the principal
components and as we just discussed we
usually don't want all of them at least
when we're looking at dimensionality
reduction which is what we're doing here
so we're going to turn this into a data
frame and we are going to only grab p
com
there we go p comps4 we are only going
to grab the first four columns because
remember we don't want all of the
different components we only want the
first four so remember the first four
components explained a little more than
75 percent of the variance but we also
saw that the first 10
explained more than 95 of the original
variance so i'm also going to grab some
variables and store the first 10
principal components um so that we can
compare them
beautiful okay so now we have these
principal components instead of our raw
data we can actually train models on the
principal components rather than the
real data and of course this is a huge
dimensionality reduction because
originally we had about 29 variables and
now we're down to either four or 10. so
in the long run our models are going to
take a lot less time to run so let's go
ahead and build a model i'm going to
call this lr1 and i'm going to build a
logistic regression because what we're
going to do is we're going to use these
principal components to predict whether
or not a biopsy was cancerous malignant
or not
so i'm going to go ahead and fit this
model i'm going to say lr1.fit
b
c
features oh that doesn't need to be a
string uh features and then the outcome
that we're predicting is gonna be the
diagnosis column
there we go
um so this is going to be our model as a
baseline comparison this is using all of
the original variables and we're going
to go ahead and fit it there and then
compare it to our principal components
with only four components and 10. so
before we build those models let's go
ahead and just have a print statement so
we can tell how this model does okay so
again baseline model
next we're going to build a model with
our 10 principal components because this
logistic regression
this supposedly has 95 of the original
variance so
instead of fitting on all of the data
we're gonna fit on p comps 10
and then our outcome is still the same
because we're still trying to predict
diagnosis
beautiful and then again i'm going to
add a line to print out the performance
of this model okay and last but not
least
let's fit a model on only the first four
principal components we'll say logistic
regression
and then we're gonna say lr
three
and then instead of fitting on all of
the data or even on the p comps 10 we're
gonna fit on p comps four
and then again
diagnosis because our output is the same
in all of these models
okay last but not least let's add that
little line that says how well the
models do and then let's run this and
compare the output of the models between
all of the data 10 principal components
with 95 of their original variability
and four components with a little more
than 75
okay so you can see the results are in
all of our data did pretty well
98.7 percent accuracy and 10 principal
components which remember is like 19
fewer variables than all of the data
also did 98.7 accuracy so even though we
lost a tiny bit of information there we
didn't lose any accuracy in the output
of our final model
we did lose a little bit when we went
down to four principal components
instead of 98.7 we have 97.0
accuracy but honestly this is the beauty
of principal component analysis yes
we're sacrificing a little bit of
information and potentially a little bit
of accuracy in the output of our model
but the
reduction in the number of variables is
huge we can get down from 29 to four
variables and barely sacrifice just a
little bit of accuracy and so that's one
of the reasons why we love trying out
pca as a way to reduce the dimensions of
our data all right so one other thing
that we can do with principal components
which will make a little more sense in a
couple of lectures is plot them remember
principal components are just
combinations of our original variables
so they do still tell us something about
our data for instance here i'm going to
take the first four principal components
i'm just renaming the columns to make it
a little easier for you to understand
and then i'm going to pull out our
trusty gg plot and i'm going to say p
four there we go
and a yes
x equals p c zero
y
equals pc1
and then i am going to make a scatter
plot of the first two components
remember the first two components
contain a lot if not most of the
information in the original data at
least in our case so i'm going to say
plus geom point
plus theme
minimal
all right let's go ahead and run this
okay so this graph is of the first two
principal components and while it
doesn't contain all of the information
about the variability remember the first
two components contain a ton of the
variability and so this gives us an idea
a little bit of how spread apart and
variable our data points are and this
can be really helpful especially if
you're doing some clustering with high
dimensional data it's not always
practical to plot you know two by two
each combination of your variables so
plotting the first couple of principal
components can give you an idea of the
structure of the data without forcing
you to make about a million graphs all
right so there's one more piece of
information that we can grab from our
principal component analysis and that is
the loadings so remember that each
principal component is a combination a
linear combination of our original
variables and if you want to know what
exactly that combination is you can find
that out from the loadings so we're
gonna go ahead and run this and i'm only
printing out the loadings for the first
component because you would have them
for all of the components and that would
be very long um
but what you can see here is for each of
our original variables you can see their
name as i scroll
the loading tells you how much of that
original variable is included in this
component and so there are more
complicated ways to interpret and get
information from that but what i want to
point out to you is that when you look
at the loadings what you're looking for
is bigger numbers so either really big
positive numbers or really big negative
numbers because it tells you how much of
that variable is included in this
principal component so variables the
original variables with high loadings or
big in magnitude loadings
are very influential in that component
and variables original variables that
have very small and magnitude loadings
don't have that much of an impact on
that principle component
so you can kind of get an idea of which
variables are influencing which
components if you pull out and look at
all of the different factor loadings
okay so let's step by step go through
and build another pca model i've loaded
in some lizzo spotify data and what
we're going to do is we're going to take
these continuous variables from that
data and we're going to perform pca on
it so the first thing that we need to do
is make sure that these variables are on
relatively similar scales so we're going
to go ahead and z score so i'm going to
say z equals standard scalar
and then i'm going to say liz equals
z
there we go dot fit
transform
transform there you go uh liz
bracket features so this is gonna take
our original data uh only the columns
that we want to look at and z score them
store them back in the data frame all
right let's go ahead and run that and
then we can build our pca model so i'm
going to say pca2 equals pca
and now that we have our pca model we
can fit it so i'm going to say pca2.fit
liz this is our z score data remember
that we stored
okay and then let's go ahead and run
that and now we can grab that data frame
of the explained variance and the
cumulative variance so we can make our
scree plot all right i went ahead and
copy and pasted that in but you can
pause if you want to look at the code i
just want to point out right the number
of components is going to be different
depending on the number of variables
that you put into your model so remember
that number needs to change when making
this data frame but let's go ahead and
look at a table of these oh we forgot a
comma let's go ahead and run that okay
so we can see our explained variance our
principal components and our cumulative
variance now we can go ahead and use
that to make our screenplot all right so
let's gg plot pca
df2
and then we'll say aes
x is going to be our principal
components
y is going to be the explained variance
and then we will add of course our plus
geom line
and then just for aesthetics plus geom
and plus beam minimum
all right
so you can see our scree plot is a
little bit steep we can easily see where
the inflection point is it actually
looks like it's at the second component
so if we were deciding via the elbow
method i would probably keep two
components because this seems to be the
inflection point however i always like
looking at the cumulative variance graph
so let's go ahead and make that as well
all right so we're going to start off
very similarly ggplot pca df2
and then
as
x equals the principal components but
then now y is going to equal the
cumulative variance
and then we're going to add our geom
geom
and
theme
all right so let's go ahead and look at
this so this gives us a good idea of how
we are going to choose if we wanted to
go with that threshold method um so for
instance if i wanted to keep at least
eighty percent of the variance i'd have
to keep one two three four five at least
five components and remember you can as
we did before add the zeros to this
graph so that you can
better see the steepness of the increase
so i'm going to pause and do that and
come back when i'm done awesome okay so
you can just see from this graph all
we've done is add a point for no
components but it gives you an idea of
how steep the increase is from no
information to the first component and
this is really interesting so this pca
plot shows us that there is a little bit
less common information here because
each component actually contributes
quite a lot of variation you know maybe
until we get to the very final component
or two
and we can tell a lot from the shape of
these pca graphs here because we can see
if there's a lot of shared information
we expect really steep pca plots when
there's not a ton of shared information
which may be the case here although
there is some
you can see that the pca plots are a lot
less steep
alright that's all i have for you i will
see you guys next time

 
hello and welcome to your
expectation-maximization lecture I'm
really trying to have this be an
extension of the Keynes lecture that you
covered last time because k-means and
expectation maximization with mixtures
of gaussians which is just a mouthful
I'll use I'll usually call it Gaussian
mixture models or expectation
maximization is really related to
k-means and when I first learned this no
one really drew that parallel for me and
when I finally figured out all the math
of it I was like what the heck why did
no one tell me this so I'm trying to
present it in that way because I think
it might be helpful for you now
just a note expectation maximization is
an algorithm that can be used for way
more than what we're using it for right
now
right now we're gonna use it to cluster
things but just FYI expectation
maximization is an algorithm that is
used for so many more things than that
so before we dive into the actual
clustering part of it let's really
quickly review what a normal or also
known as a Gaussian distribution is you
may have heard of it as like a bell
curve and things like standardized tests
or height or weight or things like that
are typically modeled as being a normal
distribution I have the formula over
here it's not important that you
memorize it but this is the formula that
gives you this normal distribution curve
and what is important for you to know is
that the normal distribution is
symmetric unimodal and it has this
really cool property that most of the
data is in the center and as you get
further from the center the data becomes
less and less likely so you can see from
this shape here you know you have like
not that much data here a lot of it here
again goes down and not much here okay
so this is a normal a ka a Gaussian
distribution and these are going to be
important because the expectation
maximization we're doing
is with Gaussian distributions so let's
talk a little bit about what I mean by
that
so say you have some observed data that
has this pattern this in particular is
the number of dollars spent on fast-food
per week and say you see this pattern
and you're wondering huh what causes
this pattern well one explanation is
that it's a result of having two
separate groups that each have their own
distribution so for instance we have the
pink group and the blue group the pink
group might be the people who don't
spend that much on fast food and they
tend to get less when they do go get
fast food and then you have people in
the blue group who spend a lot more
money on fast food but tend to have a
larger variance and the amount of money
that they're spending so you can kind of
see from this plot how these two groups
when put together would make this I mean
you can see it on that one too but would
make this purple curve so the idea is
that when we observe data like this we
can often think hmm there's a bunch of
latent which just means unobserved
groups that make up this distribution
and I'd like to know what those groups
are so what we can do is use the
expectation maximization algorithm with
Gaussian or normal distributions
hopefully you can kind of see how if we
extended them out these distributions
are like normal distributions we can use
that to calculate okay what are the
normal distributions that make up this
pattern of data that we see so really
quickly I'm gonna talk a lot about the
similarities between k-means and
of gaussians um but let's really quickly
talk about the differences so when we do
k-means we assign each data point to a
particular cluster it's not a member of
any other cluster it is only a member of
the cluster it's been assigned to this
is called hard
assignment because the labels are like
very strict we don't change them we
don't think they're like wishy-washy
like they're hard labels in contrast
with mixtures of gaussians what we can
do is instead of saying this data point
is in this cluster we can instead assign
each data point a probability of
assigning to of being assigned to each
cluster so for instance let's look back
at this picture and that's a race these
extra lines they're very ugly okay so
say we have a data point that is right
here I don't want that color right here
so this person spends like 40 dollars
and 50 cents on fast food a week so we
could say okay well which cluster will
we assign this to well it looks like if
you draw a vertical line up it looks
like it's most likely in the pink
cluster because this number is more
typical of people in the pink cluster
than it is of people in the blue cluster
but it's not like there's no one in the
blue cluster that has this amount of
spending per week so we might want to
instead of saying it's in Group A that's
it instead of saying that we might want
to say well there's a really high
probability that it's in group in the
pink group but there's a small
probability that it might be in the blue
group too and this is called soft or
probabilistic assignment because instead
of saying it's in the pink group and
having no flexibility with that we
actually say well there's a really high
probability it's in the pink group but
there's a tiny probability that it's in
the blue group and this applies if we
have multiple groups as well okay so
that's hard assignment the other thing
that is different between k-means and
of gaussians is that in k-means we had
this assumption that all of our clusters
were spherical and we knew that
assumption wasn't usually true but it
worked well enough so we left it alone
however sometimes this can cause really
big problems like imagine if you had
data where the clusters were like this
um take a million years to draw I showed
them in the different college that's
okay but imagine your clusters are like
this they really have this kind of
oblong spherical not spherical
ellipsoid shape but if you did k-means
with them you would assume that these
clusters are spherical and you might
miss some of the pattern that exists
within these clusters so the way that
expectation-maximization with mixtures
of gaussians gets around this is by
instead of assuming that all of the
variables have the same variance they
explicitly estimate what that variance
is so instead of spherical clusters
where all the in all directions you have
the same variance it allows us to have
like elliptical clusters like something
I'm trying to get the blue like
something like this where you can see
that the variance in one direction is
pretty small and the variance in the
other direction is pretty big and that
gives us this elliptical shape so just
to review k-means has hard assignment
of gaussian soft assignment or
probabilistic assignment k-means assumes
that all your variables have equal
variance or really that your clusters
will all have the same variance in every
direction
eeehm with mixtures of gaussians will
instead explicitly model whether the
variance is the same in all directions
and will allow your clusters to not be
spherical instead they can have like
rounded ellipsoid shapes okay
oh yes I made this meme for you um just
the hall was really funny because like
k-means has hard assignments and a lot
of your chapman computer science
professors have hard assignments get it
okay yeah anyway so just a review
k-means had a couple steps that we did
and it's an iterative process or
repeating some of these steps but it had
this iterative process where we first
chose K random centers and then assigned
all of our data points to one of those
centers then once we had each of our
data points assigned to a Center cluster
we would recalculate what that center
should be based on all of the data
points in that cluster so basically
every data point in that cluster had a
little bit of a say in what the new
cluster Center was then once we moved
our center's we would recalculate what
point excuse me what points were in each
cluster and then we would recalculate
the center's and then reassign the
points and then recalculate centers etc
etc that are until things converged and
so I want to draw attention to this
process because the
expectation-maximization process is
actually really similar okay so that was
for k-means let's go over how a lot of
these exact same principles apply to
expectation maximization so the first
step is really similar we're going to
choose random starting points for each
of our clusters or distributions now
just FYI there are really cool ways of
like expertly choosing your starting
points so that your algorithm converges
more quickly I'm not going to go into
that here it's way more than you need to
know for this class but I find it
fascinating and if you do too I highly
recommend looking it up it's so
interesting to read how people
efficiently choose like random starting
points for expectation maximization but
for now you can just assume they're kind
of randomly chosen so this is really
similar to in k-means now for
expectation maximization we talked about
the fact that we do soft or a
probabilistic assignment instead of hard
assignment so instead of assigning each
data point to a cluster for each data
point we calculate the probability of
being
in each cluster so instead of getting a
hard assignment of you're in this
cluster you're in that cluster we're
instead gonna calculate the probability
for every data point there in every
cluster now using these and these will
be now probabilities we're gonna
recalculate the means and variances for
each of our distributions or each of our
clusters now since we don't have hard
assignment every single data point in
our set data set gets a little bit of a
say in what the mean and variance of the
distribution is but it might make sense
that if a data point has a really really
low probability of being in that cluster
then it's gonna get way less of a say in
what the mean and variance is compared
to a data point that is very likely to
be in that cluster the math of this is
not super important although I'll link
some resources that might help you dive
into that math a little bit more but
essentially what I want you to know is
that we are recalculating both the mean
and the variance for each cluster and
that the probability of a data point
being in a cluster means that like the
higher the probability the more
influence that data point has on the
mean and variance of that cluster okay
so these two steps should seem a little
different but very familiar to the
k-means stuff that we've done and just
FYI the way that we call this the
expectation maximization this step two
is called the e or expectation step and
this step three is called the M or
maximization step and that's where we
get the name expectation maximization
because we're gonna repeat these two
steps the east' step and the M step over
and over and over and like k-means we're
gonna stop once our clusters or our
distributions aren't changing very much
so hopefully you can see how that
k-means is very similar to this
expectation maximization maximization is
a little more complicated but it has
like a lot of the same structure okay
so I'm gonna show you an animated gif in
a second
this didn't save as an animated gif
because PDFs are lame
but I just want to show you the the way
that expectation-maximization works in
one dimension so we only have one
variable here or one feature and it's
along the x-axis and I want to show you
how it works in one dimension this works
obviously in most of multiple dimensions
we can use more than one feature to
cluster our data but it's a little bit
easier to tell what's going on in one
dimension so this right now is showing
you the first Yi step or expectation
step where these two green in the blue
are our random initial distributions
these are the what we think our clusters
look like but obviously these aren't
very great guesses and so what we're
gonna do is we are going to
probabilistically assign each of these
data points to the clusters now I've
colored the data points by the cluster
that they're most likely to be in but
remember we're doing soft assignment
right now so every data point has some
probability of being in the green
cluster and some probability of being in
the blue cluster okay let's look at the
gif it's gonna take me a second to pull
up so hang in there with me okay it
actually pulled up a lot faster than I
thought so let's wait for it to restart
so you can see at the very top you can
see the e or the M step that we're on
from one at the very beginning to the
very end when it converges and you can
see what's happening is that the
distributions are very slowly at first
moving and then you can see that some of
the data points are changing which
cluster they're most likely to be in and
this is supposed to reflect the fact
that we're changing right both the
probability of each data point being in
a cluster and then we're changing what
is the mean and variance of that cluster
based on all of the data points that are
most likely to be a part of that cluster
so this is sort of what the algorithm
looks like in practice of course this is
a very simple example it's only in one
dimension but you can see how it's
learning the shape of our two clusters
and you can see how the data points that
are
most likely to be in that cluster are
really influential in the shape of that
cluster and like what the mean of that
cluster is so no this is very slow at
the beginning but then you can see oh
it's speeding up okay and then those are
our final clusters and it restarts over
so I'm gonna try and post this gift
somewhere so that you can see it as well
as all of the individual images you can
like if you really wanted to walk
step-by-step through the process but I
hope it makes a little bit of sense in
the same way that k-means eventually
converged on different clusters but
eventually the expectation maximization
with mixtures of gaussians eventually
converges on to distributions that we
think make up the data that we're
observing so these would be our final
clusters and we can either look at just
the probability of each data point being
in each cluster or we can assign each
data point to the cluster that it has
the highest probability so at the very
end of the skip you can see like the
green points we would say are in the
green cluster and the blue points would
say in the light blue cluster and but
the nice thing about the soft assignment
is that we actually have the probability
for each data point that it's in each
cluster okay that was a lot of math and
theory hopefully that makes sense and I
will see you for our Python lecture on
Gaussian mixture models

 
hello and welcome to your k-means in
python lecture so as usual i'm going to
start off
by running
all of my import statements so the first
thing we often want to do when we have a
new data set that we want to use
clustering on is exploring it as i
mentioned in previous lectures i'm not
actually going to do this during the
lecture because it's something pretty
easy that you can do you know make some
scatter plots some histograms explore
the data
but what i am going to do with you is
now build a model
so i'm going to run this data which is
some burger king nutritional information
data and so what you can see from the
head of the data here is basically we
have a bunch of different nutrition
information
like
carbs sugar fiber sodium etc
about a bunch of different items from
burger king
so what i want to do is i want to
cluster these items so what i'm going to
do is i'm going to create the data that
i want to use so let's say i want to
just cluster just for simplicity
i'm going to say features equals i want
to only cluster on fat
um grams
and
sodium
in my megagram milligrams oh my gosh i
don't know my units um but i'm just
grabbing these from the column names so
just for simplicity because this is our
first time doing this in python i'm
going to only use two variables but
remember you can use as many variables
as you want i am only using two here
just so you can see what's happening so
that when we do more complicated
examples you already have an idea of
what's going on so in k-means like a lot
of our clustering algorithms that we're
going to learn
z-scoring or otherwise scaling your data
is going to be really important even if
two of our values like saturated fat
trans fat
carbs fiber are all measured in grams it
doesn't mean that they're on the same
scale right something probably has a lot
more total grams of fat than trans fat
so there's more variation in fat grams
than trans fat same thing with protein
there's probably a lot more variation
there and so it's still important to
z-score even when technically the units
like grams are the same here we actually
don't even have that though we have
grams and we have milligrams
so
there's no space there let's go ahead
and grab our data so i'm going to say
x equals bk our data frame bracket
features
features here we go
and then i'm going to z-score so we
don't have to train and test set here
because we're not doing supervised
machine learning anymore where there's a
right answer
so we're typically not going to have
model validation like a training set and
a testing set so we don't actually have
to worry about data leakage yet which is
a benefit so what i'm going to do is
create my z equal standard
scalar
and then i'm going to say x bracket
equal z dot fit
transform
x bracket features
now um technically i don't have to do
this there's no non-continuous variables
but i'm just being careful and note that
i'm doing fit and transform in a single
step here typically we would do dot fit
and then on a new line dot transform
but because we don't have a train and
test set and i'm not worried about data
leakage i'm going to do them in one step
so now that we have our data let's make
a scatter plot so i'm going to say
ggplot x
i'm going to say aes
um x equals fat
grams
y equals sodium
in
millimicrogram some type of grain um
plus geom point
let's just plus theme minimal
and then we'll just even say plus labs
title
equals fat versus sodium
for burger
king
okay so let's go ahead and run all this
again we're grabbing our data we're z
scoring it and then we're plotting it
and so we can see a very beautiful plot
of the fat versus the sodium in these
different burger king items so i
actually don't right away see any very
distinct clusters but let's say that i
want to make a two cluster model so i
want two groups of food items
so to build my model i'm gonna say km
whatever variable you want is fine
equals k
means this is just the sklearn function
to do k-means and then the argument that
you need to pay attention to is n
clusters and in this case i'm going to
choose 2 but we can choose whatever we
want
and then i'm going to say km.fit
and the data that i want to fit it on so
we have our data in x so i'm going to
say km.fit x
now once we fit our model we have some
things that we can pull from it so for
instance i'm going to say a membership
equals km.predict
x this is basically going to say which
group
each data point is in and we're going to
store that in the variable membership
and then let's go ahead and add that to
our data frame so we're going to say x
bracket
cluster
go equals membership
all right so let's just look at this
data frame really quickly beautiful okay
so we have the fat we have the sodium
and then we have which cluster that the
data points belong to
so now that we have that let's make a gg
plot we're going to say gg plot x aes
and on the x axis let's put what did we
put last time fat so we'll say fat in
milligrams
plus
the
minimal plus labs
tidal equals
fat versus
um sodium clusters
now this is basically the graph we made
before so let's add one thing to help
make it a little more readable
which is let's enter
we are going to color
by
so let's go ahead and run this
beautiful so now we can see the
different clusters that the k-means use
um by the way see how this is using a
continuous scale we can always use
factor here
um in order to tell it these are not
continuous values
beautiful okay so same graph just
different colors so we can see that we
have a cluster that is kind of a low
sodium low fat cluster and then a
cluster up here that is a high fat
high sodium pretty much cluster we have
a bunch of outliers that are like very
extreme in these groups but basically
those are the patterns that it falls
into
so for instance if i know anything about
bk which i really don't
i would say that these are probably
going to be like
apple slices healthier sandwiches maybe
a salad if they have them and these are
probably going to be things like ice
creams uh huge burgers you know even
like a really large chicken sandwich or
like a bacon cheeseburger or something
like that so now that we've built our
model and visualized it visualizing it
is one really important way to assess
how well the clusters are made
for instance here these clusters are you
know understandable right i just talked
about different food items that might be
in there but they're not super separated
from each other like you can see there's
a little bit of space here but not a ton
which means that these clusters at least
in some of their areas are pretty
similar to each other
now another way that we can assess it is
numerically with something called a
silhouette score
so i'm going to use silhouette
there we go i have to say it like that
so i spell it right silhouette score and
you're gonna give it uh the data
and then um the
membership
so this is basically saying here's the
data we used and here is the cluster
membership and when you run this you're
going to see a silhouette score which
will help you determine how well your
clusters are doing all right so just as
a review remember silhouette scores go
from negative one which is pretty bad to
one which is very good and we typically
want scores that are at least above zero
and definitely closer to one as we can
get remember silhouette scores are
measures of cohesion which is how
similar data points within a cluster are
to each other and separation which is
how distinct different clusters are from
other clusters so our graph has a
silhouette score of about
0.488 so that's not horrible it's
definitely better than zero and it's
certainly getting closer to 1. but you
can see it's not very close to 1 and we
can see that reflected in the graph as
well because these clusters are not
super separate and there's actually a
lot of spread within clusters so both
cohesion and separation aren't like
where they could be which is why we
don't have a silhouette score of exactly
one all right let's do another example
so we're still using the burger king
data but instead of just two variables
i'm going to use fat sodium calories
cholesterol sugar and protein so let's
go ahead and repeat the process that we
did earlier but with more features so
we're going to say x equals bk
bracket features
and then z equals standard
scalar scaler there we go um and then we
can say x bracket features
equals z dot fit transform
x bracket
and then we can build our model so we'll
do km equals k
mean
clusters equals
let's do five
and then we can fit our model km dot fit
x
and then membership equals km.predict
and let's add that to our data frame x
bracket cluster
equals membership mem
do i spell the right mum
or
 i think i spelled it wrong up here
all right um and then we can also
calculate a silhouette score so let's
say print sill what
score
features comma membership
okay so that was a lot we just did again
we are creating our data we are z
scoring all of it because it's important
that they're all on similar scales we
are creating our model we are fitting
our model and then we're grabbing the
cluster membership for all of our data
points so let's go ahead and run that
beautiful okay so we have our model with
all of these different variables and
actually not a very high silhouette
square with a silhouette score of
0.279 so there are probably uh sort of
distinct clusters but not a ton and
certainly maybe less so than we saw up
here um and we can look and see
let's just print out
let's see membership
so let's just look
beautiful okay so we can see that there
we have representation of all of our
five clusters from zero all the way up
to four and we can see what they are now
when we have a bunch of different
variables that we're clustering on it's
totally valid and a good way to separate
clusters of data if that's what we want
to do but it's a little bit harder to
visualize we would have to like plot
each individual like pair of features
together which is definitely something
we will do sometimes but i'm not going
to take the time to do here so you might
be asking especially in the case where i
can't just like easily plot my data like
we could above here
how could i choose k
well there's a couple of different ways
and honestly clustering is more of an
art than a science at this point but i'm
going to show you two ways that can help
you decide what k should be besides just
looking at the plot and saying like oh i
think there should be two clusters or
three clusters which by the way
perfectly valid way to choose the number
of clusters you have but if you don't
have that luxury here's some things you
can do
so basically what we're going to do is
we're going to look at two different
metrics we're going to look at the
silhouette score and we're going to look
at the sum of squared errors so we
already know what a silhouette score is
right low is bad high is good above zero
is kind of like our bare minimum that we
want but the sums of squared errors
might be a little bit less familiar
although we talked about it a little bit
with linear regression in clustering the
sums of squared errors is basically how
far apart on average are data points
from the center of its cluster so it's
basically kind of a measure of cohesion
how close together are different data
points within a cluster
so what we're going to do is we're going
to try a bunch of different k's here you
can see i've listed a bunch of values
from 2 to 20 and we're basically going
to fit a k-means model where k is equal
to 2 3 4 all the way up to 20 and then
we're going to look at both the
silhouette score and the sum of squared
errors so to do that we're going to use
a for loop so first let's do sse
equals empty and stills equals empty
these are lists that are going to store
the silhouette scores and the mean
squared or the sum of squared errors for
each of our models so let's say 4k
in ks
uh km equals
oops i go okay
means
uh n
clusters
km.fit
x which we have
from above
and then we will say
sse.append km dot
inner
inertia underscore
and then uh sills dot append
uh silk what
um x
uh km.predict
x so basically we're kind of doing
things that we've done before so what
we're doing is we're creating a model k
is going to be equal to whatever the k
currently is and then we're going to say
fit that model and then from the model
you can grab something called inertia
which is going to be the sums of squared
errors so this is going to be for the
clusters like what is the average
distance between a point and its
center
and they're also going to append to
sills the silhouette score for that
number of clusters
so once we're done with that we can um
use that to help us make a decision so
i'm going to make a data frame i'm going
to say c no sse there we go df
equals pd data
frame
and i'm going to say k
is the case
sse
equals i need that to be a string sse
is ss e
and silhouette
is souls so i'm basically making a data
frame where i have the value of k the
corresponding sum of squared errors and
the corresponding silhouette scores and
then we're going to use this to make
plots all right so let's go ahead and
build our first plot so we're going to
say ggplot sse df
aes we're going to put the k
so x equals k so the number of clusters
on the x axis
y equals sse
and we'll say plus g on point
plus geom line we'll do both
plus theme
minimal
plus um
let's do labs and do title equals
uh sum of squared errors for different
okay
all right let's go ahead and run this
might take a second because we're
running a bunch of different k-means
models but once that's done we have a
beautiful plot so what we're going to do
here is use the elbow method which is
something that's actually going to come
up a lot in this class so basically as
you create more and more clusters the
average distance between a point and its
cluster center is just going to go down
but what we're going to do is look at
this graph and see if we can find an
elbow and this is not an exact science
but essentially you're looking for the
inflection point so to me it looks at
about five when k equals five we see
this inflection point which is basically
the point of diminishing returns it
means that as we add more clusters we're
not actually getting that much of an
improvement in the sum of squared errors
so we typically want to choose something
around this elbow point but let's make
another plot
actually i'm going to copy and paste
this because they're very similar
so i'm going to make the same plot but
instead of the sum of squared errors i'm
going to use the silhouette
and now let's change this so we'll say
what
score for different k so let's plot this
so you can see similarly we're going to
get a plot but instead of looking at the
elbow method right with silhouette
scores we always want like higher is
better
so again
no perfect answer and it's totally okay
to choose k based on like your domain
expertise if you know a lot about the
data you can be like oh i think there's
five clusters that's totally fine
when you don't know that you can use
these so for instance here with the
elbow method i want to round five and
actually luckily here these two plots
agree
the five is where the silhouette score
peaks so in this case i would choose
five which is well i chose five above
here um
what i would choose for k to be all
right so now that we've learned a little
bit more about how to choose k and about
k-means in general in python let's do
one more example
so i'm going to load in this data which
is major league soccer data
from all years and a bunch of different
clubs
all right so this data set is pretty big
um i think it has like 16 000 rows or
something so let's first um
pair that down a little bit so i'm going
to say mls um
2018. oh 2018
equals mls.loc
um
mls.year
equals
2018 and then all the columns so this
basically mls 2018.
this should grab all of the rows that
are from 2018 but not like all of the
rows total which will be a little more
manageable let's actually just call dot
shape as well see how many rows there
are
okay so there's 829 data points in this
so that should be not thousands but
enough that we can do some meaningful
clustering
okay so i am going to cluster on three
variables i'm gonna cluster on goals
assists
and fc which is fouls committed to get
like a defensive stat in there
so we're gonna just basically do what we
did before because i personally know
almost nothing about soccer and so i
don't know how many clusters we should
have
so i'm going to say ks equals um 2 3 4 5
6 7
8
9 10. um because i don't think having
more than 10 clusters would really be
useful to us because i mean at a certain
point those clusters are going to be too
niche um so i'm going to only let it be
up to 10 that's the only ones i'm going
to test and then we're going to say sse
equals blank sills equals blink
um and let's grab our data so we're
going to say x equals mls
2018
bracket print
okay so let's go ahead and z-score our
variables because you can see um they're
on very different scales so we're going
to say the
equals standard
um and then x
tread equals z dot fit
so this is basically going to z-score
our data and we've grabbed only the
three columns that we want to look at so
we're going to say 4k in case
km
n clusters equals k
and then sse dot append there we go k m
dot inertia
and sills dot append uh silhouette
uh km dot predict x
okay and once we have done that we can
make the two plots that we made earlier
so we'll need to make our data frame i'm
going to copy this from above
so we have our data frame with the k the
sum of squared errors and the silhouette
scores
and then let's copy and paste our plot
so that is going to be the sum of
squared errors and then
um do our
silhouette
so we'll say
beautiful for we'll just change this
different
case
all right so let's go ahead and run this
it might take a while there's a lot of
data
beautiful so we can see our plot and
then let's look at our silhouette plot
oh what did i
still
what probably just misspelled it
beautiful okay so let's take a look at
our plots and see if we can decide on a
number of clusters based on this
all right okay so let's look at these
plots a little bit so when the sums of
squared errors graph we see an elbow
oh it's not super clear maybe around
three or four
and then with the silhouette scores it's
kind of all up and down and all over the
place but yeah three or four maybe four
is a little bit better here again more
of an art than a science i'm gonna go
with four here so let's go ahead and say
km equals k
n clusters equals four
and we say
km.fit x and then we'll say
mls 2018
equals km dot predict
and then we'll just look at that mls
2018.
head
beautiful so now that we know that we're
going to start with four clusters let's
go ahead and run that beautiful so all
of our data as normal and now we just
have the clusters appended to the end of
our data frame
all right so i'm actually going to use a
for loop and loop through all of our
different predictors which is goals
assists and files committed and i'm
going to make a box plot so we can see
for the four different clusters like how
many goals assists and fouls do they
have so i'm going to say print gigi plot
the first thing we have to do is give it
aes x equals factor
y equals whatever p is and let's make it
pretty we'll say fill equals factor
cluster as well
and then we'll say
gm
box plot
and then make it pretty plus theme
uh labs title equals p
so let's go ahead and run
oh now what is that like
oh i accidentally put an underscore
instead of parentheses let's change that
okay beautiful
so okay we can see for goals that
cluster zero has very few goals most
people have zero
cluster one has a lot of goals we have
some really high scores up here but
they're higher than all the other
clusters anyway
cluster two has a moderate amount of glo
goals and cluster three has
not too many but definitely more than
cluster zero
um for a cis we have cluster zero is
people who do not have any assist so
these people do not have goals do not
have a lot of assists
cluster one had really high goals their
assists are actually a little bit lower
but they still have quite a few
cluster two has a lot of assists and
cluster three doesn't have a ton
it's not great not horrible
and then let's see for fouls committed
so clusters one
two and three are actually
roughly similar um cluster three tends
to have a lot more outliers of people
committing a lot of fouls here and then
as usual cluster zero has very few
um so basically what i would say on my
very limited soccer knowledge is that
cluster zero are people who maybe aren't
getting a lot of play time or aren't
like where the action is because they're
not scoring a lot of goals they're not
getting a lot of assists they're not
getting a lot of or like they're not
committing a lot of fouls either
um cluster one looks like some of your
starters like uh offensive i guess
because they're scoring a lot of goals
they
they have some assist though so i would
actually say these people are like the
stars players right because they have a
lot of goals but not as many assists as
some of the other clusters
and they have a moderate amount of fouls
um cluster two interestingly seems like
people who are like
helpers i don't know i mean i don't i'm
not a huge soccer player but basically
these are people who score some goals
but they really shine compared to the
other clusters with in terms of assist
so these are people who are helping
these cluster one people score their
goals
uh and then finally i would say cluster
three is probably like
defensive people because they're not
scoring a lot of goals they're not
having a ton of assists but they are
committing a similar number of fouls to
these two groups in the middle which are
like very heavy like uh offensive like
goal scoring players so these are
probably people who are not in those
positions but are still like getting a
lot of game play
um let's make some other plot so let's
go ahead we can say ggplot
aes x oops x equals g
y equals a
and we'll do
color equals uh cluster
and actually we should do factor cluster
and then plus
point
and then plus theme minimal
plus labs x equals g
uh title
hills goals versus
assist
all right so let's go ahead and run that
okay so we can see pretty much the same
information we saw before so our cluster
zero is like people who are not really
doing much offensive playing at all
they're like at the bare bones of both
goals and assists um whereas our two
like kind of more higher performing like
offensive player clusters are both up in
goals and assists but you can see very
clearly
cluster two is like very high assists
and not as high goals and cluster one is
very high goals not as high as cis
purple's kind of there
more offenses which we'll see in a
second so let's
go ahead and
copy and paste so instead of goals and
assists let's look at goals and fouls
committed
goals versus files
all right
yeah so we can see a similar pattern to
what we saw before so the interesting
part here is that the purple cluster now
kind of pops out right these people are
having a lot of fouls which probably
means they're getting a lot of game time
but they're not players that are like
offensive scoring a ton of goals
all right what else can we plot well we
actually don't even have to make a plot
let's use group by and like summarize
things so we can just get a bigger
picture so let's look at mls
2018 dot group
cluster i think it's called dot mean
and let's just grab the ones we care
about we'll say goals
assists um and fouls committed
um what doesn't it like here
don't mean oh right we have to do this
beautiful okay so you can see using
group by the different clusters we can
just see all the information we've seen
before again this is like kind of our
high scoring offensive like superstar
players
um and these are players that really
don't look like they're getting much
play time at all and then these two are
in the middle with these being like
higher assists than goals
so one thing we can do that's a little
bit fun especially because we have
hundreds of data points is we can like
randomly sample data points and just see
who is in each cluster
and use that as a way to kind of like
see what the clusters are again i don't
really know a lot about soccer but
hopefully if you do you know a lot about
who these players are
so let's start with cluster one because
these seem to be like our superstar
players in terms of scoring goals so
hopefully we'll know who they are
so what i'm doing is i'm saying only
grab the people in cluster one grab
their names and then randomly sample 10
of them so we can just see who they are
so let's go ahead and run it didn't like
that
oh it's because this should be cluster
all right so we have someone named
bradley wright phillips uh dom dwyer
vaco i mean this person goes by a single
name so i'm sure they're
very famous
uh zlatan who was on the galaxy recently
i don't think he is anymore um so
hopefully if you're into soccer you
recognize some of these names and maybe
you can confirm to me and your friends
that these are players that are like
getting a lot of gameplay getting a lot
of goals
um let's see who the
uh defensive players are so that would
be
these people yeah so cluster three
so let's sample 10 people from there
okay so we have russell t
bear
antonio
de la mia
abu dhan ladi chris mccann jack price so
again if you know soccer hopefully you
know who these people are they seem to
be getting a lot of game time because of
the number of fouls they commit but
they're not scoring a lot of goals so i
assume they're kind of like in the back
half of the field um but you can even
play around with this on your own sample
more players or do this a bunch of times
so you get different players and see if
you recognize the names that come up
last but not least i'm just going to
make a couple graphs for fun so let's
say ggplot mls 2018
yes we'll say x equals a cluster oops we
need to as a string
and we'll say
uh fill
equals position
yeah that should be it and then we'll
say plus gm bar
lamps title equals cluster versus
position so if you play soccer and know
anything about soccer i bet there's some
differences in the clusters of like
which positions are happening in each of
our clusters so let's go ahead and make
this graph
okay so this is actually something
called a stacked bar chart which is
super fun so basically each bar the
height of the bar is like the count of
players in that cluster
and then the color is like what
proportion of that cluster is each of
these positions i don't actually know
what these mean with the dm i guess
defensive and mid
um but we see like the big ones are like
the separate ones so interestingly we
see that this cluster three remember
these are the people that are getting a
ton of foul so they're getting a ton of
play time but they're not really scoring
a lot of goals and interestingly we see
a lot of defensive players there which
makes sense because the offensive
players are probably the ones that are
actually scoring the goals so it makes
sense that a lot of defensive players
are here but we see almost no defensive
players in these clusters one and two
which again are the ones that are
scoring a lot of goals and um assists
so you can see here we have some mints
we have some forwards but we're not
really seeing much defensive players the
players that are not really getting much
play time they're not really doing much
at all anything at all uh they are you
know kind of all over the place but it's
really interesting to see that pattern
that we recognize that cluster 3 was
pretty defensive and then we see
a lot of those players are defensive
all right so last but not least i won't
make you sit through the creation of
this but what i'm going to do is look at
a plot that tells us a little bit about
the play time between the clusters so
basically we have all these different
clusters and we've kind of
self-identified that clusters one and
two probably have a lot of play time and
maybe cluster three as well but we think
that cluster zero are people who don't
really get on the field much so let's
confirm that
beautiful just as we predicted so we can
see that cluster zero really is those
players that are not getting any field
time which makes sense that they
wouldn't have any goals assists or even
really fouls because they're like never
getting on the field um clusters one two
and three are playing a ton which makes
a lot of sense and that even confirms
what we knew about cluster three right
they're not scoring a ton of goals or
having a lot of assists but they're
clearly getting a lot of play time as
evidenced by um
just like the sheer number of fouls that
they commit um so we can see that these
players still are getting a lot of play
time and it again confirms that these
are probably more defensive players all
right i won't bore you with soccer
anymore but that is all i have for you i
will see you next time

 
hello and welcome to your topics in data
science lecture
so this is going to be a bit of an
amalgamation
of three topics that i think are so cool
so interesting but we didn't have time
to cover them fully during the semester
and the first thing that i want to talk
about is related to things that you've
obviously already learned
and that's clustering and in particular
this is a type of clustering called
node-based resilience clustering
that is pretty unique and i came across
this as part of my research
doing a survey of how machine learning
specifically
unsupervised machine learning is used in
autism spectrum disorder research and i
found it
fascinating so
one way that we can think of data
especially the type of data that we've
been using in our class
is using this kind of k nearest
neighbors graph
and in this graph you basically have
every single data point in this case it
is
hamburgers i believe from wendy's and
mcdonald's and a bunch of other fast
food restaurants
and we can think of a graph as
connecting a bunch of different points
that are related
and in this case with a k nearest
neighbors graph as the name implies
each data point is connected to its k
in this case i think it's three or four
uh
nearest neighbors and this makes a graph
and if you take in graph theory i'm sure
this makes a lot of intuitive sense to
you
but even if you hadn't you can basically
think like this
is a map that tells us how close
together
all of these different in this case
sandwiches are
and node-based resilience clustering
uses graph theory and uses graphs like
this
in order to cluster data which i think
is so cool
and we'll revisit this in a second but
the reason why i think this is so
intriguing and unique
is that we don't necessarily need the k
nearest neighbors graph we also have
data
in this world that is just naturally in
graph form whether it's
people who follow other people on
instagram or
facebook or people who have dated or
worked together
um this specific graph is i think i've
mentioned before
from love island the 2018 series
of all of the people who coupled
and so basically instead of a k-nearest
neighbors graph where we just take data
that we're used to seeing
and making it into a graph by connecting
data points that are similar
this is naturally graph-based data
people in love island
basically couple up with one other
player
and you can switch couples throughout
the entire game
and so the connections in this graph are
just inherent to the way that that game
works
same thing if you did this with
instagram followers or
something like that and this is super
cool because this is a slightly
different
type of data than we've been looking at
but it's something that's
super common in the real world and
can totally handle that so whether it's
a k-nearest neighbors graph that we've
created
or a graph that naturally occurs due to
the nature of the data
node-based resilience clustering can
break it up into
separable clusters the way
that node-based resilience clustering
works is it looks
for points in that graph that are
connected to a lot of
other different points and those
connectors
actually tend to be outliers they're
people that are connected to
uh people in a lot of different clusters
or i guess data points in a lot of
different clusters
and so the whole point of node-based
resilience clustering is that we try and
find those connections
that when we break them and in this case
on the screen you can see that they're
represented
by these blue dots when we break those
connections
which ones can we remove so that the
broken connections result in all of
these
like dense separable clusters within
that graph
and basically we remove those points
they're called the attack
set in this case but we remove those
to give us all of our separable clusters
and so similar to db scan which we
talked about handles
noise which are points that are not
really quite part of one cluster or
another
node-based resilience clustering also
allows us to do that
we can pull these connectors that we're
removing from our graph to make
separate little clusters and we can do
whatever we want with them we can throw
them out we can examine them as their
own
outlier cluster or if we really have to
assign them to a cluster we can just put
these ones that we removed
in the closest cluster to us
to give you a better idea of how this
actually looks in real life
here is the clustering results from
for those hamburgers you can see that
once we removed a couple of those
densely connected burgers
in fact let me scroll back up
and show you for instance this
is a very densely connected point it's
not
really near any of the um
other data points but it connects these
two other groups
and when we remove it those become
separate
self-contained clusters and you can see
that's exactly what happened here and so
now once we've removed a couple of those
connector kind of like on the outskirts
outlier points
we now have these three separate
clusters so for instance
i haven't eaten all of these but based
on the names
these over here look like they're um
very decadent
you know cheese bacon like tons of stuff
these seem to be kind of your basic
like lower end sandwiches that are just
very simple
and these are maybe sort of in the
middle and you can see that node-based
resilience clustering was able to
take these clusters and detect them
by removing those sandwiches in this
case that were sort of
in between all the clusters to give us
these highly separable clusters
similarly this works on the love island
data when we remove a couple of the
players that kind of connected
groups of other players you can see that
we get these clusters
of love island contestants that were all
connected
just to each other and not to any other
cluster
i guess the tldr of this since you're
not going to have to implement it is
that there are some really cool
clustering methods out there
we learn just the very basics the ones
that are implemented in sk learn and
that people talk about in tech
interviews
there's a whole world of techniques out
there that can work not just with the
data that we're used to
but with other really cool and really
prevalent types of data like these
graphs like those connections that you
make on facebook
or of co-workers or something like that
and i hope that when you continue on
past this course
that you'll look into some of them
because there's not
just k-means and gaussian and db scan
there's so many cool methods out there
and i really hope that you
go look for them the next topic that i
want to talk about
is actually lots of them together
i want to talk about ensemble methods so
so far in this class we build just like
one model right you know sometimes we
build a couple different models and
compare them
but we've only really been building a
single model
but in the real world especially if you
have huge amounts of computational
um power you might want to fit
multiple models right why not get the
best
of both worlds or not even both of all
worlds
and ensemble methods allow you to do
just
that and they're basically what they
sound like
ensemble methods are ways of using
multiple models and
pooling them and their wisdom so to
speak
together so instead of building one
model i might build a bunch of models
and if they're say classification models
i would let them vote on what the
correct answer is so maybe i have 10
or 30 or 100 different models and we fit
them all separately
and then we let them all vote on what
they think the most likely category is
one super common occurrence of an
ensemble model
is actually pretty simple and it's
called a random forest
we covered in this class decision trees
which are
like i've said flow charts you make with
math and
the idea behind a random forest is that
we take a
bunch of decision trees we train them
separately
and then we just run all of them and let
them
vote so each tree gets a vote
on what the correct category should be
and that's why
you know a bunch of decision trees we
get a random
forest but the cool thing about a random
forest
is we're not building tons of copies of
identical trees
rather there are a couple things that we
can do to improve
the generalizability of our random
the first one you may have heard of
before is called
bagging which is short for bootstrap
aggregating
and before i go into what that is let's
just really quickly cover what
bootstrapping is
so bootstrapping is this idea that we
treat
all of the rows of data that we have
kind of like a population of data
and we sample from it so for instance if
we started out with 10
pieces of data like you can see over
here bootstrapping says
okay create another sample of 10 by
randomly selecting
with replacement from the original set
of 10
and this means that we end up with a
bootstrapped sample the same size as our
original sample
but because we're sampling with
replacement that means that some rows
might occur
more than once and some might occur not
at all and so we basically get a bunch
of different samples if we
did this over and over that are sampled
from the same data but aren't
exactly the same well in a random forest
bootstrap aggregating is basically
saying that when we train each tree
instead of training each tree on all of
the training data that we have available
to us
we will use bootstrapping bootstrap
in order to select the data that we'll
use to train
which means not every tree will see
every single data point and that can
help a little bit because remember we
know that sometimes models don't
generalize well
because they're picking up on noise
that's in the training data that doesn't
apply to the whole population
when each of our different trees is
trained on slightly different samples of
our original data
that can help our trees generalize a
little bit because then they
might be less likely to glom onto those
tiny little patterns because not every
tree
gets to see every data point so that's
dealing with the rows but we can also
select a subset of columns to train on
now we're not sampling with replacement
here but it's a similar idea
so instead of training every single tree
on
every single feature or predictor that
we have available to us
we can randomly select which features to
include for each
tree and again this helps with
generalizability because
if there is an over reliance on a
certain pattern
well not every tree gets to see every
single possible feature
and so it's less likely that the forest
as a whole
will be overly dependent on some kind of
noise pattern in the training data
which will help it generalize to future
better so just to summarize we talked
about bootstrap aggregating or bagging
where we randomly select
with replacement rows from our training
data so that every tree doesn't see
every single data point
and we can also do a feature selection
where we only choose a couple of our
features or predictors
for each tree so not every tree sees the
same features
both of these things can help our random
forest as a whole
generalize past just our training data
and like i mentioned at the beginning
with all ensemble models the random
forest basically builds all these trees
using those two
methods that i talked about and then
once all of our trees are trained
any time we have a point that we want to
classify we run it through
all of our trees and then every single
gets a vote as to which category it
thinks that data point should be in
and our forest classifies that data
point as whichever is the most
common category out of all of the trees
in the random forest
now ensemble methods don't end with
random forests in fact random forests
are a pretty simple ensemble method
there are tons out there and again with
all of these topics i hope you go and
explore it on your own
but i wanted to plant that seed because
ensemble methods are really
common in a lot of fields and it's
important to know one
what they are and two why an ensemble
method rather than just fitting a single
model can be
really beneficial depending on your
computational power
and your problem oh and last but not
least
one of my favorite things in the world
you've probably heard me talk about this
uh flippantly during class the last
thing that i want to talk to you about
bayesian statistics and we've had a
little bit of
a taste of bays through the naive bayes
algorithm
but i want to tell you an entirely
different way that we can apply
bayes rule-based theorem to do
statistics
and inference in general the beauty
of bayesian statistics is that it takes
evidence that we can grab from data and
combines it with previous beliefs or
knowledge to
get new beliefs or updated beliefs
so for instance you maybe are meeting a
new person
and your prior expertise says that
people are typically pretty cool
but you know there's a little bit of
variation and then you start to get to
know this person
that would be the data and they tell you
that they're a huge star wars fan
and that they love blaze pizza
and you start to update your previous
opinion and say hey
i think this person is pretty cool you
took what you knew
just generally without any data about
human beings and
updated it with specific data or
information
that you got from talking to your
hopefully new friend to
come to the conclusion that like hey
kind of like this person
bayesian statistics while it's not
always so vague in general
is basically a way of using math to do
exactly that
and this contrasts with a lot of the
typical ways of doing statistics which
you may have learned with like
p-values and hypothesis testing where
in those cases you're really not
considering prior beliefs
or expert knowledge you're just looking
at the data and making a conclusion
without including explicitly all of the
things that you knew before
you even saw that data and in my opinion
that's an incredibly valuable thing to
be able to do
okay let's look at a little bit more of
a
an interpretable example so remember
that bayesian statistics is all about
combining what you previously knew or
previously believed
and updating it using evidence given to
you by data
so say that we run into this woman we're
on a boardwalk and
she claims to be psychic and she says
that there is
an 80 chance
that she can predict a die roll so if
you roll a die
and you don't show it to her she has an
80
chance of knowing what that die roll was
well we know just based on math that if
you are randomly guessing what the
outcome of a die roll is
you have about a 16.67
chance of correctly guessing 16.67
is just one sixth as a percentage
so say that we are together and
i am a huge
skeptic i don't think that people can be
psychic
so i just come into this whole situation
even before seeing any evidence
thinking this person is probably not
you however are a strong believer in
psychic abilities and you have a lot of
experience with it so you think it's
actually pretty likely that this person
if they claim to be
is actually psychic and so we
decide to test this woman out
and we make three dice rolls
and each time she is able to
correctly predict what that die roll
outcome was
well if she was really clairvoyant and
could predict the die roll
the probability of correctly guessing
those three
die roll if she has an 80 chance at each
one
would be 0.8 times 0.8
times 0.8 or about
51.2 percent so 51.2 percent of the time
that she rolls those three die
we would expect her to get all of them
correct which makes sense because she
claimed that she can
on the other hand if she's just guessing
getting three out of three die roll
predictions correct
is quite unlikely it's 0.16
times 0.16 times
0.16 um for approximately
let me erase that uh 0.4
chance so this is data we've just
collected data on whether or not this
woman is psychic
and the evidence points to her being
psychic because it's
much more likely that she would get
three out of three correct predictions
if she were psychic
compared to if she wasn't and so
the beauty of bayesian statistics is
that we can take this evidence which is
the same for both of us we witness the
same die roles we know all the
probabilities
and we can use it to update our beliefs
so
i was a huge skeptic so while i'm
slightly more convinced this woman might
be psychic
it's not enough evidence to fully
convince me because i was so
skeptical to begin with so combining my
prior knowledge and beliefs
with this updated evidence i'm more open
but i'm still really unsure and doubtful
that she's psychic
you on the other hand who was already
ready to believe that she was psychic
take that previous belief update it with
the evidence of
three out of three correct die roll
guesses and you now
are fully convinced that this woman is
psychic at least when it comes to dice
so you can see that we can take our
beliefs we can look at objective
evidence and we can
update those beliefs and that is the
entire beauty
of bayesian statistics now that's a very
real world
example but bayesian statistics is also
super applicable in things like science
and data science
for example think about a regression
model that has coefficients
we have so far been modeling things in a
way
where we are basically saying that
before we see any data
any possible value of the coefficient is
equally likely it could be a million
it could be zero it could be one it
could be 0.2
who's to say we don't know but in the
real world we
often do have some kind of idea about
maybe some of the upper or lower bounds
of our coefficients
for example if we want to look at the
effect of hair color
on salary we might not know exactly what
that effect is
but we probably know that having for
instance blonde
hair the effect is probably going to be
less than say
two hundred thousand dollars a year
because
there's just no way that it can have a
bigger effect like that
otherwise we would have uh blondes
making millions and millions of dollars
and people who aren't blonde making like
nothing
bayesian statistics allows us to take
those very reasonable ideas that like
really big coefficients for hair color
on annual salary
aren't very likely whereas smaller
coefficients are a lot more likely
and this information can empower us to
make even
better conclusions about our model
because we're taking
outside information combining it with
the data that we have
to make an even stronger conclusion
in fact we've kind of already been doing
when we talked about lasso and ridge and
just regularization in general
we talked about the fact that we add
these penalty terms
to our loss functions and you can kind
of think of these penalty terms as us
injecting outside information into
our model analysis basically both
lasso and ridge are our way of
injecting the belief that coefficients
that don't really improve the model fit
and are kind of large
should actually be a lot smaller in the
case of ridge
or exactly zero in the case of lasso
we are forcing those beliefs they don't
come from the data
we are saying oh we have this belief
about what coefficient should be
lots of them should be zero in the case
of lasso or very small in the case of
bridge
and we are taking that outside
and injecting it into our model and
that's exactly what bayesian statistics
does but you're not just limited to
penalizing coefficients you can do
a lot more complicated things and
incorporate some really useful
and informative beliefs into your model
that don't come from the data but just
come from knowledge and expertise that
are
outside the specific data that you
collected
so i guess technically you guys have
invasion
all along to wrap that up basically what
i want to say
is bayesian statistics in general is
just a great way to combine
evidence from data as well as expertise
and knowledge that we had before seeing
the data
to combine that and just get this
holistic updated belief that we have
looking at both what we knew before and
evidence that we collected in a data set
i could and have talked about this for
days
so i'll stop it there but i just wanted
to say bayesian statistics is huge
especially if you go into industry
or any place that you have people or
gurus that are really knowledgeable
bayesian statistics is a way to
incorporate the experience of that
80 year old you know cto who knows
so much about what a b test should look
like
or knows how much churn to expect
bayesian statistics
allows you to quantify and include that
expertise that
years of knowledge built up that people
have
into your actual analysis which is not
something that is usually
appropriate or even allowed in other
frameworks so
i'm a little biased here but i think
bayesian statistics is incredibly useful
both if you end up in an academic field
or especially if you end up in industry
all right that is the last topic i had
for you but i hope getting a taste of
some of these
really cool things that we didn't have
time to cover in the rest of the
semester
just gets you excited for things that
you can learn beyond this class
you seriously only scratched the surface
of what data science is
and i think you're going to have a lot
of fun exploring
all of the cool more specific and
applied things that we don't get to
cover
here but i hope that you take the
fundamentals that we learned
and combine them with all of the cool
methods that you're going to learn in
the future
and just be a well-rounded kind
empathetic and effective data scientist
okay i will leave it there see you guys
next time

 
hello and welcome to your ethics lecture
so
while we've touched on ethics here and
there and how it applies to certain
things that we've learned
we've mostly so far been focused on the
algorithms
the math and the coding implementation
of how you actually perform
data science but i now want to talk to
you more about what it means to be
an ethical data scientist and
unfortunately
being an ethical data scientist isn't
something that i can teach you
it's not a checklist that you have to
complete it's not a to
don't list of things that you should
avoid
it's more of a process and it's
something that hopefully will continue
through your entire career
as a data scientist the other part of
this lecture
is to watch short videos from people who
practice data science
in different fields and listen to how
ethics impacts their work as data
scientists
so this lecture which hopefully will be
very short is
meant to be a conversation starter it's
meant to be a glimpse into
how to ask yourself questions and see if
you're doing ethical work
and improve the ethics of your work
it is by no means meant to be a
comprehensive list of
things to look out for or things that
could possibly be issues in ethical data
science
the main thing that i want you to take
away from this lecture is that you
should be
constantly asking yourself questions
well i wish i could give you a
comprehensive list of questions at least
i'm hoping to
spark in you this idea that you should
be asking questions
and honestly not just of yourself you
should also be seeking help from
experts whether it's experts in machine
learning or experts in the domain that
your data is from
you should never go about data science
alone despite what all those
medium tutorials teaching you how to
make a deep neural net may suggest
data science should be in some sense a
collective process
where you learn from other experts and
you bounce your
ideas off of them and ask them and
yourself
a ton of questions to make sure that the
data science that you're doing
is ethical the first scenario that i
want to talk about is the 2020 gcse
which was cancelled due to covid
instead of holding the in-person exam as
normal an algorithm was developed to
adjust
teachers predictions about how well
students would do
theoretically the goal of this algorithm
was good it was trying to combat things
like great inflation
so it would uh raise the scores of
people
when they thought that the teacher was
too harsh and lower the scores when they
thought the teacher was too generous
but as a result of some of the structure
of the algorithm
the algorithm systematically was biased
against people with lower
socioeconomic status because those were
the people that were more likely to
attend
the larger state-funded what we would
call public schools
and raised the grades of people
in smaller private very expensive
schools
this is a great example of times when
your algorithm
does things that you don't intend it to
it's really important to look at the
results of your algorithm
and make sure things like this are not
happening
or at least trying to limit the amount
that these things happen
another example is actually something
that we've talked about in class before
algorithms that are used in our justice
system
for example there was a system that was
used to try and predict recidivism which
is basically when people
who are in the justice system might
recommit
like misdemeanors or felonies shortly
after their first one
and this algorithm even though it didn't
have
a super large difference in overall
accuracy between white defendants
and black defendants the way that the
model was wrong
between black and white defendants was
incredibly
startling the model essentially
under-predicted recidivism for white
defendants
and over-predicted it for black
essentially meaning that the algorithm
said that it was more likely than it
actually was
that black defendants would recidivate
which is a word i just learned reading
about this
this is why it is so important to not
only look at overall metrics like
accuracy
but also look at how your model is
incorrect are there certain groups for
which it is
biased and if so what can you do to
combat that
in your model now you all might be super
annoyed that when i give you quizzes i
ask questions like specifically what are
the consequences
and i take off points when you say
things like the model's inaccurate
it's because of situations like this
where the way that a model is inaccurate
for different groups
is incredibly important you have to be
specific
in thinking about how your model is
wrong
often overall metrics like accuracy
are only the top layer you really often
have to dig
deeper another point that i want to make
is even when people
go in with good ethical intentions
you have to be very critical
and questioning of yourself to make sure
that you're not accidentally introducing
bias into your model
for instance this model was created in
order to connect
families with public resources and
hopefully to combat homelessness a team
that built this model decided that they
didn't want to
include race they didn't think it was
fair
and so they took out any reference in a
column that had to do with race
but when trying to be careful about
including information that you think
might bias your model
it's really important to think outside
of the box
for instance even if they didn't
explicitly include race as a predictor
including things like zip code can often
encode very similar information and you
have to be really careful
to limit that in your model if you have
decided that it is inappropriate to
include
race another point i want to make with
this model is that
even when predictors improve the
predictive performance of your model
like
race might in this case it doesn't mean
that you necessarily
should include it in your model if you
think that race is an
inappropriate predictor despite
improving the accuracy of your model
you should definitely exclude it another
thing to think about
is the fact that when you're designing
algorithms you'll often be designing
them for
good or benevolent purposes but you have
to be cognizant of
how the information that you put out
there and the models that you build
could be used for bad for instance you
might build a really great algorithm
that matches ads with people on
instagram
and while typically that's going to be
used to help someone find some shoes
they really love
or help them find a new restaurant that
they end up checking out
you have to think about how that
algorithm which targets specific groups
of people
can be used for negative purposes
for instance often on social media sites
like facebook
people who all have lower socioeconomic
status or
people who are in underrepresented
minorities are often targeted with ads
that disenfranchise them
basically they make them feel like their
vote doesn't count or encourage them to
boycott voting or
even give them incorrect election
information
in the hopes that it gets in the way of
them voting so even though you designed
the algorithm to be good
you should always be thinking about what
the potential harm
or misuse of your data or your
algorithms could be
just like in this case and the final
thing that i want to talk about although
again this is by no means a
comprehensive list
of issues that you should consider is
the pale male
problem which is essentially a lesson in
extrapolation
often in things like facial recognition
or medical trials the data is made
predominantly of
white men and this is not inherently
problematic
but it becomes hugely problematic when
the results of these trials or these
are intended to be applied outside of
that group
and that can have really intense real
world
consequences when you build things it's
really important to think about not just
does your model do well
on the data that you have but is your
data
representing well the population of
people that it will be applied to
another huge issue has to do with
medical trials
historically people who were assigned
female at birth
are systematically excluded from medical
trials
because people thought that the
variation in hormones that occurs during
a
person's menstrual cycle could muddy the
waters or
you know obscure medical effects
on the screen i have listed three
examples from an article
of huge studies that did not
include women and this is problematic
because the results of these studies
were meant to
extrapolate and apply to women and
inform their medical care looking back
it seems silly to
extrapolate results from a study that
did not include
people that you're applying the results
to
but it happened all the time and still
has lasting effects on
the health care that people who identify
as women
are getting and this is a huge
problem especially in things like for
instance heart attacks
which often present differently in women
than they do in men
if we don't study those differences we
are likely to make more medical mistakes
or
not provide a level of care that people
actually need
so i want you to be asking yourself not
just does my model perform well on the
data that i have
but is the data that i have
representative of the population
that i want to use this for now i'm
going to end this here
this is a conversation that we could
have for an entire three unit course
but i want to leave you with this being
ethical
is a process it's not a qualification
that you can
attain and list on your linkedin
it's something that will hopefully
continue to be something you work on
throughout your entire career of working
with data
in the data science community it can
often be easy to
focus on the technical aspects like
whether or not you can write sql queries
or how good your python skills are
or whether you can define k-means in
five minutes
but ethical skills and considerations
are often overlooked and my hope as your
teacher
is that at least you won't overlook it
so i want to leave you with that
continue to question yourself
and reach out to others get as much
information as you can
so that you're making ethical decisions
when you're doing data science
okay that's all i have for you i will
see you next time

 
hello and welcome to your first neural
networks lecture so today I just want to
cover some of the basics like an
overview of what neural networks are
because neural networks are really
flexible and some can get really
complicated and it's a lot more material
than I want to cover here you could take
an entire class on this and you could
actually literally take an entire class
on this next semester so I'm just going
to cover some of the basics here like
the basic ideas of what our neural
networking is so neural networks are
just algorithms they're just a way of
taking input data doing some
calculations and outputting something
that you want whether it's a continuous
value or a category or a probability a
neural network just is a way of
calculating what you want your output
from your inputs and you often see this
type of structure and it might make you
think of a brain with all of its
connected neurons which is why a neural
network is called a neural network
because there's some aspects of it that
mimic the human brain now neural
networks like I mentioned before are
really flexible and they can get really
complicated but that doesn't mean that
they have to be complicated in fact
today I'm going to talk to you about a
couple models that you should be
familiar with if you've been paying
attention in class and talked about how
we would represent them as a neural
network so the first one I want to talk
about is linear regression I mean
hopefully work familiar with linear
regression because in linear regression
you just take a series of inputs and we
do some calculations and turn it into an
output usually a continuous value so
let's talk about the different parts of
a neural network in the context of this
linear regression so in linear
regression right we have our inputs
that's one of the main things that we
have is what is the data that we're
putting in and in this case in a neural
network we usually represent inputs so
each individual variable as a node these
little circles here are no
and the reason that we call them nose is
because we're essentially just storing a
value in them a node is a way of storing
a numeric value so we have an input node
those input nodes contain whatever
variable it is that we're plugging into
our linear regression in this case it's
age sex salary and rent and the output
that we're trying to predict over here
label that is the amount that someone
spends on video games so when I said
that a neural network is just you have
some inputs you have some outputs and
then you have some calculations in the
middle that take you from that input to
that output well that's what this is
representing here you can see the lines
that are highlighted in green we call
them a neural networks we go up and call
these weights but you can think of them
in linear regression as the coefficients
from our linear regression model and
this just shows how all of the nodes are
connected so you can see that to get the
output node over here we take all of the
input nodes weight them by whatever
their coefficient is we add something
called a bias now this is just like the
intercept of your model so you can just
think of it as an intercept but
technically in neural networks we call
it bias so you're taking your weighted
coefficients you're adding an intercept
and when you add all of those together
you get your output node or your guess
for how much money someone spends on
video games now the last thing that I
want to talk about about the structure
is that this thing over here the
activation function so an activation
function is just do we do some algebra
to the output value before we return it
to you and in this case of linear
regression we don't all we do is we take
the individual input values we weight
them we add them together we add a bias
and then we output that as our guess for
how much someone spends on video games
in the next slide we'll talk about a
different activation function but for
now you can just realize that this
doesn't really have one or it has
something that we call a linear
activation where we're just returning
the value as is now that's the structure
of the neural network and the other
thing that we have to think about is how
are we training this network how are we
getting the network to learn what is the
best answer how does it make the best
predictions well that's actually also
very similar to what we talked about in
linear regression we have some kind of
loss function sometimes people will call
this a cost function as well and in the
in terms of linear regression we usually
use the sum of squared errors or the
mean squared error which is just the sum
of squared errors divided by the number
of data points and what we're doing is
this loss function is something as we
talked about before that we want to
minimize we want the loss to be as small
as possible and that is exactly how your
neural network learns it iterates
through all of the data and tries to
minimize that loss function when it's
effectively minimized the loss function
then it's done training and it'll output
your model for you so let's talk about
it now in terms of logistic regression
and this should look really similar in
fact the only thing that's different is
in this output node here two things one
now because it's logistic regression
we're actually predicting a binary
variable we're predicting whether or not
someone is a twitch streamer and you'll
notice this little S we sometimes call
this a sigmoid activation and remember
when we did a logistic regression we
said that instead of predicting a
probability or product predicting the
odds
we're actually predicting the log odds
so we have log of P over 1 minus P and
this activation function is essentially
saying I'm going to predict the log odds
and I want you to return to me a
probability and so this sigmoid
activation function just represents the
fact that before we return the values to
the user we are going to put it through
a little bit of algebra so that we can
turn our log odds into a probability or
even a binary 0 1 now the other parts of
the model are exactly the same and we
talked about this in our logistic
regression lecture that logistic
regression is just linear regression in
disguise and I think that becomes even
more clear when you talk about logistic
and linear regression in terms of how
they would be structured as a neural
network because you can totally see look
the only difference in the structure is
right here we put it through an
activation function before we return it
to the user
now just as we talked about in logistic
regression lecture the loss and we're
using this formula right here which as
we talked about before is essentially a
measure of how far was your guess from
the truth which is either it was or it
wasn't part of that category okay so
hopefully this gives you a good overview
of what the structure and a little bit
about what a loss function is in terms
of a neural network but neural networks
aren't just for doing the simple models
that we've talked about in class like
linear and logistic regression you can
build much more complicated models and
there's three main things that you want
to think about when building what's
called a feed-forward neural network
which is just one of the simplest types
of neural networks that you can have the
first thing that you want to think about
is the structure in the linear and
logistic regression examples we had a
very simple structure we had one layer
of inputs and one layer of outputs and
then we had a bunch of weights
connecting them but you can have
multiple layers in a neural network you
don't have to go straight from input to
output you can go from input to another
layer to another layer to another layer
to another layer to another layer all
the way out to the output however many
layers that you want although obviously
you have to think about your computer
having to fit all of these parameters
but you can technically have as many
layers as you want and just FYI when you
hear about deep learning it usually
means a neural network that has two or
more layers the second thing that you
have to think about is the connections
between your layers so you can see here
that we have something called dense
connection every node in one layer is
connected to the node in the
next layer and so on and so forth but
that doesn't have to be the case you can
have some nodes connected to other nodes
and not to other ones the last thing
that you want to think about for each
layer is the activations we talked about
activations in the logistic regression
example because before we return any
value to the user we wanted to make sure
that we put it through some algebra so
that we were giving someone a
probability instead of the original log
odds that a logistic regression model
spits out but you can have activations
at any point in any node in your neural
network and you have to think about what
are the activations that you need or
want in each case now I'm sure there's
gonna be a lot of questions about how do
I pick these what do I do and in some
cases there aren't really good answers
sometimes building a neural network can
feel more like art than science there's
not a lot of hard fast rules that you
have to follow although some people do
you have a lot of good rules of thumb
for how you should build a neural
network and I'm not gonna cover a lot of
those here because there are other
classes that you can take in this series
that will cover neural networks and all
of the details but for now just know
there's a lot of flexibility you can
pretty much build any type of structure
with any type of connection and any type
of activations that you want okay so I'm
going to save this for the next lecture
and I will see you for your Python
lecture

 
hello and welcome to your hierarchical
clustering in Python lecture so just FYI
I've already loaded the libraries and
the data that we're gonna use so let's
really quickly take a look at this data
so this is fake data I made it up but
this is representing students
performance over five tests throughout a
semester so their score on a test is
represented on the y-axis and the test
from the first test to the last test is
represented on the x-axis so you can see
that some students get a little bit
better and then they Plateau
maybe they do a little bit worse on the
final because they're really stressed
and it's hard and they're preparing for
a million different tests but you can
see different patterns throughout the
different students data so what we're
gonna do is we're gonna cluster these
students based on their performance on
these five tests so we could easily do
this in Python but I don't really want
to deal with this right now I'm just
loading the data in a different format
so that it's good for clustering so you
can see here in this data set each row
is a participant and each column is one
of their tests and this is the
participant ID which obviously will not
use it to cluster because it's just an
ID so the first thing that we probably
want to do is to decide which features
are we gonna use to cluster so we could
for instance features we could for
instance just look at some of the tests
I want to look at all of them so I'm
going to so I'm going to say 0 so we're
gonna look at test 0 test1 test2 test3
and then last one test for okay so these
are gonna be our features and we're
gonna say x equals what did I call my
data tests wide features and normally I
would think about scaling my data
however all of these tests are on the
same 0 to 100
so I am gonna think about that and go
hmm okay I actually don't need to scale
my data here they're already all all on
the same scale
okay so let's create our hierarchical
agglomerative clustering object which in
SK learn is called AG low merited which
is so hard to spell clustering so
there's three main arguments that you
will need to give this clustering
function we don't always need to do them
sometimes the defaults are okay but I'm
just going to show you what they are so
the first one is n clusters and this is
similar to k-means and
expectation-maximization where you're
telling it what us what cluster
assignments you want it to grab do you
want it to grab it where we have four
clusters or three clusters or two
clusters this is not going to change the
way that the hierarchical clustering
works so it's still gonna build the
hierarchy when you give it the end
clusters argument you're just telling it
which assignments do I want to pull and
you can change this if you want to later
it's not really important but for now
let's say to the second argument that we
want to think about is called affinity
and this is talking about what is the
distance metric how do we measure the
distance between two points so normally
and the default for this function is
Euclidean Kligman but we could use
cosine we could use Manhattan whatever
okay and then the last one that we want
to think about is linkage and the
default is wards and that's the one I'm
going to put here so essentially this is
saying okay to measure the distance
between two points use Euclidean
distance and then to measure the
distance between two clusters use Ward's
method so I can create this model and
then I'm going to say model dot fit and
give it my data so when I do this one
thing that I can also do is build a
dendrogram and I'm gonna say done drew
equals so the way I'm gonna do this let
me scroll up to that the way I'm gonna
do this is actually with the side
package now we haven't used this before
and the reason I'm using this instead of
ggplot is because there's no built-in
way right now to make a dendrogram with
ggplot so there is an R but not in
Python so I'm not going to be using that
because that would be such a headache to
have to build that manually so we're
just gonna use the default which is from
the SyFy package and I'm importing it
here using Sai pipe cluster 2 hierarchy
and calling this s CH so in this one I'm
going to say SC H dot dunder Oh Graham
and then you're gonna say SC H dot
linkage and you're gonna say what is the
data and then what method of linkage
Emily's Ward and then this is the
default is Euclidean but if you ever
need it the metric I believe it's called
um will allow you to change what
distance metric you're using and
actually let me just double check that
really quickly
be right back okay I'm back yes it's
called metric I totally forgot what that
was called for a second I always have to
Google the documentation so we can say
Euclidean if I can spell that correctly
the default is Euclidean so if you're
using Euclidean distance you don't have
to specify but in the future if we do
use something else this is how you
change that so when we run this it's
going to build our dendrogram for us and
it actually color codes it by selecting
what are the clusters that maximize
remember we talked about maximizing that
distance that vertical distance in the
dendrogram so in instance for this case
the best number of cluster seems to be
two with all the green points in one
cluster and all the red points in a
different cluster so this can help us 1
choose the number of clusters that we
want to pull from our hierarchical
clustering but also we can look and
actually this is the exact same one that
I put in your slide show we can examine
the dendrogram to see how cohesive and
separate our clusters are so these
actually look pretty good you can see
pretty high density at the bottom and
then it's very non dense at
top which means that these two clusters
are pretty separate and because there's
a lot of density at the bottom they're
probably pretty cohesive so yeah that's
a lot of times what we're going to want
to report when we're doing clusters is
we want to look the dendrogram
color-coded or not to tell us like what
are the patterns in this data and what
is the optimal number of clusters so we
can grab the labels membership equals
from our model by saying H a see your
model dot labels underscore and so if
you do this and let me print them out
you can see if we put everything in two
clusters which is what we specified up
here this is what their cluster
assignment would be according to the
hierarchical clustering so if we do that
let's look at the silhouette whoo what
score look look at the silhouette score
for two clusters so we would give it X
our data and then we would give it to
the membership of our data and when we
run this we can see okay that's actually
a pretty good silhouette score 0.64 so
it does look as our dendrogram already
kind of told us like these clusters are
pretty cohesive and pretty separate so
let me add this cluster membership tests
wide to our data frame I'm going to call
it cluster two and I'm gonna set it
equal to membership there we go and then
I'm just gonna build some GG plots
because often you know clustering isn't
supervised machine learning so there's
no accuracy score or anything like that
so it's really important to go in and
look at the different clusters and
describe for yourself or have an expert
described with you what are these
clusters characterized by so for
instance I'm going to say ggplot and
I'll give it tests wide and let's see
what do we want to look at x equals
we'll just look at the first test in the
last test is zero
and y equals weight was the last test
for believe and then we'll say color
equals factor cluster two so that's the
cluster assignment and then we'll add GM
point so let's run them okay so we can
see our two different clusters here so
for instance it looks like one we can
see from the data generally that there
is a really strong correlation between
how you do on the first test versus the
last test and that makes sense because
different people have different ability
levels for different subjects so it
makes sense that these two things are
correlated but we can see that one of
the clusters cluster one tends to be the
lower scoring students and cluster two
tends to be the mid to high scoring
students so this is super useful for us
we can also build box plots or other
scatter plots we can look at all of the
different ones to kind of get a sense of
what the clusters are representing based
on just this graph I would have to
examine it more I would think okay this
is probably a low scoring versus a high
scoring group for this subject now if
you want to use a different number of
clusters then the optimal solution
according to the dendrogram you totally
can do that we can build another model
let's put that down here just gonna copy
and paste our code so if we build a
model and tell it give us three clusters
even though we can tell from this Tundra
gram that two is probably the optimal
number of clusters we can still ask it
for three clusters this is really
helpful especially like I pointed out in
the other lecture when you're in some
kind of like marketing context or
something like that and you want three
clusters it's okay to ask the
hierarchical clustering for three
clusters and so we would do that like
this and then say H AC dot labels
underscore and when you run that you can
see now it's split it into the three
cluster is 0 1 &amp; 2 so you can still ask
it for a different number of clusters
even if the optin
number of clusters is a different number
okay hopefully that made sense I will
see you next time

 
[Music]
hello and welcome to your regularization
Python lecture so the first thing that
I'm going to do of course is going to be
to run all my packages and the next
thing I'm going to do is load all of my
data so this data set is super fun and
it is courtesy of the wonderful Julia
soldier who provides the code to scrape
this data on her website that I have
linked here so what we have here is a
data set about this show the office and
it has a bunch of information but if I
scroll over the thing that we're going
to be interested in today is predicting
what is the rating of the episode of the
office on IMDB and we have a bunch of
different variables here we have well
something we're not going to use is the
episode name but we have the season we
have the episode and then we have the
count of times that each character talks
so you can see here for instance in
season 1 episode 1 Dwight speaks 29
times Jim speaks 35 times etc etc for
everyone and then we also have boolean
values zeros or ones for which producers
were a part of making the show or like
the episode of the show that we're
looking at and we want to use these
variables to predict what is the IMDB
rating for this show so um I'm loading
in just a different format than the data
set it's pretty much the same it just
works easier for DeeDee plot if you're
interested you can go and look but let's
just really quickly explore this data so
I'm gonna say GG plot office long so
let's plot a name so this is the name of
the person and then y equals value this
is the number of times they spoke per
episode
I'm gonna make it a little pretty we'll
say cell equals name and then plus Jian
box plot okay and so when we run this we
should get a beautiful rainbow graph
that shows us across
all episodes of all different seasons of
the office the range of number of times
that each individual person talked so
for instance the clear one that stands
out at least in this one is Michael
Scott and Michael Scott has a huge range
of number of times that he's talked and
that's probably because he wasn't in the
final seasons but he's a main character
for every season that he's in so you can
see that often he's talking a lot but
there's occasions when he talks not at
all probably because he's not in the
episode another interesting woman that I
saw here was Jan if you've watched the
office Jan as a character where she when
she's in an episode she's usually part
of the main storyline but she's not in
every episode and that probably explains
the pattern we see here where for most
episodes she has zero lines but when she
is in an episode she has quite a few
which we can see in all the outliers
that are coming out of this box plot
here for other different characters
probably as you expected for instance
its Pam so she talks a lot sometimes a
lot sometimes a little bit but she
generally has more lines than most
people you can see Jim here and Dwight
here so even if you just look across
this plot you can see well clearly Jim
Dwight Pam Michael are some of the most
main characters at least that are
consistent throughout this series we
could spend hours and hours talking
about this obviously I love the office
and I think it's really interesting but
let's move on for now so I'm gonna grab
the columns basically all the numeric
columns from our data set I'm just gonna
grab their names and store them in the
variable feet and then I'm going to grab
my X values and then the Y values so
here I'm predicting the IMDB rating so
I'm gonna grab those here and then I'm
gonna do a train test split just for
ease of showing you I've seen a lot of
people when you're justifying your
validate your model validation method of
saying like what we used in in class
that's totally fine but you should be
thinking about your actual data set when
you're doing your homework the only
reason that I'm doing train tests but
here is because we have a little bit a
limited amount of time in this lecture
so when
you're actually doing your validation
think about your data set a little bit
more okay so I'm going to run this
actually before I run this let's add
some more things so remember with lasso
and Ridge it's important that you're
standardizing your variables before you
put them in because when you think about
the loss functions that we talked about
in the last lecture those aren't really
accounting for the scale of the
variables lasso and Ridge just say oh if
the coefficients are big make them
smaller so we want to make sure that our
variables are on a relatively similar
scale to begin with and the way that
we're gonna do that here is by using the
standard scaler so say Z equals standard
scaler and then X X train feech equals Z
dot fit
oops fit transform X train feet and then
X test feet equal Z dot transform X test
okay so there we go so when we do this
we should essentially have our Z squared
variables let's just make sure that we
do by saying X train okay so let's run
this yep okay so now all of our
variables are Z squared which is very
nice for us okay so now that we have all
of our Z squared variables let's go
ahead and build our models so what I'm
gonna do is I'm going to build a regular
regression model a little Ridge
regression model and a lasso regression
model and we're just gonna compare them
a little bit now in the future ending
well in class and we'll talk a little
bit about how to choose that penalty
harshness term in the last lecture we
talked about lambda which is essentially
a measure of like how harsh do you want
to be in the penalties for your
regression coefficients and the bigger
limb
the bigger your penalty the smaller
lambda the less that the besides of the
coefficients matters we're gonna talk
about how to pick those in class but for
now I just want to show you the syntax
for how to create these models in
general so of course you should remember
for linear regression we use the linear
regression function and we are going to
say L r dot fit X train Y train and then
what I'm going to do is I'm gonna print
out the mean absolute air for both of
these models and so we can compare or
not both these models for the train and
the test data so we can compare what is
the mean absolute error in our training
data versus our test data because
remember if there's a really big
difference between those that is to a
sign that may be our model over fit a
little bit so I'm going to say print
train okay and then I'm gonna say mean
absolute error and you could look at
mean squared error R squared but I chose
mean absolute error because it's
sometimes mean absolute errors a little
easier to understand cuz it's not like
squared anything so that's why I think
that's I'm gonna say Y train comma ll r
dot predict X train oh and I want test
no no I want train first okay so this is
going to be the mean absolute error for
a training data set so this is data that
we've already seen and then we'll do the
same for a test data set so we'll say Y
test then we're gonna have it predict
based on the test values so let's run
that really quick okay so we can see
that our training mean absolute error is
zero point two nine two whatever about
point three and if you think about the
IMDB ratings that we've seen let's
scroll up here you can see they're
probably on like a ten point scale so
that's actually not bad if the mean
absolute error is about zero point three
that means that our ratings are
typically off by zero point three so
it's not like super great but we're you
know within our
anbal range on our test set however our
mean absolute error is about 0.4 so we
can see that there is kind of a big
difference between our training accuracy
and our test accuracy which indicates
that it's possibly our model is
overfitting to our data let's see what
we can do with Ridge regression next I'm
actually gonna copy this because it's
gonna be really similar so um well let
me just change all of these really quick
first okay so what we're gonna be
changing here is instead of linear
regression we're gonna use the Ridge
function and this is essentially gonna
fit a regression model linear regression
model with a Ridge penalty which
remember pushes small coefficients to be
very very small it doesn't push them all
the way the zero necessarily but it
makes them very very small so let's see
what happens when you run this cool okay
so we actually see not that huge of a
difference in this case so our training
accuracy is again very similar to our
the one from the normal linear
regression we can see that our test
error is so ever so slightly lower but
probably not meaningfully lower so it
looks like at least in this data set
that Ridge regression helped maybe a
little bit but actually not that much
let's see what lasso does okay so with
lasso regression it's very similar to
Ridge we're just gonna use the lasso
function Oh get rid of that extra
parentheses gonna call this LS are cool
so when I run this again lasso pushes
small coefficients to be exactly zero
and so we should see a little bit of a
difference compared to Ridge but it's
kind of a similar idea okay interesting
so with lasso we actually see a higher
error however we see a lot less
difference between the training set and
the test set error and that's really
good because in this case at least it
indicates that our model is probably not
overfitting as much as it was in the
previous two models so essentially you
can think about well okay my error is a
little bit higher in both of the set
however I'm willing to sacrifice a
little bit of accuracy for the fact that
my model will work on all data not just
my tiny little sample so essentially
that's what you're looking for when
you're doing raid your lasso regression
is you want your model to be a little
bit more generalizable okay so those are
the very basics we'll talk a little bit
about how to choose lambda because when
you do Ridge and lasso regression you
can actually choose how harsh your
penalty is and we'll talk about that
more in the lecture but until then I
will see you next time

 
hello and welcome to your db
scan lecture db scan is one of the
clustering
algorithms excuse me that we're going to
talk about in this class
and dbscan is just an acronym it stands
for density based spatial clustering of
applications with noise
which you'll hopefully see what that
name means a little bit later on
but what we've been talking about in
terms of clustering
so far is this idea that good clusters
or at least well-fit
clusters have high cohesion
meaning that the data points in the
cluster are close together
or similar and have high separation
which means that the other clusters are
relatively
far away and this basically
lines up with the similar idea that good
clusters are
dense they're cohesive and they're
separated by areas of low
or no density which essentially means if
you think of like a scatter plot of
clusters that you have clusters with
tons of points in the same area
and then areas between each of our
clusters that have very few
or no data points at all and
dbscan basically takes that idea
and implements it directly as an
algorithm
one of the huge benefits of db scan as
you maybe saw in the full name
is that it approaches this concept of
noise and if you look at the three data
sets that we have
here first of all you'll see there's
some weird shaped clusters which we'll
talk about in one second
but if you look at this third data set
this database
three let me scroll up a little bit so
you can completely see it
you can see that there are based on what
we think as humans
three distinct clusters may be four if
you count this little tiny cluster
here but there's also kind of a
smattering of random points in the graph
that don't necessarily feel like they
belong to a certain cluster
rather they feel like outliers or
noise maybe and the beauty of db scan
is that it not only can pick up on
clusters but it can also recognize
data points like that like these random
smattering of data points
that maybe aren't a real part of a
cluster
in the previous uh types of clustering
that we've covered
we don't really have an option for that
we include
every single every single point in
a cluster but dbscan introduces this
concept where we can have
points that aren't assigned to any
cluster and rather we classify them as
noise so let's go back to these
weird shape clusters that we see in the
previous methods that we've talked about
like k-means or gaussian mixture models
we basically have these assumptions
about what the shapes of the clusters
truly
are in the case of k-means it's very
limiting because in k-means we assume
spherical clusters in gaussian mixture
models as we discussed we assume
that there are elliptical clusters so
basically the
clusters don't have to be perfect
spheres they can be ellipses
but we're still assuming something about
the shape of these clusters
and if you've ever seen real life data
you know that those things don't always
hold for instance if you look at
any of the clusters in database 2
or 3 here you'll notice that there are
clusters that are either
very weirdly shaped and i don't even
know what to call these in the middle
here
and on the right hand side for database
3 we do have some
more spherical clusters but we also have
clusters
that are very clearly not spherical
or even really ellipsoid shaped
one of the huge benefits of db scan is
that
it allows us to accurately portray
and find these weirdly shaped clusters
because we're not making
any assumptions about what the shapes of
the clusters are
like we are in k-means and gaussian
mixture models
however it can't be all fun and games
there are some disadvantages that go
with the db scan algorithm
for instance one of them is that dbscan
might not be as great at finding
in high dimensional meaning lots of
features or lots of predictors
data second it's not great with
overlapping or touching clusters
something that we discussed in
gaussian mixture models can handle
so you can see for instance on your
screen
on the left hand side we have these very
two clear groups or at least it seems
clear to us
as humans but they're touching or
overlapping
and you'll see why when we talk about
the algorithm dbskin
actually has some trouble with this and
similarly it also let me scroll up so
you can definitely see it
it can have some trouble with clusters
where one cluster has
a different amount of density compared
to the other
for instance if you look at this group
here this looks like a very clear
and then this also looks like it could
be another cluster
but the density which is very important
in density based
db scan db skin can have some trouble
picking up the clusters as is there are
some ways around that that we're not
going to cover in this class but
as for pure db scan it can sometimes
struggle
when different clusters have different
densities
we'll come back to this later once we've
gone over the algorithm so you can
better understand it
but just to give you an idea when we run
db scan on these two data sets that i
pointed out
you can see that it doesn't do a great
job at clustering anything
on the left hand side it puts everything
in a single cluster
and on the right hand side it puts
almost everything in one cluster and
then uses
these blue points to represent noise
so it's really not doing a great job of
separating that dense and non-dense
cluster that we pointed out earlier
okay let's talk a little bit about the
of db scan now there's three
main things that you want to think about
three parameters of db scan that you
need to choose
the first one is similar to basically
all of the clustering methods we've
talked about so far
you need to pick a distance metric how
do we decide
how far apart two points are from each
other do we use euclidean distance do we
use
cosine similarity do we use manhattan
distance
any of the distance metrics we've
discussed so far
will totally work here depending on the
application of your data
next we have two db scan specific
parameters we have
epsilon sometimes we'll refer to this as
eps and then we have the minimum number
of points which we sometimes call
min points and these two parameters are
really important in how we determine
what clusters are in the dbscan
algorithm i'll talk about them a little
more in a second
but epsilon basically says how far we're
gonna look away from a point for
neighbors
where if epsilon is small we're only
looking right around the data point for
its neighbors
and if epsilon is big then we're willing
to look pretty far
and minimum points basically helps us
determine the minimum amount of density
that we need in order to determine a
basically when a data point has more
than minimum points neighbors
we're going to consider all of those to
be in a cluster together
so before we start let's talk a little
bit about some definitions
the first definition i want to talk
about is something called a
core point a core point
is a core point if it has at
least min points so that number we
decided earlier
neighbors within x that distance we
talked about before
distance of itself so in this graph with
these data points you can see that when
we have an eps and epsilon of 0.125
and a min points of 3 if we look at
any data point and try and determine if
it's a core point
we're going to look at that data point
and look at any possible point within
0.125 distance of it
which is represented here with this
circle right so a radius of 0.125
means that in this circle would be any
neighboring point
within one 0.125 distance of
our data point and then min points is
how we determine whether or not
something is a core point
so for instance if we're trying to
assess whether the
green point is a core point we're going
to count the number of
neighbors that it has inside this little
circle
of radius epsilon that we've drawn for
ourselves
so here we can see that there's one two
three four data points within its
neighborhood
which means that our green data point is
a
core point and remember core points are
basically
points that have uh density meaning that
there are a lot of other points
immediately surrounding it
the next definition that i want to talk
about is a border point
a border point is similar to a core
point but it doesn't have
quite enough neighbors remember we said
min points is the minimum
number of neighbors for a core point so
if a data point does not have enough
like this green point here which really
only has one
but one of its neighbors is a core point
then we consider it a border point so
if the point has a neighbor that is a
core point
but isn't a core point itself meaning it
has less than min
points neighbors we consider this data
point a border
point definition i want to go over for
now is a noise point
and a noise point is basically a point
that is those smattering of random
points that aren't
really part of a cluster and the way
that we determine a noise point
is if it does not have min
points or greater neighbors and
none of those neighbors are actually a
core point so remember a border point
is doesn't have enough neighbors but one
of its neighbors that it does
have is a core point the difference
between that and a noise point is that
the noise point does not have a core
point
as its neighbor so
just to make things a little prettier
and flow chartier
i made this flowchart of how we decide
whether a point is a core point
a border point or noise so the first
thing we look at is we look at that
neighborhood with radius epsilon
and we say does this point have more
or less than the min samples neighbors
and so we would say if it does have
enough neighbors if it has
more than the min points or min sample
those are two words for the same
variable
then it is a core point so if a point
has a bunch of neighbors within its
core point if it doesn't have
enough neighbors to qualify as a core
point then we ask
does it have any neighbors that are
themselves
core points if it does then we consider
it a border point which doesn't have
enough neighbors to be a core point
but is in the neighborhood of a core
point and if it doesn't
then we consider it noise so again first
we check how many neighbors of points
have
if it's more than min points or min
samples we consider it a core point
if it's not then we ask whether or not
it has a core point neighbor if it does
it's a border point otherwise if it
doesn't it's a noise point
if you need to take a step back and
review what i just said
feel free to rewind and go do that
because now we're going to talk a little
bit more about definitions that are
slightly more complicated
so this definition is going to be
directly
density reachable and basically
when we have two points that are
directly density reachable
we're talking about a point p that is
in the neighborhood of a core point q
so if you have a core point any point
that you can reach any point that that
it is
neighboring from that core point is
density reachable again that means
a point is directly density reachable if
in the neighborhood of a core
point so p does not need to be a core
point itself
but it is directly density reachable
from q a core point
if it is in the neighborhood of that
next we have the definition of density
reachable
which basically means two data points
where they don't have to be
immediately in the neighborhood but
basically there are two points where we
can have a chain
of points within each other's
neighborhood that
connect p to a core point q
so for instance if we're going from here
which is one of our core points
this point is in the neighborhood of
this point which is in the neighborhood
of this point which is in the
neighborhood of this point
so even though our end point we'll call
that p is not
in the neighborhood of q we'll call this
q
it is density reachable because there
are a
chain of core points that connect q
all the way to p as we go from q there's
a neighborhood in that point which has a
neighbor which has a neighbor
which is q so just to review the
difference between directly reachable
and density reachable is whether or not
the two points are in each other's
or in the case of just density reachable
or sometimes we'll call this density
connected
whether there is a chain of points that
connect q a core point
all the way to p all right now that
we've got all those definitions out of
the way
let's talk a little bit about how we use
those definitions to actually perform
the algorithm
of dbscan in db scan a cluster
is defined as any grouping of points
we'll choose a random core point q
and the cluster containing q is all of
the points
that are density reachable by q
so let's take a really quick example of
how the algorithm would work
we would randomly select a point and
that point is a core point the one that
i'm pointing to here
we determine the cluster containing this
point in our case the pink cluster
to be any and all points that are
density
reachable from this point so what our
algorithm is going to do
is it's going to randomly pick that
starting point and find
all of the data points that are
from that core point once it runs out of
points that are density reachable
it'll stop and randomly select another
point to continue on to
if that point is a noise point like this
one which means it's not
actually near any other data points
it'll say that that's a noise point and
move on to another data point
uh until it can do some more clustering
so for instance if the next data point
that we selected is this yellow one
we are gonna go and iteratively find all
of the neighbors within this cluster and
then all of their neighbors
until we've finally done all of the
reachable points in this cluster
so hopefully you can see how this
algorithm performed on this data set
because you can see as it chooses points
it goes and it
does all of the density reachable points
from a core point
and so on and so forth until it's
classified every single data point in
the entire data set
any data point that is noise is not put
in a cluster
remember these are data points that are
not density reachable and don't have
enough neighbors
to be considered a core point or a
border point
so just to review the way the db scan
works is it'll randomly select a point
decide if it's a core point if it is
it's going to find all the points that
are density reachable from that core
point and consider them a cluster
then it'll move on to another point if
it's noise it'll mark it as noise
if it's another core point it'll find
all the density reachable points there
and it will classify that as a cluster
and so on and so forth
iterating through the entire data set
until every data point is classified
either as part of a cluster or as noise
so now that we have discussed the dbscan
let's circle back to those benefits that
i was talking about
so because dbscan is literally just
going from point to point finding all
the points that are density reachable
we can see maybe why this is good for
non-spherical
or non-ellipse shaped clusters because
if all we care about is that a point is
density reachable
then it makes sense that it doesn't have
to be in a perfect sphere or perfect
ellipse in order to be called a cluster
and so when you have clusters that are
weirdly shaped that are
certainly not spherical then db scan can
be a
really great option because it allows
for clusters to be
oddly shaped and doesn't impose a
certain structure on them
the way that k-means and gaussian
do secondly now that we've seen how
noise is determined
it can be really great to have a way for
your algorithm to say
well this data point doesn't actually
fit well with any of our clusters
because if a data point doesn't fit well
with a cluster then
we probably shouldn't put it in that
cluster and dbcn is one of the many
clustering algorithms that allows us an
official way to do that
unlike gaussian mixtures or k-means
which forces every data point to be part
of a cluster
next as we've seen before hopefully it
makes a little more intuitive sense
why the examples i showed you one on the
left where there's
overlapping clusters and one on the
right where the clusters have very
different densities
aren't so great in db scan
for instance on the left even though we
think there are two distinct
clusters the two clusters that we
perceive
are actually density reachable from each
other and therefore
of course dbscan depending on the
parameters you choose
is going to classify all of those data
points together because their density
density reachable from each other on the
right hand side you can see that because
min points and eps our epsilon
are chosen for the entire algorithm that
it doesn't do a great job when the
density of the different clusters is
different because we're using the same
epsilon and the same min points and it
doesn't really fit the
two clusters in the same way now i'm
going to pause here to just say
there are ways around this and there are
algorithms and extensions that can
address this problem
we're not going to cover them here but
you are more than welcome to explore
them on their own
because obviously this is a very real
world problem that people have have to
overcome
and there are some mathematical ways to
overcome the differences in densities
that you might see in clusters
so now that we know how the algorithm
works and some of the pros and cons
let's talk a little bit about those
parameters the minimum points
actually in sklearn it's called min
samples so i sometimes use those
terms interchangeably uh as well as the
epsilon that we talked about earlier and
how you determine them
because these are hyper parameters
they're things that are not
chosen necessarily by the algorithm
they're things that you have to supply
and as we've talked about with hyper
parameters there's really two ways of
choosing them
one is based on domain knowledge and all
of the other like things like distance
metric that you're using
so if you have expertise that allows you
to say
well i know how many minimum neighbors a
point should have
in order to like cluster effectively
feel free to use that expertise that's
exactly what we want to use our
expertise for
but in the many cases where we don't
have that expertise
here's some intuition and rules of thumb
for how to choose the minimum number of
neighbors a point has to have to be a
the first thing that you want to know is
that the more data you have
the larger you want your min points or
an sqlearn min
samples to be hopefully that makes sense
because if you have more data there's
more likelihood that for
areas of high density there will be a
lot of neighbors
next if you have more noise in your data
set
you may also want to have a larger min
points or min
samples value and finally
you may want to have more or excuse me a
larger number of men
points uh when there are more features
so more predictors that you are
clustering on
now for eps or epsilon uh there's
actually a really great way to choose it
if you don't have domain knowledge again
domain knowledge or expertise about the
context and situation of your data
are always going to be great ways to
choose these parameter values if you
happen to have it but if you don't
there's this really great method called
the elbow method
which allows you to choose based on the
data
what a good measure for epsilon should
be
now there's a few variations in the way
that people will implement this but the
one i'm going to talk about
is we're going to make a k dist graph
and k is just a number of neighbors for
instance it probably is the number of
min points or min
samples that you chose so say for your
db scan algorithm you've chosen
five as the min point slash min samples
which means that a core point must have
at least five neighbors within its
radius
well if you know that how can we
determine what that radius
should be in order to do effective
well we can make a k-dist graph where we
look at the five
nearest neighbor in other words for
every single data point
in our data set we will say how far away
is its fifth nearest neighbor and then
we put all of those data points or
excuse me put all those distances in
order
to make the k-dist graph which is what
you see on the right-hand
side of your screen so from the smallest
distance of the fifth neighbor all the
way to the largest distance
you can see this line or scatter plot of
all of the k
distances for every data point in our
data set
the elbow method says that when we look
at this graph
there may not always but there may be
some kind of inflection point
in other words kind of like an elbow
like if you literally look at the graph
it kind of looks like the elbow of the
graph
and whatever the value for the distance
is
at this point is the epsilon that we can
for our db scan algorithm for instance
in this case
we could say okay if i go over
to this side pardon the not straight
line there we go
we might want to use a an epsilon of
about
0.4 or 0.37 whatever the elbow
is and that would be an effective number
for epsilon
based on the k distances graph
essentially what this is doing is
empirically using the data to select
what a good epsilon is by seeing how far
apa
part the points are generally and
choosing that inflection point or that
elbow of the graph
as a good cut off for having your
neighborhood of your points
again just to review if we know the min
point slash min samples value
making a k dist graph using those
will help us using the elbow method to
choose an
epsilon that's appropriate for our data
based on the actual values in our data
all right just to review because we
covered a lot here
db scan or density based spatial
clustering of applications with
noise which we now know what that means
is a great way
to do clustering where we're not
imposing any assumptions about the shape
of the cluster and where we maybe want
to separately classify
noise data points separately from our
instead of including it in a cluster
there are three main things that you
have to think about when implementing db
scan
one as with any distance based metric
what is the distance metric we're going
to use
you can use euclidean distance etc and
then you need to think about
epsilon which is the shape not the shape
excuse me the size or the radius of our
neighborhood that we're going to look at
and min points in sklearn it's called
min samples
which is the number of neighbors that a
data point has to have
or more to be considered a core
point and as a review when we're looking
at individual data points
the way that we decide whether something
is a core point a border point or a
noise point
is first to look at its neighborhood the
size of which is determined by epsilon
and count the number of neighbors that
it has if it has
more neighbors or as many neighbors as
min samples or min points
then we consider it a core point if it
doesn't
then we ask is one of its neighbors a
core
point if it has a core point neighbor
then it's considered a
border point if it does not have a core
point neighbor it's considered a noise
and we use these definitions in order to
iteratively create our clusters
by looking at random data points finding
all the data points that are density
moving on and doing that over and over
until every single data point is
classified either as part of a cluster
or as part of noise all right i know
that was a lot of definitions but
hopefully it made sense and i will see
you for the python portion of your
lecture

 
hello and welcome to your db scan and
python lecture
the first thing i'm going to do as usual
is run
all of my import statements
and while i'm doing that i'm just going
to point out we went over dbscan in a
very basic way but if you're really
interested in some of the deeper math
that goes into db scan
i've linked a paper here for you to read
in case that is something that is
interesting to you
so let's go ahead and load our first
data set
and then let's take a look at it so
we'll say d1.head
and we run this you can see that this is
just a simulated data set with two
variables
x and y and because db scans similar to
all the other clustering methods that we
used is a distance based algorithm
we're going to need to z-score or in
some way scale our variables
just to make sure that they're on
similar scales because when you
calculate distances
it's helpful if all of our variables are
on the same scale so there's not one
scale that like
dominates all of the others so in order
to do that
as we always have we're going to say z
equals standard
scalar and then we're going gonna say d1
and x y
equals z dot fit
transform d1
x y and
you may be asking me well why do we not
have a train and test set why
all of this and maybe as we briefly
touched on before
remember that clustering and all of the
algorithms that we've learned as a part
of this
are unsupervised machine learning
there's no
truth labels and therefore we're not
estimating
typically our out of sample performance
and therefore we don't have our train
and our test sets
and we don't have to therefore worry
about data leakage from the test set
into the model now that we've z-scored
our data
let's go ahead and plot it to get a look
i'm going to say ggplot
d one s
x equals x y equals
y and let's say plus a geom point
and then of course plus theme
minimal because we like that beautiful
black and white
and let's go ahead and run that cool so
now we have a graph a scatter plot of
and we can see that maybe there are two
ellipsoid-ish
clusters there's also some weird points
here but you could see maybe there's a
cluster
here and a cluster here let's go ahead
and see
how dbscan performs so to set up my
dbscan model i'm going to call it db1
we're going to use the dbscan
function from sklearn and we have to
provide it with
at least those two metrics we talked
about from the previous lecture
the epsilon and the min points or as s k
learn calls it
men samples the default distance metric
for db scan and the one we'll be using
throughout this process
is euclidean distance so we don't need
to specify that
but if you want to use a distance metric
other than euclidean you will have to
specify that as well
so we're going to say x equals 0.5
and min samples and this is again same
thing as min points
equals 25.
and normally we would fit this
separately but now that you've been
doing this for a while you know and
especially because there's no train test
split or anything we can just directly
fit this to the data
d1 now when dbscan returns
the cluster assignments it classifies
all of the points that are noise
as negative one and then returns the
cluster membership for all of the rest
of the data points as we've seen before
like zero one two three four et cetera
um so just to make that a little more
clear because remember we
know that noise points aren't actually a
part of any cluster
i'm gonna make some new labels so that
when we plot our clusters it'll make a
little bit more sense
so what i'm doing in this code here is
basically saying that the negative one
cluster the first cluster that appears
is just our noise cluster and then i'm
adding all the other clusters to this
list and we're going to use this to
label
the legend in our gg plot in just a
second
so in order to grab the assignments from
our model assign
there we go um we are gonna use the
db1.labels
uh attribute to grab the actual cluster
assignments from the model that we've
now
fit now that this is all in a data frame
we can go ahead and plot it
we'll say gg plot
d1 aes we'll say again x equals x
oops there we go y equals y
and now we're also going to color by
assignments
and then we'll go ahead and say plus gm
point plus theme
minimal and now the next thing that i'm
going to do
is just uh going to be to clean up the
graph to do some of the things that
we've talked about to make graphs more
effective
one of those is going to be that i'm
going to re-label the legend actually
let me why don't i show you what it
looks like as is so when we
run this you can see first of all
uh that our assignments even though
they're like negative one to one
uh uh python gonna sorry uh thinks that
this is a continuous variable so we're
going to first say
factor in order to tell python oh i know
this is a number but it actually refers
to a group
but second you can see that it just does
it from negative 1 0
and 1 and we want to be more clear that
negative 1
means noise and the other clusters are
actual clusters
so what i'm going to add here and i'll
come back when i've added it and go
through it
is just going to be to better label
those points so we can tell which are
noise
and which are actually parts of clusters
all right i'm back
just as a review this is what our graph
looked like before
and now we've made a couple changes
first we
have changed the legend so that uh
instead of using the numbers with
negative one for noise and zero through
whatever for all the other clusters
we're actually labeling them using the
labels we created earlier
and then i just added a title and got
rid of some of the grid lines just to
make it
beautiful and interpretable so let's go
ahead and run this
cool so you can see now that our cluster
assignments are a little more
easily uh labeled we can see that the
red points
are noise so they're not actually in a
cluster and then we see cluster one and
cluster two represented in green
and blue and then i just added a title
to tell you what the
uh epsilon in the men's samples was for
future reference
so we can see that db scan does actually
a pretty good job of picking up those
elliptical clusters that i mentioned
before
but it also does a good job of
separating out some points that are
pretty noisy maybe they're not truly a
part of one of the clusters
and again that's one of the benefits of
db scan we get this
noise classification that tells us when
there are points that aren't
actually part of a cluster now as with
any of the other clustering methods that
we've looked at
we may want to assess the performance of
our clusters or the fit of our clusters
using a silhouette score
there are actually different ways to
assess this and maybe some better ways
to assess
cluster membership and db scan however i
don't want to
go into that here so we'll still use a
silhouette score
so one thing we can think about is
remember with db scan we do have noise
points which aren't a part of a cluster
and then points that are part of a
cluster and so we could
calculate the silhouette score really in
two ways one
would be it to calculate it using only
the data points that are part of a
and one would be to calculate it using
all of the data points where noise
points are calculated as their own
so first i'll show you how to calculate
the silhouette score when we only look
at data points that are not
noise so to do this we're going to say
d1
clustered and we're going to use our loc
function we're going to say d1.lock
square brackets and then we're going to
use a boolean vector as before
to grab only the rows whose
classification is not
noise and remember that the db scan
algorithm says that noise points are
negative one
and all of the other clusters are like
zero one two three et cetera
so we can do that by just saying the
assignment is greater than
zero because if their cluster membership
is greater than zero it means it's not a
noise point
so once we have that we can use the
silhouette
score it's always very difficult to
spell and we can say
d1 clustered and then only grab the
columns that we want which are
x and
y then we need to tell
the silhouette score what the actual
assignments are so we'll say d1
clustered uh
clustered and assignments because that's
the column where our actual cluster
assignments are stored
so let's go ahead and run that cool we
can see our silhouette score is not bad
at all it's about 0.6169
so about 0.62 which definitely means our
clusters are pretty cohesive
and separate at least when we're
considering only the data points that
are clustered and are not noise however
technically we could also calculate the
silhouette score including the noise
clusters its own cluster
this makes a little bit less sense in db
scan because the noise data points
aren't
really a cluster they're just all of the
data points that didn't fit anywhere
but technically you can do it and
sometimes you'll see people do this so i
thought i would include
it so we would say silhouette score and
say d1
bracket x y
and then d1 bracket assignments
and so this will show you the silhouette
score when we include the noise data
points as their own cluster
but you can see that that reduces the
silhouette score which makes sense
because the noise points are by
definition
not dense not cohesive and they're kind
of spread all over the place
so now that you've seen me code through
it all of this code will be on github
but i'm going to go a little bit faster
so let's go ahead and look at this new
data set data set 2.
interesting okay here we have three
ish clusters maybe or maybe just a blob
of points
my brain is trying to tell me that
there's three clusters like one two
three but they're definitely not that
dense and they're a little bit
overlapping so
let's see how db scan handles this as
usual we are going to fit
our model we are going to specify the
epsilon and the min samples we're going
to create
those labels for the clusters and then
plot that out
so i'm going to go ahead and run this
and you can see oh
actually dbscan did a really good job it
recognizes that there's a bunch of noise
points between the clusters that don't
necessarily fit well with one cluster
but it actually did recognize those
three clusters i pointed out before
so in my opinion it did pretty well but
we probably want to check the silhouette
scores so let's go ahead and do that
cool so the silhouette score if we don't
consider the noise cluster is about 0.61
similar to before and about 0.48 if you
do consider the noise cluster separately
okay let's go ahead and try this on
another data set
uh so i've loaded in my data i've z
scored it and i've plotted it and
you can tell based on how these clusters
look
that pretty much any clustering
algorithm is going to do a really great
job at
clustering these because they're very
dense very cohesive
very separate clusters so probably any
algorithm is going to do well but
let's see how db scheme does so again
we're going to go ahead and fit our
model
setting eps and min samples and then
let's go ahead and plot it
beautiful so you can see exactly as we
expected
our dvc and algorithm like probably any
algorithm
did excellently classifying these three
clusters as separate distinct
groups when we pull our silhouette
scores we can see that they are
beautiful and they're exactly the same
both with and without noise because
there actually was no noise in this case
and our silhouette score is darn near
perfect which
is as expected based on what we saw in
the graph alright let's load
yet another data set so running it we're
going to z-score and plot
this looks like a weird set of clusters
and
probably something that in the past
k-means and gaussian mixture models
wouldn't have done a good job at so
let's see how db scan does
again we are going to set our x and our
min samples
and then we are going to plot the
results of our dbscan clustering
so i'm going to go ahead and run that
very cool so here you can see
that the clusters are actually exactly
what we would assume them to be based as
being on a human looking at the graph oh
and i mistakenly labeled that as noise
let's change that really quick
but basically what you can see is that
db scan
excelled here because it was able to
find these weirdly shaped clusters
something that k-means and gaussian
mixtures so far at least
wouldn't have been able to do
so i just went ahead and re-ran that
because it accidentally was labeling one
cluster as noise
but this is actually the cluster
assignment it has the center cluster and
then this outer ring cluster
and db scan has absolutely no problem
detecting these clusters
but let's see how some of the other
clustering methods we have learned
might do on this exact same data set
so in this section i have fit an
agglomerative a hierarchical
agglomerative clustering a k-means model
and a gaussian mixture model
and i'm going to go ahead and plot out
the cluster assignments for these three
algorithms
so you can see that higher algo
hierarchical excuse me
clustering doesn't do a great job it
basically partitions the circle in half
and includes that middle cluster in the
right-hand cluster
k-means just something similar but just
in a different
slightly different at least split and
similarly gaussian mixtures does
something very weird and actually these
results may be slightly different every
time you run the algorithms
but what is clear is that these types of
models
weren't really able to do the same thing
that dbscan was
which is just to distinguish the fact
that there is a cluster in the center
and then this weird outline outlying
of the ring around that circle dbscan
had no problem picking up on that
whereas gaussian mixtures as well as
k-means especially struggled here
alright let's look at one more data set
we're going to load and plot
now this data set looks pretty separate
and cohesive however there are slightly
different numbers of data points and
different densities so let's see how db
skin does
again we are going to create our model
setting
epson men's samples fit it and then plot
it
beautiful okay so you can see that
dbskin actually did a pretty good job
it detected a couple of noise points but
for the most part
clustered these clusters as we as humans
looking at the graph
might do so and let's go ahead and print
out the silhouette scores to get a
measurement of how well this did
beautiful so without the noise points we
have about 0.7 and with the noise points
we have about six
seven which basically makes sense given
that there aren't a ton of noise points
and these clusters are
pretty cohesive and separate all right
moving on from doing this a million
times on different data sets
let's talk a little bit about how to
implement the elbow method
again if you are a math nerd and want to
read more about it i've linked a paper
on the elbow method here
but we're just going to go over how to
implement it in python
so let's load our new data set
okay so we have this data set maybe
looks a little bit familiar from the
lecture
and we are going to use the elbow method
in order to determine
what our neighborhood our epsilon should
actually
be for db skin
so i've decided that i'm going to use
the min points value of
3 based on looking at this graph and
seeing how dense it is
so i'm going to say men's equals 3
and we're going to use this nearest
neighbor function that comes from sk
learn in order to figure out
how far our third nearest neighbor
is for each data point in order to make
that k dist
elbow graph we saw in the previous
lecture so i'm actually going to say nn
nearest neighbors equals nearest
neighbors and i'm going to give it
a min plot or excuse me mins there we go
plus one now basically this argument to
nearest neighbors is telling you how
many neighbors
to calculate the distance for and the
reason i ask for mins
plus one is because it actually
calculates the distance
between the data point and itself and
counts that in its nearest neighbors
but we don't want to count the data
point we want its third nearest neighbor
so we have to get the distance for it to
itself
plus the other nearest neighbors which
is why we say mins plus one so whatever
the minimum points you're using add one
and use that as the argument for nearest
neighbors
so then we are going to say nn.fit
on our data so we'll say d6 oops not df
d6 bracket x
y and this is going to do the actual
computation
of calculating the nearest neighbors
once we fit it we can actually
ask the function for the neighbors and
the distances
and we're going to do that using this
line distances
equals or excuse me actually distance is
nay
birth so we want both the distances and
the neighbors uh equals
n n dot k neighbors
can't see that uh and then we're gonna
say d6
bracket oops
there we go x y
so let's go ahead and print out the
distances just so you can see what this
looks like
okay so as you can see we have the
distances to the
four nearest data points but remember
that our data point is included in that
and so we have a zero for all of those
and then the distances between our data
point and its three
nearest neighbors okay so now that we
have the distances we're gonna take the
only the distances to the third nearest
neighbor so those are all just like this
column
and we're gonna sort them so it goes
from smallest distance to your third
neighbor
to largest and we're gonna do that by
saying distances
equals np dot sort and then we're to say
distance oops distance says there we go
and we only want to grab that last
column so we're going to say give us all
the rows and only give us
the min's column and then we're just
going to say axis
equals 0 just to say that we're sorting
the rows
and then this will give us the sorted
list of all of our disks so let's go
ahead and run that
beautiful so now our distances are
sorted and we can plot them
in order to plot them using ggplot i'm
going to put all of those distances in a
dictionary
so i'm going to say excuse me a data
frame dis
distance says oops there we go
tf equals pd dot data
frame and inside that is going to go a
dictionary and i'm going to say one
column is called distances
and i'm going to put that as the
distances that we just sorted
and then the other value we'll call
index
and it's just going to be like the order
of all of the data points so like from
the smallest distance up to the highest
distance
and so we're just going to say that's
list range
there we go 0 to len
not lang len distances
so once we go ahead and run that we can
see that we have a data frame with all
of our data and let's go ahead and plot
all of this code is available on github
but i am going to just copy and paste it
because
it is trying to make a very pretty graph
and i don't want to spend the time
typing all of that
so i'll be back in one second with that
all right now that we have the plot and
all of this is like not necessary this
is just to make the
uh plot dark themed because this is the
plot that i used in your lecture
but let's go ahead and take those
distances and look at the k
discs graph to find the elbow to see
what the epsilon should be
so we're going to go ahead and run that
beautiful okay
so we have this great graph of our k
dists
to our basically in this case our third
nearest neighbor
and we're trying to find an inflection
point i would say that the elbow or the
inflection point is probably around
here so i'm going to choose my epsilon
to be about
0.45 it's not an exact science you could
calculate this more accurately but for
our purposes i think 0.45
is as close to this elbow as we really
need to be
so you can see that as we did before
we've set up our db scan but now we're
using the epsilon
based on that elbow method that we just
created
and then plot the clusters
so you can see that this did an
interesting job of clustering everything
it for sure recognized that very dense
cluster up on the right hand side
and it actually recognized another
cluster in the center of this very
spread out diffuse cluster
and then a ton of noise points around
that maybe this isn't a really bad
clustering of it but we can also
print out the silhouette scores to see
whether these clusters are cohesive and
separate
so i'm going to go ahead and print out
the silhouette scores both with and
without the noise
and you can see that when we don't
consider any of the noise points the
clusters are actually pretty cohesive
and separate
but with the noise points there is a lot
more variation which makes sense
so just to review in sk learn you can
use the db scan algorithm by setting
the epsilon and the min samples and
fitting our model
if you ever need to choose your epsilon
based on the data we can make this k
dist graph
find the elbow and use that as our
epsilon
instead of having to choose it just
based on our expertise based on the data
all right hopefully that helped i will
see you guys next time

 
hello and welcome to your regularization
lecture
before we jump into regularization let's
quickly review loss functions remember
that loss functions are metrics that
help us measure the performance of our
model where lower is better
we use loss functions in order to
optimize or fit our models we want our
model to have the lowest possible value
for the loss function so we minimize it
for example when we talked about linear
regression we talked about some metrics
like a mean squared error mean absolute
error or the sum of squared errors that
allows us to gauge how well our model is
doing where zero would mean that our
model is not making any errors at all
when we fit our model we choose
parameters say like a coefficient value
in the linear regression that minimize
this value but we've seen over and over
with various models that our models have
the potential to over fit
meaning that the models are too complex
and are fitting too tightly to the
training data and don't generalize well
to unseen data we talked a little bit
about the bias variance trade-off where
we increase the bias of our model in the
hopes of decreasing the variant
overfitting has pretty much been the
bane of our existence and we want to
have tools in order to prevent it or at
least mitigate it regularization is one
of those tools in general regularization
refers to adding some sort of penalty to
our loss function in order to reduce the
complexity of our model thus reducing
the chance of overfitting we're going to
talk about two very common specific
types of regularization bridge and lasso
the left hand side of this equation
should look very familiar because it's
just what our loss function was before
we added any and here we're going to use
the sum of squared errors but really we
could use any loss function that we want
the important part that we recognize is
that the left hand side is still just a
regular typical loss function what's new
is this plus lambda and then a penalty
for ridge regression the penalty is
related to the sum of the squared
coefficients meaning that bigger
coefficients and when you square them
will have a larger penalty on this
overall loss function in essence this
penalizes the model for having
unnecessarily large coefficients you can
see from this formula that we're kind of
having a balance or a trade-off if all
of our coefficients were zero then our
model wouldn't really be doing anything
but a coefficient essentially has to
pull its weight in order for the size of
its coefficient in the penalty to be
worth its reduction in the sum of
squared errors in the left-hand part so
if we have a variable that is really
good at predicting our outcome it's
going to reduce the first part of our
loss function and so even though its
coefficient might be big it will still
be worth it ridge regression penalizes
large and magnitude coefficients this
means coefficients that are both large
positive or large negative values
essentially ridge is going to pull all
of our coefficient values closer to zero
and coefficients essentially have to
justify their existence by decreasing
the regular loss function in order to
make up for the penalty that that
coefficient causes the lambda term is
basically the harshness of the penalty
if lambda were zero you can see that the
right hand side of our equation would
just cancel out it would basically be
like not having a penalty at all but
when lambda is really large we are very
harshly penalizing the coefficients in
our model you can imagine that as lambda
goes to infinity essentially all of our
coefficients are going to be zero now of
course in real life we're going to
choose a lambda that's somewhere in
between and we'll talk more about how to
do that in the python part of the
for now let's reiterate what we just
learned
the left hand side of this is very
familiar to us it's just a typical loss
function in this case the sum of squared
errors on the right hand part is what's
new
it's a penalty for the size of the
coefficients remember the goal of adding
a penalty to our loss function is to
reduce the complexity of our model and
thus hopefully reduce overfitting here
in ridge we do that by penalizing the
sum of the squared coefficients in our
model
thus larger coefficients larger penalty
next let's talk about a different type
of regularization called lasso now
similar to ridge we can see that the
left hand part of this is very familiar
it's just our regular loss function that
measures how well our model is doing the
right hand side is again a penalty but
this time instead of squaring the
coefficients we're taking the absolute
value of the coefficients now similar to
squaring absolute value takes positive
numbers and negative numbers and makes
them positive and so we're still
penalizing both positive and negative
coefficients if they're sufficiently
large or sufficiently far away from zero
like ridge lasso takes our coefficients
both big positive and big negative and
moves them closer to zero it does so by
penalizing large coefficients like with
ridge coefficients essentially have to
pull their weight and reduce the left
hand side the actual error in order to
be worth the size of their coefficient
in the penalty
also like with ridge the lambda is the
harshness if lambda were zero we would
essentially have an unpenalized
regression like linear regression
if lambda goes to infinity all of our
coefficients are going to end up being
zero so you might be wondering why would
we have these two separate things
they're pretty much the same thing
except for ridge uses the squared
coefficients and lasso uses the absolute
value both of them penalize our loss
function and take big coefficients both
positive and negative and draw them
closer to zero but it turns out that the
small difference between squaring your
coefficients and taking the absolute
value does end up affecting the way that
the models are penalized and before i
move on i just want to say these are
very simplified formulas that we're
using here if you ever look up the
formulas that computers use they might
be a little more complicated than this
but they still have the same ideas ridge
penalizes the sum of the squared
coefficients and lasso penalizes the
absolute value all right so what's the
difference between ridge and lasso well
i'm going to tell you and then i'm going
to tell you why lasso does something
called variable selection
while ridge and lasso both take large
and magnitude coefficients and draw them
closer to zero
lasso is way more likely to drag those
coefficients all the way to exactly zero
whereas ridge will bring them near zero
but usually not exactly zero when you
make a coefficient zero it's the same
thing as removing that variable from the
model thus lasso does variable selection
it basically removes variables from the
model for you
this can be really useful to you both in
a prediction context where you want to
know which variables you don't need to
collect or use anymore as well as in an
inferential context in which you want to
know which variables are the most
important so again lasso is more likely
to take our coefficients and drag them
to exactly zero thus removing them and
ridge while it does the same action is
way less likely to drag coefficients to
exactly zero they might be near zero but
they're far less likely to be exactly
zero thus ridge keeps all of our
predictors in the model
all right so here's another way to think
about ridge and lasso we talked about
ridge and lasso as being penalties on a
loss function but we can also think of
ridge and lasso as a budget that we set
for ourselves for our coefficients
in these two graphs you can see that we
have two coefficients beta 1 on the
x-axis and beta 2 on the y-axis
on the left-hand side we have a graph
representing lasso and on the right one
for ridge
the blue areas on each graph represent
the budget that we have basically we can
choose any values for our parameters
that are inside these blue areas the
little diamond for lasso and the circle
on both graphs the black dot represents
what the coefficients would be if we
didn't have any penalty or budget and
the red ellipses are basically contours
representing worse and worse models so
every point in this contour has the same
amount of error every point in this
contour has the same amount of air etc
and as we get further from that black
dot we have more and more training air
so we have to stick to our budget but
we'd like to have as little error as
possible so in the graph the solution
that we choose is represented by the
point at which these contours these
ellipses first intersect with our budget
you can see for lasso that happens at
one of the points so in this case b1
would be set exactly to zero or removed
from the model
in the right hand graph for ridge you
can see that the point at which our
ellipse
intersects with our budget
is not where b1 is exactly zero so even
though b1 is reduced a lot it is not
drawn to exactly zero and is therefore
not removed from the model to review we
can think of ridge and lasso as a budget
that we set on our coefficient sizes we
want to choose a solution that's within
our budget but is as close as possible
to the actual solution leading to as
little error as possible at least within
our budget for lasso this tends to
happen when one or more variables is
exactly zero whereas for ridge this
tends to happen when the variables are
small but not exactly zero and for those
of you who are math nerds one
interesting thing that you might have
noticed is that when we use the absolute
value in the penalty for our loss
function it makes our loss function no
longer differentiable and so we can't
use typical gradient methods to optimize
our model well one way that we can do
that is by looking at this as a
constraint problem so we basically look
at the area of our budget and we
optimize within that area so we
constrain the area that we search for
instead of having to deal with a
non-differentiable function all right
that's all i have for you i will see you
next time

 
hello and welcome to your hierarchical
agglomerative clustering video as the
name suggests hierarchical clustering
creates clusters that have a
hierarchical relationship with one
another in other words clusters are made
up of subclusters which are themselves
made up of subclusters one hierarchical
relationship you might be familiar with
is blood blood is made out of red blood
cells white blood cells plasma and
platelets however within those
categories there are subcategories for
instance in white blood cells we have
basophils lymphocytes neutrophils etc
and within lymphocytes we have b cells t
cells natural killer cells and within t
cells we have helper t cells suppressor
t cells memory t cells killer t cells so
you can see that each of our groups were
actually made up of multiple subgroups
and when we look at all of the other
categories you can see this is true
there as well hierarchical clustering
models clusters like this with a
hierarchical relationship the type of
hierarchical clustering that we're going
to be doing is hierarchical
agglomerative clustering the
agglomerative and hierarchical
agglomerative clustering refers to how
our clusters are created in
agglomerative clustering every data
point starts off as its own singleton
cluster and then at each step we
selectively merge clusters together that
are the closest and we merge and merge
and merge successively until every data
point is in one cluster this creates
that hierarchical relationship because
of this hierarchical clustering is often
referred to as bottom up clustering
because we start at the bottom and we
move all the way to the top where every
data point is in a single cluster
because of this agglomerative clustering
is often referred to as bottom up
clustering because every data point
starts off as its own cluster and then
we selectively merge them together until
they're all in one cluster so we start
at the bottom and move all the way to
the top to do all this merging we need
to think about two things the first
thing is a distance metric a distance
metric measures how far apart two
individual data points are you may be
familiar with things like euclidean
distance which measures the distance
between two points like this
however there are other metrics that we
can use for instance manhattan distance
manhattan distance measures the distance
between two points as if you're in
manhattan walking from one point to
another so instead of euclidean distance
which measures it like this manhattan
distance would measure it more like this
we can also think about hamming distance
which measures the difference between
categories or cosine distance which is
used a lot in topic modeling to see how
similar two vectors of word counts are
cosine distance measures the difference
between two vectors as the difference
between their angles which means that
for instance in topic modeling if we
have two documents on the same topic but
one is double the length cosine distance
will still measure those two things as
very close to each other even though one
of the documents is longer and therefore
has higher word counts the next thing we
have to think about is a linkage
criteria well distance metrics measure
the distance between two individual
points linkage criteria help us define
how close two clusters are when the
clusters contain more than one data
point i'm going to go over a couple of
the main types of linkage criteria
although there are a ton of them out
there the first one we'll talk about is
single linkage
single linkage defines the distance
between two clusters as the minimum
distance between their two points in
other words we take a look at every data
point in cluster a and every data point
in cluster b and we define the distance
between the two clusters as the minimum
distance between a pair of points with
one point in a and one point in b
similarly but on the opposite spectrum
we have complete linkage complete
linkage defines the distance between two
clusters as the maximum distance between
those clusters in other words we look at
every data point in cluster a every data
point in cluster b and look at the
maximum distance between a pair of
points where one point is in a and one
point is in b next average linkage looks
at the average distance between data
points in cluster a and cluster b
unlike single and complete linkage
average linkage takes into account every
single data point including extreme ones
next we have centroid linkage as the
name implies centroid linkage looks at
the centroids or centers of each cluster
which is just the middle of the cluster
defined by the point that is at the
average of all of the different
predictors or features in that cluster
centroid linkage defines the distance
between two clusters as the distance
between their two centroids last let's
talk about words linkage words is often
the default in a lot of software
packages when doing hierarchical
clustering and is for instance the
default in sk learn words defines the
distance between two clusters as how
much the within cluster variance
increases when you combine those two
clusters remember within cluster
variance measures on average how far
apart data points are from the center of
a cluster if we merge two clusters that
are very similar the within cluster
variance isn't going to go up a lot but
if we merge two clusters that are very
different the within cluster variants
will go up a lot so wards defines the
distance between two clusters is how
much that within cluster variance goes
up when combining those two clusters and
therefore words will put two clusters
together when combining them only
increases the within cluster variance a
little bit we use our choice of distance
metric and linkage criteria in order to
do that successive merging from every
data point in its own cluster to every
data point in a single cluster when
doing this we basically create a
dendrogram a dendrogram is a graph that
represents those successive merges and
the hierarchical relationship between
the clusters in our data you can see at
the bottom of the dendrogram every data
point has its own line
and by the way on the x-axis the order
of things don't really matter it's just
chosen in order to make the dendrogram
look pretty what we care about is the
y-axis distances the y-axis represents
the similarity or the distance between
clusters when two clusters are joined
for instance these two right here
the vertical distance on the y-axis
tells us how similar or dissimilar these
clusters were when they were merged for
instance here we have a very short
y-axis distance indicating that these
two points when they were put into a
cluster together were very similar
however when we look at the top of our
dendrogram we can see that when we
clustered these last two clusters
together they were very separate we can
see that by the long vertical distance
on the y-axis in the previous lectures
we talked about two concepts cohesion
and separation that helped us measure
how well our clusters were doing if you
think about what cohesion means it means
that data points are close to other data
points in their own cluster
we can assess cohesion using a
dendrogram if we have cohesive clusters
that means that the data points within
that cluster are very close together
which means that when we're doing our
first few merges data points will be
very close together and therefore have
very short distances on the y-axis
for instance you can see in this plot
that that's the case when we look at the
bottom of the dendrogram we see a high
density of short distances meaning that
data points are very close to their
neighbors and therefore our clusters are
more cohesive on the other hand we also
have separation
remember that separation means that
clusters are very different from other
clusters
to look for this in a dendrogram we're
going to look at the top of a dendrogram
if we have distinct separate clusters in
our data set we are going to see very
tall distances on the y-axis at the top
of our dendrogram because if we say have
two or three clusters that are very
separate when we finally combine them
into one cluster at the very top of the
dendrogram we're going to have very tall
distances because these two clusters are
very separate or dissimilar here we can
see that those tall vertical distances
are indicating that this cluster over
here and this cluster over here were
very different and therefore they're
very separate again we look for cohesion
by looking for a density of short y-axis
distances at the bottom of our
dendrogram and we look for separation at
the top of our dendrogram by looking for
very tall distances our ideal dendrogram
would have both cohesion and separation
so we're looking for those tall
distances at the top and a high density
of short distances at the bottom let's
take a look at another dendrogram this
dendrogram seems to indicate that there
is not a lot of cohesion in these
clusters you can see there's a lot of
longer distances towards the bottom and
even the middle of our dendrogram
indicating that data points aren't very
close to their neighbors and therefore
our clusters are not very cohesive
there's also not a ton of high
separation when we look at the top of
our dendrogram we see some taller y-axis
distances but they're pretty short
relative to the rest of the dendrogram
indicating that our clusters at the end
were not very separate from each other
to review hierarchical clustering is a
great really flexible algorithm one of
the things that's great about
hierarchical clustering is that it
doesn't assume anything about the shapes
of the clusters it's also very flexible
with the choice of distance metric and
linkage criteria we're really able to
have some flexibility in how we want our
clusters to be formed also if you want
to model the hierarchical nature of your
clusters hierarchical clustering is at
least so far the only clustering
algorithm we've learned that's able to
do that so if you have or want a
hierarchical structure you got to use
hierarchical clustering on the negative
side hierarchical clustering can be very
slow with a time complexity of o n cubed
meaning that if you have a large data
set with a lot of features hierarchical
clustering might not be feasible
another negative is that we can't
un-merge data points that have been
merged together say for example that two
data points like these get merged
together early on but when we look at
the big picture of all of the data
points we actually think they should
belong to separate clusters but because
of the way hierarchical clustering works
we can't unmerge them even if later on
we think they should be in separate
clusters all right let's talk about some
applications of hierarchical clustering
the first application we'll talk about
is my master's thesis in my master's
thesis i used hierarchical clustering in
order to cluster people based on their
behavior in a task called the
meta-memory task in the meta-memory task
people are shown words and then are
asked to bet a certain number of points
from 0-10 based on how likely it is they
think that they'll be able to remember
the word later on in a round of this
task we ask them to do this 12 times we
show them a word we ask them to bet word
bet word bet 12 times and then we ask
them to recall as many words as they can
we then repeated the round five times so
that a person did a total of 60 words
one of the questions that i wanted to
answer was whether we could extract
patterns based on how people bet
throughout the 60 words in the task this
radar graph shows those patterns from
the clusters we extracted for instance
when looking at three clusters we saw
something interesting if you see the red
cluster you can see that as people go
through the tasks starting at trial 1
and going all the way to 60 some people
had this weird cyclical behavior where
they would bet really high at the
beginning of each round and then
steadily drop off until the next time
they begin around the other two clusters
blue and green basically are just more
extreme versions of each other with
people in green betting really high on
average and people in the blue cluster
betting really low on average another
cool application was that someone used
hierarchical clustering in order to
cluster people based on their movie
reviews again if you want to check any
of these out i've linked them under
extras and lecture links next what i
think is a really cool application is
hierarchical clustering was used to
classify the behavior of people in
images and videos for instance you can
see here just from this graphic that
this makes a lot of sense for instance
look at the left hand side of this
picture where someone is playing a
guitar versus playing a violin you can
imagine that there's an overarching
cluster for playing an instrument and
then within that you have the plain
guitar cluster and the playing the
violin cluster organized hierarchically
and last but not least this paper used
hierarchical clustering to measure
across time and distance crime hot spots
all right that's all i have for you i
will see you next time

 
hello and welcome to your neural
networks
in python lecture so um
today we're actually just going to build
a very simple
linear regression model in uh keras
which is a package we haven't used yet
and i am going to just show you how to
build
very simple neural networks so the first
thing i need to do is obviously load all
my packages
and then load my data and today we're
going to work with as usual
the spotify data so we have for
different artists
all of these different variables about
their different songs
um so the first thing i'm going to do
just to clean up my data is make sure
that i don't have any missing values and
i'm going to do that by using the drop n
a function
and then we will get to our actual model
so because i want to build a very simple
model
i'm going to predict fail valence based
on danceability
energy loudness and acousticness so i'm
just preparing those variables here
storing the predictors in x and the
predicted value
and y so now we get to the actual neural
network part
and keep in mind this is an incredibly
simple neural network
if you're ever building neural networks
in real life you're probably going to
have
a little bit more of a complicated
structure but i want to get you
comfortable with just the general
rules of making a neural network so in
keras we are going to build a model and
we're going to say model equals
and we're going to use kb and just
remember i imported that as kb
so that's why it's called that so we're
going to say kb
dot sequential and sequential is the way
that you're telling
keras you're telling python that you
want the type of neural network that
we've seen before
where each layer feeds directly into the
next layer
there are other types but we won't
really go over them here
so then we're going to give it a list of
layers to have in our neural network
and with a very simple linear regression
model we're going to have two layers
we're going to have an
input layer and we're going to have an
output layer so let's build
our input layer so we're going to say kb
dot layers dot dense and a dense layer
just means that every node in that layer
is going to be connected to the layer
next to it and that's what we want in
this case
and so then you're going to say what is
the number of nodes in this layer well
we have four
input variables so we're going to have
four nodes
and then with the first layer that you
have you have to give it this argument
input dot shape and then tell it like
what is the shape of the data that we
expect
in this case we expect data that has
four columns so we're going to tell it
four and actually should put an equal
sign because it's an argument
okay so now we've built the first layer
the input layer of our neural network
all we have to do now is add our output
layer
so we're going to say kb dot layers oops
layers dot dense
and we are going to tell it the shape
and we only have one node
in the output layer because all we're
doing is predicting one
value so we're just gonna put a one here
and because this isn't the first layer
that we're using we actually don't have
to tell it the input shape we just have
to tell it one which is the number of
nodes in this layer
so now that we're all done we've
actually created our model and we're
pretty much good to go
now we have the structure of our model
the next thing that we have to set
is how are we training it and the way we
tell
keras or python is we say model dot
compile
and then there's a lot of different ways
that you can customize the way that your
model is trained
but for our purposes there's two things
that you want to think about
one is the loss function and the other
one is the type of optimizer you want to
use
and um just for ease sake i'm just going
to say
for the types of models we're using
you're going to want to use kb
dot optimizers dot sgd
which just stands for stochastic radiant
descent which if you take a further
neural networks class you'll learn a
little bit about
but let's focus on this loss function so
typically when we're doing
linear regression we minimize the sum of
squared errors
but we can also minimize the mean
squared error as i mentioned in the
other lecture which is just
the sum of squared errors divided by the
number of data points
so that's what i'm going to do here i'm
going to say mean squared
error and that's it that's i've told
them
so now i've told the model one what is
the structure
and two how should i train the model
and the last thing that we do actually
should look very
similar to what you've done in sk learn
is we need to actually fit the model so
we've told it how and now we actually
have to do it
the way we do that is model dot fit so
very familiar
x comma y and then we have to give it an
argument
of epochs and you can give this any
number for really simple models it
actually
turns out not to matter as much i'm
going to give it 100 for the sake of
time
but this is essentially the number of
times it's going to run through the data
and train
typically in neural networks the longer
you train it the higher the accuracy
so you often want to train your model
for more than 100 epochs but
for the case of a very simple linear
regression model and because i'm
recording this and don't want it to take
an hour
i'm just going to do a hundred so when
we run this
you can see that it's going to tell us
that each step
what uh well one what step it's on and
then two
the current value of the loss function
so it starts out at 0.1547
and you can see that it's continually
going down and remember our goal
is to minimize the loss function so if
it's going down
that's pretty good and you can see as i
scroll down
it does this a hundred times until it
finally gets to the final step at which
point it says
okay we're done training and it gives us
our model
then the next thing that we do well
actually we don't have to do anything we
now have a fully functioning
neural network and essentially what this
neural network is doing because of the
way we structured it is linear
regression
but we might want to use this model to
predict values or to test how well this
and in the same way that we did with sk
learn
we can use this model to use the dot
predict function in order to return
those predicted values
so i'm going to say y pred
equals model dot
predict which hopefully is a little bit
familiar to you
i'm going to give it xn and just by the
way the reason that i had to do this is
because
the neural network is expecting a numpy
array
not a pandas data frame which is why i
had to change it just fyi
so i'm going to give it the x values i'm
going to have it the model predict those
and then i'm going to put this in a data
frame so that i can graph them
so i can see my true versus predicted
values
so let's go ahead and run that oh didn't
like that hold on
oh that's right okay sorry so when you
predict it you have to use this dot
flatten
argument actually here let me show you
why it didn't like that really quickly
before i fix it
so when i ran this let's print it out
you can see that it returned the
predicted model as a list of lists
or actually technically it's an array of
arrays and
we just want one single list and to get
one single list from a list of lists
we're going to use the dot flatten
argument or functions you see and that
hopefully that will run now
yes it does okay so then the last thing
i'm going to do is i'm just going to
show you in the same way that we've done
in other lectures
how to make a graph oops
true pred um of the true versus the
predicted values so i'm going to say
yes x equals
true y equals pred
and then plus g on point
so when i run this you'll get a scatter
plot of the true versus the predicted
values and you can
from there hopefully assess a little bit
about how your model is doing
okay i know that was very basic but
hopefully that makes a little bit of
sense the main things that i want to
emphasize
is when you're building a neural network
you have to tell it the structure
you have to tell it the specifics of how
to train the model and then you have to
actually train the model in order to
create a neural network
okay i will see you next time

 
hello and welcome to your k-nearest
neighbors lecture in python the first
thing as usual that we will do is load
all of our imports
once that's done let's load our data
we're going to be working with some
diabetes data where the outcome variable
indicates whether or not someone had
diabetes with one being yes and 0 no we
have a bunch of demographic variables
that can help us predict whether or not
someone had diabetes
so of course let's build a k n model to
do that the predictors we're going to
use in this case are going to be bmi and
blood pressure
so let's set our x
data
predictors
in our y
outcome
all right now that we have our x and y
let's do a train test split so i'm going
to say x train x test y train
y test
train test
split
x y
and test
size equals 0.2 to do an 80 20 train
test split
now that we have our training test set
we can safely z-score so let's go ahead
and do that
standard
standard there we go scalar
and we'll say
z x train
equals z dot fit
form x train
and then we'll copy and paste and do the
same thing for the test set
so x test
x test
and of course we do not want to refit on
our test set next let's build our empty
model i'm going to call it knn and set
it equal to k
neighbors classifier
and i'm going to say
that the number of neighbors or k is
going to be in
neighbors equals 5. now we can go ahead
and fit so we'll say knn.fit x train
y
train
and let's go ahead and run beautiful so
now we have a fitted k n model and since
i only use two predictors we can
actually really easily visualize the
decision boundary between having
diabetes and not having diabetes via
graph
so let's go ahead and do that i've built
this function here and i don't expect
you to be able to rebuild this or even
understand all of it but basically what
it does is it takes in your x data your
y data and the n for number of neighbors
and it creates both a model as well as a
plot outputting the decision boundary
i'll show you what i mean here
so i'm going to run this function with
my x train and my y train data and i'm
going to set k to equal 5.
let's go ahead and run this and you can
see that along with our model it
outputted this graph this graph
represents the decision boundary between
our data you can see on this graph that
the solid points are the actual data
points that we have
in the background the lighter colors
represent which category a data point
would be if it were at that spot so for
instance anything with this red
background color would be classified as
red if we had a new data point there and
anything in this blue area would be
classified as blue or having diabetes
this can be a really cool way to explore
how k affects your decision boundary for
instance let's rerun this with k equals
one you can see here that our decision
boundary got a lot more jagged and
there's a lot more islands of red and
blue within each of the other's
territory similarly increasing k by a
lot say 15 will give us a bit more of a
smoother boundary
now just a little caveat you'll get to
play around with this function and these
graphs in the class work a lot more but
if you're ever trying to use this on
your own this only works when you have
two predictors otherwise it gets a bit
too difficult to visualize so just keep
that in mind you wouldn't really be able
to use it in any of your analyses unless
you only had two predictors all right
now that we've gotten to explore the
decision boundary a little bit let's go
back to our original model that we fit
with k equals five and let's see how
well it did in terms of accuracy
i'm going to pull both the train and the
test accuracy so we'll say print
train accuracy
and then we'll use accuracy
score
y train
k n dot predict
x train
and then do the same but for the test
set
so our model did okay definitely better
than just random our training accuracy
is 0.757
and our testing accuracy is about 0.64
so i am a little suspicious that there
may be some overfitting going on here
because there is a very noticeable
difference between the training and the
testing performance let's further
explore by using our confusion matrices
so say plot
confusion
matrix
and that'll take our model k n x train y
and then of course we will repeat with
the test set
x test y
test
all right so let's go ahead and run
these and then we can compare
again when we're looking at confusion
matrices we want to look for weird
patterns of course we know the main
diagonal are the guesses that we made
correctly and the off diagonal are
errors here i'm not seeing anything too
concerning i don't have rows or columns
that are all zeros and i actually don't
have that much of an imbalance of errors
either we can also grab any of our
metrics i'm here going to plot our roc
curves that will also give us the roc
auc values so we're going to say plot
roc
curve
k n
x train y
and then we will copy and paste
and do it for the test set
beautiful so you can see that our roc
curves are not bad they are completely
up in the top left corner but they're
substantially better than a straight
diagonal line and this training auc
would be 0.81 and this is 0.64 so again
i am noting a concerning difference
between the training and the testing
performance meaning that our model might
be overfit so hopefully you see that
building a k n model is not any
different from any of the other
classification models that we've built
so far
however there is one new thing i want to
talk about and that is hyper parameter
tuning in k n we have what's called a
hyper parameter a hyper parameter is a
value or parameter of a model that's not
fit by the model and that must be
supplied by the user for instance k in
knn
we have to tell the algorithm what k
needs to be now so far we've basically
just been using either our expertise or
trial and error to pick this
hyperparameter value but there's a
better way hyper parameter tuning this
is a more automatic way of choosing your
hyperparameter values using the data
however in order to do this we don't
just need a train and a test set we need
a third pool of data called a validation
set because basically what we're going
to do is we are going to try out
different hyper parameter values in this
case different k's for k and n
and see how well each k does then we're
going to pick whichever k had the best
performance but remember we cannot touch
our test set until it is time to
evaluate our final model that means we
can't use the test set in order to
evaluate the different values of k
so in addition to our regular train and
test set we need that validation set
which essentially acts like a pretend
test set from the training set that can
allow us to test the performance of
different hyper parameter values in
order to get this train test and
validation split we're going to start
out with something that looks really
familiar
here we're going to do a train test
split i'm going to say x train x test
y train y test equals train
x
and test size equals 0.2
so so far this should be very familiar
this is what we've done up until this
point
the next step is what's a little
different
to create our validation set because
currently we only have a training in a
testing set we're going to take our
training set and further split it into
that train validation set
to do that we're basically going to have
a second train test split but this time
instead of splitting on all the data we
are going to just split the training set
basically what this is doing is instead
of splitting on all of the data like we
did with our original train test split
we're splitting the training set into
that training set and validation set
this will allow us to have that
validation set as basically a fake test
set in order to assess the performance
of different hyper parameter values
next we have to choose a set of hyper
parameter values to look at
here i'm going to choose the possible
case from 1 all the way to 10. and
lastly i'm going to create an empty
dictionary so i can record the accuracy
for each of those values of k in order
to assess these values of k i'm going to
need to fit a model with that value of k
and see how it performs on our
validation set so let's create a for
loop i'm going to say for k in pass
k
then we're going to say create a model
say k n
equals k
neighbors
classifier
and we'll set n neighbors to be the
current value of k
then we'll go ahead and fit it k n dot
fit
x train two so this is our actual
training set
n y train two last we want to see the
out of sample performance so we will see
how our model that we just fit performs
on the validation set
to do this i'm going to put the value in
the dictionary so i'm going to say ack
square bracket k
equals
accuracy
and we could do this with any of our
metrics the actual values which would be
y val for the validation set uh comma
the predicted values which would be
knn.predict
val
once we're done we can go ahead and
print out that dictionary of accuracy so
i'll say print ack
all right let's go ahead and run this
just so you can see what's happening so
we're going to go ahead and run and you
can see we have a dictionary of
accuracies and we can manually sort
through and see which of these has the
highest accuracy in this case it's
looking like it's k equals 8 because
that has a 67 accuracy and as i scroll
through i'm not seeing anything higher
than that um so we can do this by hand
i'm going to use a little shortcut you
may have learned something similar to
this in 230. basically we are grabbing
the key associated with the highest
value in the dictionary once i have that
i'm going to print it out print chosen
okay
and then i'm going to fit our final
model on all of our training data using
that k
so we'll say k n final
classifier and
neighbors equals
chosen okay
and then we will say k n final dot fit
x train and notice this is all of our
training data
and then let's say
let's grab an accuracy so now that we
fit our model so again let's just review
when we were looking at the different
performances of the different k's
we used our smaller training set and
then assess the performance on our
unseen validation set again the
validation sets acting like kind of a
fake test set
once that's done and we've chosen the
best hyper parameter value we then refit
our final model using that value and
using our entire original training set
that's again the training set and the
validation set together
once we fit our model we can then
actually look at the test set
performance our model is completely
built so it's time to touch the test set
to do that let's look at the test set
say print
ack
and
space
accuracy score
final dot predict x test
all right so let's run it again with all
of those steps
so when we run it you can see that this
time the best k was 6 and that gave us a
test accuracy of about 60
and again we could pull any metric or
any plot that we wanted to for those
metrics for time i'm just going to use
so that is how we can do hyper parameter
tuning by hand the main ideas are that
we need now a validation set because we
need to test the performance of
different hyper parameter values but we
can't touch the test set until our model
is fully built we need basically a fake
test set in the form of the validation
set to test the different hyper
parameter values now all of that was a
lot of work but luckily python has a
function that will do a lot of it for
you
so let's do the same thing but this time
we're going to use something called grid
search
grid search is a function in sklearn
that will basically do what we just did
above for you but it has a little more
flexibility and options for instance
instead of just a trained test
validation split we can actually use
k-fold to create our validation set
luckily this all happens inside the grid
search function so you don't have to
code any of this by hand to start the
first couple of lines should look really
familiar we're setting up our x and y
data and doing an initial train test
the grid search however is going to take
care of the train validation split for
us and we'll choose the best hyper
parameter to do this the first thing we
need to do is just create a simple
typical model we're going to say k
there we go classifier
classifier there we go but we're not
going to specify the number of neighbors
so next we need a dictionary of all the
possible ks or hyper parameter values
that we want to try i'm going to call
this case and i'm going to set it equal
to a dictionary the key in the
dictionary is going to be the name of
the hyper parameter that we're trying to
tune
here that would be n
and then the value is going to be all
the values that you'd like to try here
i'm going to do range
1 to 30. so this will try k from 1 to
29. next we're going to use this grid
search function and this is going to do
so much for us so what we're going to do
is we're going to say grid
grid
cv
and we're going to give it a couple
arguments the first is going to be that
empty model that we haven't fit yet so
i'm going to say knn2
the second thing we're going to do is
give it a dictionary of the
hyperparameter values k
s
and then we're going to give it an
argument called scoring scoring just
tells our model how to decide or which
metric to use to gauge which hyper
parameter is the best
in this case i'm going to go with
next i'm going to have an argument
called cv and remember how i told you
that grid search could actually use
k-fold for your validation train split
this is just saying how many folds do
you want when doing that training and
validation split i'll do five here and
then last we're going to say refit
equals true
so this grid search function is going to
take our empty model take our possible
hyper parameter values and it is going
to decide which one is best based on the
performance from the validation set it's
then going to retrain a model using that
hyperparameter value the best one that
it found and return it to us in the form
of the grid function so grid search is
going to take that empty model and that
dictionary of possible hyper parameter
values and it's going to try all of them
out for us by creating a validation set
and seeing which hyper parameter does
the best on the validation set
once it's figured out which one was the
best it's going to train a new model
using that hyperparameter value and
return it to us luckily the syntax
should look really familiar we're just
going to say grid.fit and we are going
to fit it on our training set so we'll
say x train
and when we run this we have a fully
fitted model all of the hyper parameter
tuning happened inside the grid search
function and what's returned to us in
grid is just our fitted model with the
best hyper parameter
so let's go ahead and run it so of
course once we have our best model we
might want to know well what was the k
that we used to do that we can say the
name of our model dot best estimator
underscore dot get
params
and then we will say we want the
parameter n
and when we run this this will be able
to tell us what that k value was so here
it chose k equals 29. if you're
interested i've linked the documentation
for the grid search function so that you
can see all the other types of things
that you can pull from this fitted grid
search however now our model is just
like a regular model that we fit and we
can do things like use dot predict dot
score or assess the performance of our
model let's go ahead and do that let's
pull the train and the test accuracy so
i'm going to say print
uh train act
in accuracy score
y train there we go
k n not k n grid dot predict
and then print
act
inaccuracy score y test
grid dot predict x test all right let's
see what our accuracy values are
beautiful so you can see that we have a
training accuracy of about 68 and a
testing accuracy of about 57
so as you can see grid search tv is a
really helpful function that saves us a
lot of hand coding
however there is an issue that i want to
talk about that comes with using grid
search cv
you'll notice in the previous model i
didn't z score and that was on purpose
remember how we talked about data
leakage before
data leakage is when any information
from the test set gets leaked into our
model thus making our predictions of out
of sample or test set performance
usually a little bit optimistic because
we accidentally had information about
the test set when building the model one
of the ways this can happen is fitting
our z-score object on all of the data
including the test set when we're doing
our z-scoring so so far what we've done
is we wait until we have our train test
split and then we z-score fitting only
on the training set a similar thing
though can happen with the validation
set remember the validation set is
basically just like a pretend or fake
test set to choose our hyper parameter
values and we don't want any data
leakage from the validation set either
just like we don't want any data leakage
from the test set however notice when we
fit our grid search model we gave it the
entire training set this means we
technically gave it both the training
and the test set
because grid search of course will do
that train validation split for you
however that means that if we z-score
our training set before putting it in
this grid search function
we are actually leaking information from
our validation set into our model in
order to prevent this we need something
called pipelines and pipelines are
actually really useful in machine
learning workflows anyway
a pipeline which we're going to use
using these two new imports is basically
a kind of checklist or to-do list for
all of the different things we want to
do as a part of our model building so
let's build a model using a pipeline
first we're going to do some things that
look really familiar namely getting our
data setting our x and y and doing our
train test split then as normal we're
going to create a model i'm going to
call this k
and again we're not going to set our end
neighbors here all right so here's where
things get a little bit new instead of
just doing a typical z-scoring what
we're going to do is make something
called a column transformer and a column
transformer is basically going to be a
list of all of the columns that we want
to transform and what transformations we
want to do
to do this i'm going to create something
called z and i'm going to use the make
column
transformer function
and this is going to take in a tuple of
values the first is going to be what
transformation we want to do in this
case we want to do standard scalar
and the second argument is going to be a
list of all the columns we want to apply
this transformation to
in this case both of our predictors are
continuous so we can z-score them but
i'm going to put this here just in case
you ever have a situation where you only
want a z-score sum and not all of your
variables so we'll say
bmi comma blood pressure
and then the last argument that we need
to do is give it something called
remainder
equals pass through this basically says
if there are any columns that i didn't
transform leave them alone and pass them
through to the model
if you don't have this argument it will
actually drop those columns meaning any
column that you didn't transform will
not be in your final model all right now
we have both the steps that we want to
do we have our model and we have our z
so let's make a pipeline to put those
things together i'm going to call it
pipe and say make
pipe line
and we are just going to give it in
order the steps that we would like it to
do so in this case i would like it to
z-score and then fit my model so i'm
going to say z comma k n the rest of
this should look very similar sort of
so the first thing that i'm going to do
is i'm going to say case and i'm going
to set a dictionary of the potential
hyperparameter values
however because we have a pipeline
our piper parameter is no longer just
called k neighbors because it's tied to
the step of our pipeline that it's
associated with
so in order to figure out what that name
is i'm actually going to create our
pipeline and see what all of the names
are and look for what end neighbors is
called now to do that i'm going to use
this code which is going to print out
the different parameters of our pipeline
so let's go ahead and run
okay so i'm gonna scroll through this
and i'm gonna look for something that
looks like the end neighbors so scroll
scroll
okay so i found this which looks like n
neighbors and this means that our
parameter is actually called k neighbors
classifier underscore underscore and
underscore neighbors so i'm going to
copy and paste that because i'm going to
need to use that in our dictionary of
values for our hyper parameter so i'm
going to put that as the key and then
the values i'll use range 1 to 30 again
next same steps that we did before i'm
going to create grid grid
and i'm going to give it our model which
this time instead of our regular model
like k n we're going to give it the
entire pipeline and remember that this
pipeline is taking care of our data
leakage worries because we're not z
scoring our training set before using it
in the grid search function
we are going to save ourselves from
worrying about data leakage because
it'll happen inside the grid search
function
so i'm going to give it our model or in
this case our pipeline then i'm going to
give it ks which is our dictionary of
possible hyper-perimeter values then
we'll say scoring equals accuracy
cv equals five and refit equals true
once we have this we can fit our model
so i'll say grid dot fit x train y train
and notice that because the z scoring is
happening inside the pipeline for the
model we are giving it raw data when we
fit the model again because z-scoring is
happening in our pipeline as a part of
our model process
we are giving it raw data which will
then be z scored as a part of the model
and then fit so now that we have our
fitted model we might want to do a
couple things like grab the k that it
chose and get the train and the test
accuracy so to get the k it chose i'm
going to say grid dot best estimator dot
get params and grab the k neighbors
classifier and neighbors parameter
and then i'm going to print out our
train and our test accuracies
all right let's go ahead and run
and you can see all of our outputs here
so here it shows k equals 22 and our
training accuracy is about 69
and our testing accuracy about 64
to review now that we're doing hyper
parameter tuning using grid search we're
again worried about data leakage this
time from the validation set because
grid search is doing all of our
validation train splitting for us we
can't z-score the original training set
before we put it into the grid search
model
instead we create a pipeline which is
basically a list of steps that we want
as part of our model fitting process in
this case our steps were a z-score and
then fit the model because we're giving
grid search a pipeline it will actually
take care of everything for us and
prevent that data leakage
now that we know about grid search and
we know how to be careful with grid
search to avoid data leakage using
pipelines let's think about what other
things we could hyper parameter tune in
k n the hyper parameter is k
but think about with decision trees one
of the things we could set was the
maximum depth of the tree
setting the max depth helped prevent
overfitting and sometimes we don't know
what the best max step should be
in comes grid search we can use grid
search to choose things like the max
depth or the min samples in a leaf node
so let's go through a quick example
first we are going to load in our data
and do our initial train test split
next let's make a z-score object which
we'll make with make column transformer
again make column transformer takes in a
tuple with the transformation we want to
do and a list of columns that we want to
do it to
in this case we only want to z-score one
variable amount because cashback and
home are both categorical so we are
going to say we want to do a standard
scalar transformation and we only want
to do it to the column amount
next we have to say remainder equals
pass through because of course if we
don't put that it would actually drop
these two non-transformed columns next
let's make our model so i'll say dt
equals decision
tree
and notice i'm not setting the max depth
here all right we have our two steps
let's make a pipeline so i'm going to
say pipe equals make
pipeline and in order i'm going to put
the steps i want it to do so z comma d t
next as before i'm going to print out
the names of the parameters so i know
which value max depth is in order to put
it in the dictionary of potential max
depths so i'm going to run this
and i'm going to look for something that
looks like max depth
all right so i found a parameter called
decision tree classifier underscore
underscore max underscore depth and so
i'm going to use that in creating my
dictionary of potential values and here
actually it's not k it would be max
depth
so i'm going to say
depth equals
and the name that i found and then the
different depths that i want to try
here i'll say what range
one to nine so this would try a max
depth from one all the way to eight
next we can create our grid search
object so grid equals grid
and we're going to give it our pipeline
so we'll say pipe comma depth
accuracy equals or excuse me scoring
equals accuracy
then we can go ahead and have our model
fitted on our full training set because
again grid search takes care of the
train validation split so we'll say grid
dot fit x train
y train and again remember because z
scoring is part of the pipeline
we are giving raw data to the dot fit
because it's expecting raw data and
z-scoring will happen inside the grid
search function
all right let's go ahead and run that
and you can see we now have a fitted
decision tree model as before we can
grab things like the parameter it used
so we can say grid dot best estimator
get params and this will tell us the max
step 3 that it ended up using we could
also if we wanted to grab things like
the accuracy or other metrics from this
model but for time i'm not going to do
this here last but not least quickly i
want to go through what using grid
search would look like if you use k-fold
for your trained test split
so here this should all look really
familiar we are grabbing our data
creating a k-fold object and starting
our k-fold for loop
here like normal we're going to create a
model k
and then i'm going to create a z-score
object make column transformer
and i give it a tuple standard
scalar
and i'm going to give it which variables
i would like to z-score which in this
case is all of them so i'm going to copy
that
and then of course just to be safe i'm
going to say remainder equals pass
through just in case there were any
columns that we didn't transform that
we'd like to keep and then make a
pipeline
put in order the steps
and then get a dictionary
of k's
and we'll say k
neighbors classifier underscore
underscore and
and we'll try range 1 to 30 yet again oh
and i put an extra n
all right so now we can build our grid
search we'll say grid
search c
v
and this should look exactly as it did
when we did just a regular train test
split so we'll say pipe
case scoring
cv equals five refit equals true
and then we will go ahead and fit we'll
say grid dot fit
x train y train
and again raw data going in because z
and finally if we wanted to we could
also grab our metrics again for time i'm
not going to do that here but you would
grab your accuracies your rrca uc's all
of that stuff here and append it to a
list
and now you can see it's done and that's
how we would use grid search for our
train validation split if we use k-fold
or leave one out for our train test
split alright that's all i have for you
i will see you next time


 
hello and welcome to your second support
Vector machine lecture where we'll
actually get to learn about support
Vector machines
to review last time we talked about two
models the first of which was the
maximal margin classifier the maximal
margin classifier uses a hyperplane to
divide our space into two having
positive cases be on top of the
hyperplane and negative cases being
below the hyperplane
in a maximal margin classifier we choose
a hyperplane that perfectly divides
between our positive and our negative
cases choosing a hyperplane that
maximizes the margin between the closest
data points and our hyperplane however
we also talked about the fact that in
real life our groups of data points tend
to overlap so there is no one hyperplane
that can perfectly divide our positive
cases from our negative cases income the
support Vector classifier using slack
variables the support Vector classifier
allows data points to be inside the
margin instead of just on it or even be
on the incorrect side of the hyperplane
this allows us for some more flexibility
to use a hyperplane to classify groups
that are not linearly separable
so support Vector classifiers solved a
problem that we had with maximal margin
classifiers similarly support Vector
machines solve a problem that we have
with support vector classifiers
our new problem is that not all of our
data is going to be linearly separable
even if we have the slack variables that
we introduce with support vector
classifiers
for example with this data no matter how
big your slack variables are there's no
linear hyperplane that will be able to
classify our blue data points in these
groups from our pink data points in this
group on the right hand side you can see
an example of a support Vector
classifier trying to do this and pretty
much failing
so what do we do when we want to use a
support Vector classifier but our data
is not even linearly separable
introducing the kernel trick when we
take the kernel trick and apply it with
support Vector classifiers we get
support Vector machines you can see in
these pictures that these are the types
of classification boundaries that we
would really like to draw but we just
can't with a regular support vector
classifier
let's take a step back so I can show you
what the kernel trick is by example
let's take this very simple data set
that represents my happiness or
unhappiness with regards to how many
chicken fingers I eat you can see from
this graph that I'm relatively happy
when I eat about three to six chicken
fingers but I'm very not happy when I
eat less than three or more than nine
probably because I'm either very hungry
or overly full respectively now if I
wanted to use a straight line to
distinguish between the times when I am
happy versus unhappy with the amount of
chicken fingers I have I can't really do
it if I put the line here I'm
misclassifying a bunch of my data points
the same thing happens here
and here
no matter where I place my line I just
can't use a single straight line in
order to divide my unhappy versus my
happy cases but what if I introduced a
new variable chicken fingers squared all
this is is taking my original variable
and squaring it and making that my
second predictor now in this new
two-dimensional space I can use a
straight hyperplane in order to classify
Happy from unhappy it would look a
little something like this so we just
saw when we projected our data to a
higher Dimension we were able to use a
flat hyperplane in order to classify our
two groups of data points even though
that wasn't possible with our original
data this projection into multiple
Dimensions is the basis of the kernel
trick a kernel in the case of svms is a
function that calculates the
relationship between two vectors or data
points in multiple Dimensions without
actually making us calculate what the
coordinates of those dimensions are for
instance in our chicken example we
actually created the coordinates for our
multiple Dimensions we actually had to
calculate what all the chicken fingered
squared values were and use those
coordinates as if they were another
predictor in our model what the kernel
trick says is that we can calculate the
relationship between two data points
without actually calculating those
coordinates to do so we need to use a
kernel for example in the chicken
example I used something called a
polynomial kernel which looks a little
bit like this
this kernel says that the relationship
between two data points is whatever this
value equals
here X and Y are our data points R is
something called the coefficient and D
is the degree which are both hyper
parameters that often will choose via
some type of hyper parameter tuning in
the chicken fingers example I used a
polynomial kernel with d equals D or
degree equals 2 and a coefficient equals
one-half plugging that in we get that
the kernel between two data points X and
Y is equal to x times y plus our
coefficient one-half to the second power
if we expand that out we get x times y
plus one half times x times y plus one
half expanding that out further we get X
Y plus x squared y squared plus one
quarter now this value can be expressed
as the dot product between two points
those two points would be x x squared
one half
dot y y
squared one-half
so you can see what we're doing here is
basically creating a data set with three
variables
the original variable
the variable squared
and then this constant one-half which
since it's the same for all of our data
points we can just safely ignore
so basically this is saying that we're
calculating the relationship between two
points as if those points are projected
into a two-dimensional space where the
First Dimension is just the variable by
itself and the second dimension is that
variable squared which oh looks really
familiar to what we did with our chicken
example and notice that on this slide I
calculated these values without actually
projecting my data into that second
dimension if I have two data points x
equals one y equals two I can just plug
them into my kernel function and I can
get their relationship as if I projected
them into multiple Dimensions without
actually doing that projection for
instance with these examples I would get
one times two plus one half the
coefficient we chose to the second power
expanding that out we get 1 times 2 plus
1 squared which which is 1 times 2
squared which is 4.
plus one quarter
in other words the relationship between
these two points is 2 plus 4 plus 1 4.
or six and one fourth
now notice when I did that math to
calculate the relationship between these
points never once did I take my data and
project it to our two dimensional space
I just plugged it into our kernel and
that's what makes the kernel trick so
special again kernels allow us to
calculate the relationships between two
points as if we projected them into
higher Dimensions without actually
having to do that projection and we can
use the polynomial kernel with a bunch
of different coefficients for r or
degrees for D the D basically controls
the maximum amount of Dimensions that we
could have now the polynomial kernel is
incredibly useful and very popular but I
also want to talk to you about a
different kernel called the radial
kernel which is maybe even more popular
the radio kernel which is represented by
this equation here basically acts as if
it projects data into an infinitely
dimensional space before with our
polynomial kernel we projected our data
into two Dimensions the radial kernel
projects it into Infinite Dimensions so
now you can see why it's really
important that a kernel function
calculates the relationship between
variables without doing the actual
projection because it would literally be
impossible for us to project data into
Infinite Dimensions it turns out that
the radial kernel acts like a weighted
nearest neighbor classifier where points
that are further away from a data point
have less influence on what the class of
that data point should be in order to
prove to you that the radial kernel is
basically projecting into Infinite
Dimensions I need to take a step back
and review Taylor series with you
remember Taylor series are a way to
rewrite a function as an infinite sum of
a bunch of different parts for instance
here we have the function e to the X
here we're using only the first of all
of the infinite parts to approximate our
function and then you can tell it's uh
it's not doing too well but when we add
our second part it's getting a little
bit closer and we add a third part even
better fourth even better fifth even
better six oh my gosh even better and
seventh even better you can see that as
we take all of the values in this
infinite sum we're getting closer and
closer to our function e to the x that
we're trying to approximate
mathematically you can write out a
Taylor series like this as an infinite
sum and one really useful application of
Taylor series even though it's not what
we're doing with them here is that we
can take only the first few of those
infinitely many parts and use it as an
approximation for our function which is
often simpler than our actual function
all right let's get down to proving that
our radial kernel is actually projecting
data into Infinite dimensions
in order to do that we have to revisit
our polynomial kernel remember that our
polynomial kernel can take data and
project it into up to D dimensions
now before we chose a coefficient value
of one-half let's see what happens when
we choose a coefficient value of zero so
here we have x times y plus zero or
nothing to the power of d
all right let's remember this for our
next slide so when we choose a
coefficient of 0 it's a bit silly
because it basically projects our data
into a single Dimension when we choose a
d of 1 you can see that it basically
projects the data into one dimension
where the data is unchanged when we use
a degree of 2 you can see that it
where every value is just itself squared
but what if we took a bunch of these
polynomial kernels with coefficient zero
and added them together as you can see
here this would basically be like
projecting our data into multiple
Dimensions where the First Dimension is
just the variable itself the second
dimension is the variable squared and we
could actually keep going with this
where our third dimension is the
variable cubed now what if we did this
but over and over and over and over
until Infinity
that would be pretty cool we're taking
our data and we're projecting it into
Infinite dimensions
and we're doing it all by adding a bunch
of polynomial kernels together well it
turns out this is essentially what our
radial kernel is doing and I'll prove it
to you using Taylor series expansion
taking our radial kernel and expanding
it using some algebra we get this value
here
gamma is just a hyper perimeter that we
can choose so I'm going to set it to one
half just so this pesky little two goes
away now that I've had that I get this
function so we are going to take this
right hand term e to the x y and we are
going to use a Taylor series to rewrite
it I won't make you walk through all the
math but this is basically how you get
the Taylor series of e to the X plugging
in X Y for X we get this infinite series
now something interesting is happening
here so I've pasted our sum of infinite
polynomial kernels below and let's
compare these two here I see something
really similar
anything to the power of zero is one so
here I see some matching terms
similarly here I see x to the power of 1
and y to the power of one which oh I
also see here
similarly here I see squared values in
here I see squared values
and continuing on and on here I see
infinite powers and here I see infinite
Powers so with some extra math that I
won't be showing you you can basically
see how the radial kernel is basically
projecting data into an infinite number
of Dimensions essentially it's as if we
calculated new features where the first
feature is one for every variable the
second feature is itself the third
feature is the variable squared and so
on and so forth up to infinite number of
Dimensions so radial basis kernels
basically have coordinates for infinite
Dimensions which wouldn't actually be
possible to calculate if it weren't for
the kernel which calculates the
relationship between two points as if
they were in those Dimensions without
actually forcing us to calculate those
Dimensions which we couldn't because
there is an infinite amount of them now
I skipped a lot of math at the end which
was as much for my sake as it was for
yours but if this is something you're
interested in I recommend statquest
support Vector machines part 3 video
which actually walks through not only
the math that we talked about with
Taylor series expansion but finishes the
entire thing if you really want to see
the proof from start to finish now that
we've proved that radial kernels
essentially project data into Infinite
dimensions and then use a hyperplane to
separate the data in those Dimensions we
can see what it actually can do for
instance on the right hand side we can
see that original data set that we
looked at classified using a support
Vector classifier but with a radial
kernel when we pair kernels with support
Vector classifiers we call them support
Vector machines and as you can see they
allow us to have non-linear boundaries
using a linear model like the support
Vector classifier all we have to do for
a support Vector machine is change a
little bit of the notation that we have
previously this constraint told us that
all of our data points have to be above
or below the plane now we're rewriting
it and all we're really adding here is
this little Phi function this
essentially says that we're creating a
support Vector classifier after
Transforming Our data X using the
function Phi usually this will project
our data into multiple Dimensions
similarly in order to classify a new
data point we can use the same formula
as before except now instead of just our
X's we have our Phi of X for both the
data points
what the kernel actually does is
calculate this dot product 5x times
another 5x without actually forcing us
to know what Phi is what that projection
into multiple Dimensions is and to go
back to something we talked about in the
first lecture it's really cool that
where the hyperplane is and how to
classify a new data point only relies on
the dot product between two points
kernels calculate that dot product as if
they're in multiple Dimensions without
Dimensions which makes our math a lot
easier and makes these feasible to use
even when we have kernels like the
radial kernel that project into Infinite
Dimensions to review we started with a
maximal margin classifier a hyperplane
that perfectly divides our data into
positive and negative cases then we
realized that sometimes such a
hyperplane doesn't exist data points
might overlap between categories or we
might just want to have a hyperplane
that's not so affected by outliers
support Vector classifiers allowed us to
do this by introducing a slack variable
which allows data points to be inside
the margin or on the wrong side of the
hyperplane
sometimes we have groups of data that
aren't going to be linearly separable no
matter how much slack you have in comes
the support Vector machines which pair
support Vector classifiers with kernels
that project our data into multiple
Dimensions this allows us to use a flat
hyperplane to classify our data even
though in our current dimensions that's
not possible the kernel trick is really
cool because it allows us to calculate
the relationship between two points as
if they're in those higher Dimensions
Dimensions this is really nice because
often with the ever popular radial
kernel this wouldn't even be possible
for us to calculate so the kernel really
saves Us by allowing us to calculate
relationships as if we did that
projection all right that's all I have
for you I will see you next time

 
hello and welcome to your second
Transformers lecture
today we're finally going to get to
attention which apparently is all you
need in the Transformer architecture
in the previous lecture we talked about
what happens to vectors as we process
them in order to prepare them to go into
the Transformer Network
we talked about some things that might
be familiar like word embeddings where
we take words and we create non-sparce
hopefully low density vectors that allow
us to represent the semantic meaning of
a word we then added something called
positional encoding which injects
information about an item's position in
a sequence into the word representation
this allows us to feed all of the words
in a sequence to the model at one time
rather than processing them sequentially
but let's take a tour of the rest of the
Transformer architecture before we do I
just want to remind you that this
specific Transformer laid out in the
paper is used for machine translation
which means it needs both this encoder
and this decoder stack however
technically you can use an encoder by
itself or a decoder by itself and at the
very end we'll talk about some models
that do just that first up let's look at
the encoder layer the encoder layer is
actually really a stack of n different
encoder layers that look exactly like
this the job of an encoder layer is to
take in a sequence and create a hidden
representation of that sequence the
encoder layer has two sub-layers first
what we're going to focus on today which
is multi-headed attention and then it
goes through just a simple feed forward
layer the decoder layer has a lot of
things that look very similar to the
encoder and again the job of a decoder
is to take a hidden representation and
produce a new sequence just like the
encoder layer the decoder layer has
multiple sub-layers first it has a
masked multi-headed attention then it
has a regular multi-headed attention and
then of course like the encoder it has a
simple feed forward layer last but not
least we have the token generator and
even though it's not technically part of
the decoder layer it takes the output of
the decoder layer feeds it through a
dense layer and allows us to actually
make predictions about which item item
should be next in a sequence all right
now that we've had an overview of the
Transformer architecture let's finally
talk about attention before we do I just
want to show you this meme while we're
going to talk about a lot of math today
I want you to remember that attention at
least the attention mechanism we're
going to cover today is really just
doing this taking a bunch of vectors
multiplying them together scaling them
and putting them through a soft Max
function and so even though some of what
we talk about might seem a little bit
difficult or complicated remember that's
all that's happening when we talk about
actual attention conceptually attention
asks models to learn what specifically
it should pay attention to in order to
do its task like predict the next word
or classify an image in a convolutional
architecture attention would look like a
mask that tells the model what pixels
are important in classifying this image
for instance as a cat this can help our
model filter out an necessary
information while putting a very high
weight on useful information in a
sequential model where we're processing
sequences like sentences we can think of
attention like this in this Matrix we're
looking at the sentence the Frog crawled
to the bank and you can see that we have
entries for every combination of each
word with any other word in the sentence
if we look at a single column in this
Matrix this basically gives us an idea
of how important each word in the
sentence is to pay attention to when
processing the meaning of the word bank
for instance we have two very common
ways to use the word Bank like a
Riverbank or like a money bank here we
can see that it's obviously very
important to pay attention to the word
Bank when processing the word bank but
also it looks like it's really important
to look at the words frog and crawled in
order to tell us a little bit more
context about the word Bank however
these words like to and the don't really
have a huge impact on the meaning of the
word bank and of course each of the
columns in this Matrix represents that
same information but for each individual
word in the sequence for instance when
looking at the word crawled we need to
pay special attention to the words frog
and the words think and this is exactly
what attention is in a Transformer we're
basically looking to create a matrix of
values that tells us the relative
important of paying attention to
different words when processing a given
word in the sequence for instance in
this sentence the train left the station
on time we don't necessarily know what
type of station it's talking about is it
talking about a space station or a radio
station or maybe a train station
you can see from this Matrix that when
processing the word station we have very
high attention values on the words train
and left because those give us good
context that helps us understand what
type of station this might be if we pull
a column from this Matrix like the
column for station we can use these
values as weights to create something
called a context aware Vector a context
aware Vector basically takes a
combination of all of the different
words in the sequence weighted by their
attention scores so higher attention
scores means a higher weight and
combines them to create that context
aware Vector this is a new Vector
representation of our word that now
includes context from the different
words around it theoretically the way we
calculate these is with a formula like
this here we're basically creating new
word vectors for each word by taking a
sum of the weights generated by the
attention scores times all of the
different words in the sentence so we're
creating a linear combination of all the
words in a sequence that allow us to
infer the contextual meaning of our
actual word words that have really high
attention scores will have really high
weights meaning that more of their
information is included in the context
of our new context aware Vector so again
we're going to take these weights
attention weights in this case and we're
going to multiply them by each embedding
so each word in the sequence has an
attention weight higher if that word is
important to understand the context of
our current word and lower if it's not
important we then take the sum of these
and we output our context aware Vector
which now incorporates meaning both from
itself as well as all of the different
vectors in its sequence one important
thing to note is that these weights need
to sum up to one now that we've seen the
math I think we can understand this
graphic a little better first we're
creating a matrix that tells us the
relative importance of paying attention
to different words in the sequence then
for each word we're going to take its
column of scores and use them as weights
to create a context-aware vector by
combining meaning from all of the
different words in our sequence using
their weights and then outputting this
context aware Vector so so that's how
attention Works generally so how do we
apply that to Transformers specifically
Transformers use the concepts of queries
keys and values to describe the way it
uses attention you're probably familiar
with queries queries are basically us
making a request for something
for instance we might create a query of
dogs on the beach which is asking for an
image with dogs on the beach
in this case the keys are different
features that appear in that image for
instance here we're looking at beaches
trees boats Etc in order to figure out
which value we would like to look at we
take our query compare it to the
different Keys we have and see which
ones match the best for instance if
we're looking for dogs on the beach this
image here matches the best because it's
the only one that has both dogs and a
beach so to review conceptually queries
are basically requests for what we're
looking for keys basically give us an
idea of what we have to offer and we'll
look for a high match between our query
and our key once we know which querying
key match the best we can then choose
the best value
based on those queries and keys and
these are the ideas behind what
Transformers do they create these
vectors the query the key and the value
by taking all of our word embedding so
this is the same exact input for all
three of these vectors and they multiply
them by a weight Matrix these weight
matrices are full of learnable
parameters which are what are trained
when we're training our Transformers
these are also different for each
attention head but we'll get to that a
little bit later so what we're doing
here is we're taking our word embeddings
and we are projecting them into
different spaces that we've learned via
these weight matrices this results in
our queries our keys and our values just
a quick note the weight matrices for
queries and keys must be the same size
but the ones for the values could be a
different size it is a little confusing
though because in the paper attention is
all you need all three of them have the
exact same size so these queries keys
and values that we generate by applying
a weight Matrix to our embeddings are
then fed all three together into our
attention head in the paper they use
something called scaled dot product
attention which uses this formula in
order to create our attention values and
again now that we're seeing some
formulas I just want to remind you that
even though we're talking about
difficult conceptual things like queries
and keys and values and projecting data
into different dimensions attention is
literally just taking some vectors
multiplying them together a lot and
putting them through a soft Max function
alright so let's look at each part of
this formula when we looked at attention
for sequences before we had a matrix
that told us the relative importance of
paying attention to different items in
the sequence this is created with this
part of the function this as a whole
creates our attention weight first thing
we need to do in order to calculate
these attention weights is multiply our
queries and our keys together this will
give us an idea of which queries are
similar to which keys and thus which
ones we should pay the most attention to
this is represented in this part of the
scale dot product attention graph in the
attention is all you need paper now when
we do this multiplication of queries
times Keys we might end up with values
that have a lot higher variance than the
queries or Keys themselves in order to
make sure everything is on a much
similar scale we are going to scale our
values by the square root of DK which
again is the dimensions of both our
queries and our keys this scaling is
represented here in the scale dot
product attention graphic last but not
least we need all of our weights to add
up to one and so we're going going to
send the values that we just created
through a soft Max function again that's
represented here in the graphic now that
we've done that we've created all the
attention weights that we need in order
to create these context aware vectors so
the final step in doing that is taking
the weights that we just generated and
multiplying them by the values together
represented here in the diagram this
will give us our context aware vectors
so again to review we take our queries
our keys and our values and feed them
into an attention head first we're going
to multiply our queries and our keys
which is basically a way of seeing which
queries and keys match the ones that
match will likely have higher scores and
therefore be more important to pay
attention to when processing the context
of a word then because this
multiplication can create quite large
numbers we are going to scale them so
that they're on a relatively similar
scale as the queries and the keys last
once we've created these weights we are
going to feed them through a soft Max
function so that all of our weights add
up to one finally we will take our
weights and we will multiply them by the
values in order to create these
context-aware vectors now you might have
noticed that in the previous diagram I
skipped a part of the scale dot product
attention diagram masking masking is
optional but what it does is it takes
the weights that we've generated by
scaling and multiplying our queries and
our keys and masks the upper right
corner in our encoder we have access to
every single item in a sequence when we
feed it in however in our decoder that
will not always be the case in our
decoder we only have access to words in
a sequence that come before our current
word notice that this is the only
attention head that has masked in front
of it because we can only consider words
that come before we need to take the
weights that we just generated and mask
the ones that are related to items that
come after our current word in the
sequence the way we do that is we take
the Matrix that we've generated and we
set the upper right triangle to be
negative Infinity when we send a
negative Infinity through a soft Max
function these values will all become
zero essentially masking them out and
not allowing our model to consider the
context that comes from item times later
in a sequence again this only really
needs to happen in the decoder layer
because in the encoder layer we're
guaranteed to have access to our entire
sequence now that you know a little bit
about what attention is and how we
create context aware vectors based on
paying attention to different words in
the sequence let's talk a little bit
about cross attention you may have
noticed that in our decoder we
technically have two multi-headed
attention layers in the second
multi-headed attention layer our queries
are actually coming from the decoder but
are keys and values are coming from the
encoder you can see this represented
here in the diagram where our keys and
our values are coming from the encoder
layer and our queries are coming from
the decoder this is called cross
attention because we're combining
information from our hidden
representation from the encoder as well
as information from our decoder the
queries the things we're looking for
alright so now we've talked thoroughly
about each part of our scaled dot
product attention taken together this
entire process is called an attention
head it takes in queries keys and values
and then creates attention scores
multiplies those by our values and then
outputs context aware vectors for all of
our different items in the sequence if
we are working in the decoder we may
also mask our weights so that we're not
considering items later on in a sequence
now a single attention head seems
complicated enough but actually in
Transformers they use something called
multi-head attention multi-head
attention basically says that we have
multiple attention heads each which are
supposed to learn different things to
pay attention to so instead of having a
single attention head we take our
embeddings and we feed them into
multiple attention heads each which have
their own weights to create the queries
the the keys and the values these are
then all fed through their own scale dot
product detentions and at the end we
take the output from each of our heads
and concatenate it which basically means
we squish it all together in one object
so first we'd have the output from the
first head then the second head then the
third head and the fourth head Etc this
then is the output of the multi-headed
attention layer not just the results
from a single attention head but from
all of our attention heads because we
have different attention heads with
different weight matrices it allows us
to learn different things about the
sequence for instance one head might pay
attention to the tenth of things and the
other might pay attention to the subject
in a sentence each of these attention
heads gets the same input or embedding
vectors but separately and independently
learns the different weight matrices
that allow it to pay attention to
different things alright you may have
also noticed that in the Transformer
architecture we also have a couple
different ways that we are regularizing
our model first one is that we normalize
the output of our layers this is
something that we've done before through
batch Norm second we use residual
connections remember residual
connections take an undisturbed copy of
our information and feed it through a
separate path so that while a copy of
that information is being processed the
other copy remains unchanged at the end
of the process we then combine these two
things together so that the information
in our original data isn't lost but we
also get the benefit of all of that
processing that we're doing in the
Transformer architecture you can see
that every time we have a sub layer we
also have a layer normalization this
means that the output of our different
sub-layers like the multi-headed
attention layers are normalized after
they are output we also have residual
Connections in each Subway this means
that as we take our information and
process it through a multi-headed
attention layer we also take a copy of
that information don't change it and add
it back to our output once we're done
processing our multi-headed attention
alright that was a lot of information
but today we learned a lot about
attention and all of the different parts
of a transformer in the last lecture we
talked a little bit about positional
encoding and word embeddings which
prepare our words to enter our
Transformer model today we talked a lot
about attention and specifically
multi-headed attention which is what the
Transformer uses in each of our encoder
layers remember there's n of them we
have two sub-layers the first sub-layer
sends our data into a multi-headed
attention layer we take our word
embeddings and we project them so that
they become our query our key and our
value vectors we then feed these into an
attention mechanism which uses our
queries and keys to figure out what we
should pay attention to and then applies
that to our values to create context
aware vectors these are then fed through
a typical feed forward layer on the
decoder side we have some really similar
things happening first we have another
multi-headed attention layer except this
is masked multi-headed attention this
accounts for the fact that in our output
sequence we won't be able to consider
future words in the sequence like we
would when we can process the entire
sequence in the encoder we create this
mask by taking the upper right triangle
of the Matrix and setting all of its
values to negative Infinity that way
when we send it through the softmax
function these values will become zero
essentially removing them from
consideration when generating our
context aware vectors we then have a
second multi-headed attention layer this
one does cross attention because the
keys and the values come from the
encoder but the queries what we're
looking for come from the decoder last
but not least like in the encoder we
feed this through a typical feed forward
layer once the decoder is done
processing things we send it through
this token generator which actually
tells us what words we should output
based on the output of the decoder we
also talked about the benefit of
multi-headed attention which allows us
to not only have one attention head but
multiple so that different attention
heads can pay attention to different
things and last but not least we talked
about the various ways in which the
Transformer implements regularization
for instance through layer normalization
as well as through residual connections
now I mentioned earlier in the lecture
that Transformers have pretty much taken
over sequence processing in deep
learning models in fact you're probably
aware of a lot of different Transformer
models that are out there in the real
world for instance a really common model
in natural language processing for doing
things like text classification is the
model Bert which is an encoder only
Transformer model and of course I know
all of you have heard of this decoder
only Transformer model which is GPT GPT
stands for generative pre-trained
Transformer so of course it's a
Transformer model and last but not least
there are a ton of encoder decoder
Transformers out there especially in
things like machine translation
Transformer models really revolutionize
the way we process sequences in deep
learning before then we were really
stuck with recurrent architectures like
lstm or a GRU
Transformers are a lot faster and allow
us to do things that we just couldn't do
Transformers really speed up the amount
of information we can process at one
time and therefore make more efficiently
trained models which is probably why
they've taken over alright that's all I
have for you I will see you next time

 
hello and welcome to your neural
networks and optimization lecture number
three it kind of feels like in our
previous lecture all we talked about
were gradients gradients allow us to
know what nudges or changes we should
make to the different parameters in our
model in order to optimize or minimize
our loss function and honestly today is
going to be more of the same all we want
in life when training something like a
neural network is to know the gradient
remember gradients tell us for each
parameter how changing that parameter is
going to affect our loss function and
when we talked about this in our
optimizers lecture we basically just
assumed that we knew what the gradient
was or we used a very simple function
like a linear regression where the
gradient is really easy to find but in
the types of neural networks that we're
going to learn from this point on the
gradient isn't going to be that easy to
calculate in fact we need an additional
algorithm to calculate what those
gradients are one of the most common
algorithms is called back propagation
and back propagation is just a method
for computing the gradients of our loss
function with respect to all the
different parameters in our networks
back propagation is based on the Chain
rule which you may have learned in
calculus essentially the chain rule says
that if we have a composite function so
a function of a function and we want to
know how changing an input affects that
composite function we can basically use
the chain rule to multiply various
derivatives in order to to get our
answer
so for instance I have this composite
function f of G of X which is the cosine
of x squared it's a composite function
because it's made up of two different
functions it's made up of a combination
of cosine of x and of the function x
squared so if I wanted to know how does
changing X Change the output of f of G
of x a k a the cosine of x squared I
need to think about two things first I
need to think about how changing X
changes the function x squared as I
increase x what happens to x squared
however that's not the only part of the
function I also need to think about once
I have x squared how does that affect
cosine of x squared the chain rule says
in order to calculate that derivative
the derivative of our composite function
f of G of X with respect to the original
input X all I need to do is multiply two
derivatives together first I need to
look at how does changing our original
input X Change our function x squared
then I need to look at how does changing
x squared change cosine of x squared
when I multiply these two derivatives
together I get the full derivative which
tells me how it is changing X Change the
composite function cosine of x squared
and while this is a super simple example
we can actually chain together multiple
functions with the chain rule we can
just keep chaining on and on so how does
this relate to gradients and back
propagation well back propagation is
basically the application of the chain
rule to neural network parameters let's
look at this world's simplest neural
network where all we have is three nodes
with two weights and two biases
this input node X is just going to be
the data that's coming into our model
in order to get this intermediate value
H we take X multiply it by our weight
one and then add a bias one which we
don't usually show graphically but you
can see is added here then in order to
get from H to G we're going to multiply
H by weight 2 and add bias 2 to get our
final output G so for our Network once
we've produced a predicted value we need
to see how good that value is how close
is it to the value that we would like it
to be in order to calculate that we need
a loss function I'm going to use the
mean squared error because it's a pretty
simple pretty easy to calculate loss
function so remember that the mean
squared error is just the difference
between the actual value and our model's
output the predicted value so squared
and then taking the average of all of
those
if we look at how through this very
simple neural network we actually get
our output value G you can see that we
get it by taking our input value X
multiplying it by weight 1 which is
right here and adding bias one
that gives us the intermediate value H
but then we take h
and we multiply it by weight 2 and add
bias two and that finally gives us our
output value G when we plug in the way
that the neural network makes that
prediction into our loss function we get
this you can see that all we've done is
replace this G sub I with the way we
actually calculate the prediction so now
we have this beautiful loss function and
there's a ton of different values in
this last function that are changing
what the value of the loss function are
but if you think about it we don't
actually have control over what our
training data is in other words we don't
get to set the inputs and the outputs of
our models which are these X's for the
inputs and these Y's for the outputs
since we don't have any control over
them we don't really care about them all
we have control over in a neural network
are our parameters in this case just our
weights and our biases so because we
actually can change these weights and
biases we might want to ask the question
well how should I change them in order
to minimize my loss function for example
what changes should I make to weight one
to minimize my loss what about changes
to weight 2 or bias one or bias two well
that's exactly what a gradient tells you
a gradient tells you what adjustments or
changes you should make to all the
different parameters in your model in
order to minimize your loss function but
how do we calculate those gradients when
the functions used to predict our values
are getting more and more complicated
the bigger our neural networks get well
the answer is of course back propagation
so let's do an example using the loss
function that we calculated previously
let's look at how changes to be weight
One impact the loss of our model well
based on how the network is structured
we know that changing weight 1 will
change what H is that intermediate value
in the middle of our Network and
changing H will affect what G is and
changing G is will affect what our
overall loss is so we need to consider
all of those things when we talk about
how changing the first weight affects
the loss of our function so we're going
to use the chain rule here we're trying
to find how is our loss affected when we
make small changes to wait one first we
need to think about how does changing G
that predicted value change our loss
function then we need to think about how
does changing h
affect G and finally we need to think
about how does changing weight one the
parameter we're interested in change H
when we multiply these three derivatives
we get our answer how changing weight
one will affect the loss function
generally we can work through this one
by hand just because it's not too
complicated so the first loss that we
want to calculate is how changing G
affects our loss in other words the
derivative of the loss with respect to G
well in order to calculate our loss we
just have 1 over n and then the sum of y
i minus g i
squared
so all we need to do is find the
derivative of this now again I don't
expect you to calculate this by hand but
the answer is negative 2 times y i minus
g i
okay one derivative down now we need to
consider how is G affected when we
change H so in other words the
derivative of G with respect to H well
it turns out that one's a little simpler
to calculate to get G we do W 2 times h
plus B2 so just our weight times h plus
a bias well the derivative of that with
respect to H is just W2 for our last
derivative we need to see how changing
weight one will affect H in other words
the derivative of H with respect to
weight one and this math is also
similarly simple in order to get H we
take our weight one times X Plus bias
one the derivative of this with respect
to W1 is just X
so when we multiply all three parts
together using the chain rule we get the
derivative of the loss with respect to
the weight in other words this function
tells us how changing weight one will
change the loss function now of course
we need to add this up for every single
data point that we have but basically we
just figured out what the partial
derivative is in other words we figured
out the first part of our gradient how
does changing the parameter W1 affect
our loss function then we would need to
repeat this process once for each of the
parameters that we actually have control
over however we've already done most of
the work if you look at this formula
which is asking how does the loss change
when we tweak B1 or our first bias you
can see that these first two terms are
actually things we've already calculated
the only thing that's new is this how
does changing B1 affect H well in order
to get H we use this formula and the
derivative of this formula with respect
to B1 is just one so in other words we
get 1 the derivative of H with respect
to B1 times two derivatives we
calculated before the derivative of G
with respect to H and the derivative of
the loss function with respect to our
prediction G and that's it that's
basically what back propagation is
except we're going to be applying it on
more complicated Network for example
look at this network which is still
pretty simple but a little more complex
than the one we just looked at let's say
we want to look at the effect of this
weight which we can just call W1 we can
calculate the effect of changing this
weight on the loss function using back
propagation just like we did before
however because of the structure of this
network we need to think about the fact
that tweaking W1 here actually impacts
the loss function in multiple ways for
example here
it affects the loss function through
this top path and here it affects the
loss function through this bottom path
so when we ask how does tweaking weight
1 affect the loss function we need to
add up the effects it has through this
first top path as well as this bottom
path another way that things can get a
little more complicated is with
activation functions now it's not
actually G of X that's going into our
loss function by making our prediction
it's actually F of G
which is basically saying apply an
activation function to the output G now
when we use back propagation to
calculate our gradient we're going to
have an extra term because we also need
to consider how changing G affects the
output of f of G basically how does
changing G affect the output of our
activation function so you can see that
the ideas behind back propagation are
simple however as our networks grow it
gets a little more tedious to actually
calculate all of the derivatives because
we have to consider multiple Pathways as
well as deeper and deeper functions for
example if we look at this still very
shallow Network and we wanted to know
what was the effect of changing this
weight we'll call it W1 on the output
and loss of our model well there's a lot
of Pathways that that affects changing
W1 will change the output of this node
and this node then impacts all of these
other nodes
and then all of those nodes go into our
final prediction so you can see that
calculating the effect of tweaking
weight 1 on the loss function is going
to be a little bit more mathematically
complicated just because we have to
consider all of these different pathways
through which changing weight 1 will
actually affect the loss now before we
finish I want to introduce some
vocabulary terms now that you know what
back propagation is the first term that
I want to talk about is a forward pass a
forward pass just refers to sending your
data through the network and getting an
output or a prediction from your model
the backward pass where back propagation
happens is basically doing that back
propagation and adjusting our weights
accordingly based on the gradients that
we calculate an iteration contains one
forward path and one backward path
essentially we send our data through the
model make a prediction use those
predictions and back propagation to
update our weights and that is a single
iteration the last vocabulary term I
want to cover is an epic or an Epoch I
guess it depends whether you're from the
UK or the United States based on how you
say it an Epoch is one iteration through
all of the data in essence one Epoch
means that your network has seen every
single data point in your training set
remember when we talked about the fact
that we often don't use all of our data
in order to calculate the gradient in
each each step of our gradient descent
or other optimizer for instance we might
want to do mini batch gradient descent
where we have you know 30 or 40 or 50
data points used to calculate the
gradient in that case an Epoch or an
epic contains however many updates and
steps it took for your model to work
through all of the data so say you had a
hundred training examples and you were
using a batch size of 10. that would
mean that your model is going to do 10
updates before it is seen all of the
data in your training set so that one
Epic or Epoch is going to be all of
those updates if we were using all of
our data at every single update then an
Epoch or epic would just be one single
run through of an iteration as we finish
up I just want to remind you that the
most important thing to us when training
these networks is gradients all we want
from life is a good gradient and back
propagation is just one way to calculate
the gradients for our models which can
get a little bit complicated did the
deeper and more complex they are back
propagation uses the chain rule in order
to calculate how changing different
parameters in our model is going to
affect the overall loss of our model and
while that idea is simple we have to
keep in mind that especially with
complicated model structures changing
one parameter might actually have an
impact in multiple different ways on our
loss function and we need to add all of
those up when calculating the impact of
changing that parameter alright that's
all I have for you I will see you next
time

 
hello and welcome to your first lecture
on Transformers and no we're not talking
about that kind of Transformer but we
are talking about a really exciting
state-of-the-art model to process
sequences in our recurrent neural
network lectures we talked a little bit
about different ways we might want to
use sequences when building deep
learning models one of the really common
ones might be something like machine
translation in which we put in a
sequence to a model and ask it to create
a hidden representation of the meaning
of that sentence then we might ask a
different part of the model to take that
latent hidden representation and create
a new sequence for instance translating
the first sequence that was in English
to a sentence in Spanish
in the recurrent architectures that we
talked about in the previous lectures
were really state of the art for a long
time for this type of task which is a
sequence to sequence task for instance
we could have an lstm that takes in a
sentence encodes its representation
through the process of sending it
through the lstm cell and then have
another lstm that is then going to take
that hidden representation and translate
it into a different language this
decoder basically takes that hidden
representation and generates a new
sequence item by item and this type of
encoder decoder structure should
actually be pretty familiar to you
because in our autoencoders lecture we
talked about a very simple version of
this structure where we have an encoder
whose job is to take in some input and
create some type of hidden
representation of it and then a decoder
that takes that hidden representation
and tries to recreate the output but the
autoencoders that we looked at even
though they were fancy and non-linear
we're still using just feed forward
densely connected layers or at the most
maybe some convolutional layers but what
if we had an auto encoder architecture
whose encoder and decoder were recurrent
neural networks this is exactly what was
proposed by Cho at all in 2014 when the
gru was proposed the paper proposed an
architecture where the encoder and the
decoder were both dated recurrent units
the first Gru would produce this hidden
state which was then fed to the decoder
Gru and influence what type of sequence
it was going to create
as I mentioned before this is great for
things like machine translation we want
to input one sequence and have it output
another sequence either summarized or
translated in some way and this
generally is the encoder decoder model
we have an input sequence and then we
use an encoder to create some type of
hidden representation of something like
the meaning of the sentence that we
input and then we have the decoder whose
job it is to take that hidden
representation and create a new output
sequence this could be the original
sentence translated into a different
language or maybe a summarization of a
really long document so encoder decoder
architectures are nothing new to us
which is good because that's exactly
what Transformer models are they have
this encoder section and a decoder both
of which take in and produce sequences
when this architecture was proposed in
2017 it was specifically looking at
machine translation tasks however over
the past few years Transformer
architectures have taken over pretty
much all sequence processing models just
like we talked about with the general
encoder decoder model the Transformer
has a portion that is the encoder it
takes in things like Words processes
them and creates a hidden representation
that representation shown here in red is
then fed into the decoder the decoder
then takes that information as well as
its own inputs and produces another
sequence so even though this
architecture looks a lot more more
complicated than what we're used to
seeing it is still that basic encoder
decoder structure as I mentioned the
2017 paper attention is all you need
propose the Transformer architecture but
it's really blown up since then and has
taken over pretty much any task where
you're going to process sequences and in
today's lecture we're actually only
going to focus on what happens to Data
before it's fed into the encoder or
decoder this section right here and
while the paper is titled attention is
all you need which might make you think
that attention is the most important
thing about this paper I actually think
what we're going to cover today which is
positional encoding is a huge
breakthrough in these types of models
the first thing you need to understand
about what happens when we take
different words and try and feed them
into our model is something we've
already learned which is word embeddings
from the diagram you can see that we're
taking in words and we are feeding them
into a portion of the model that creates
word embeddings and we've seen word
embeddings before for instance when we
talked about word to VEC remember word
embeddings are non-sparce usually lower
dimensional vectors that represent the
semantic meaning of a word meaning
vectors that are close together should
have similar meanings or at least
similar context this is an example of
what a very simple word embedding might
look like for the words gorgeous and
python one thing that we want to keep
track of for this lecture is the
dimension of this embedding space we're
going to call this D model and it just
keeps track of what the dimensions of
our embedding space is in this case it
would be one two three four five six now
you might notice that once we put our
words through the embedding process to
get these non-sparse hopefully lower
dimensional Vector representations we do
something else called positional
encoding in positional encoding is
especially powerful before we talk about
what it is let's talk about why
when you have a typical recurrent model
unlike the Transformer and we have a
sequence that we would like it to
process for instance the cat ate the bat
we feed those words in sequentially for
instance first we would feed in the word
the then we would feed in the word cat
then the word eight then the word the
and lastly the word bat this is what
empowers recurrent architectures to
process information sequentially and
take advantage of information that it's
learned about previous inputs in the
sequence however this makes these types
of models slow to train and not
paralyzable because if we're processing
an entire sequence we have to feed it
into the model one item at a time and
it's not like we can take that sequence
and break it up and send it to different
cores on our computer
but feeding these words in sequentially
is what gives us the power to take
advantage of these sequential
information in our sequence
so what do we do well we don't feed our
words in one by one in a Transformer
instead we feed all of the words in a
sequence into the model at a single time
this has the advantage of being
paralyzable and a lot faster than
feeding in data sequentially however it
does leave us with a bit of a problem
sequentially processing the data is what
a loud recurrent architecture is to
learn about information in a sequential
way and we just totally got rid of that
so if we're not feeding in words
sequentially we need to figure out some
way to still tell the model what the
position of that word is in the sequence
and that is exactly what positional
encoding does
positional encoding is a way to take our
word embeddings that we've already
generated and inject information about
where in a sequence a word occurs we're
going to do that by taking a vector that
is the same dimensions as our embedding
vector and adding it to our embedding
vector
this will basically inject the
positional information that we want to
know about the items in our sequence so
when we're done with this process we are
going to have an embedding that takes
into account both the semantic meaning
of the word which comes from the
original word embedding as well as
positional information which is going to
come from the positional encoding but
how do we create these positional
encodings let's look at this sequence
your python code is gorgeous and here we
have all of the embeddings for these
words one way we could encode positional
information that's pretty easy is to
just add a vector that tells us what the
index of this item in the sequence is
for instance the word your is at index
0. so we would add a vector of zeros to
it for the word python it's at index one
so we could add a vector of ones to it
for the word code it is at index two so
we could add a vector of twos to it and
this would indeed add positional
information to our word embeddings but
as we get longer and longer sequences
the indices that we're going to be
adding are going to get bigger and
bigger and bigger and what's going to
happen is it's going to overwhelm the
semantic information coming from our
actual word embeddings just because the
positional information is going to be
such large numbers
so if we don't want numbers that are
that large what if we scaled our values
so instead of using the indices like 0 1
2 3 4 instead we used a scaled version
where we took all of them and had the
first index be zero the last B1 and
everything else in between be a value
between 0 and 1. essentially we're
taking these original indices and
dividing them by the largest index and
that would definitely be a valid way to
do this we could take our original
Vector the first one we would add still
zeros but for the second one instead of
adding a vector of ones we add a vector
of 0.25
well that solves one of the problems
which is that we don't have positional
encodings that are going to drown out
the information in our word embeddings
however it does have another problem it
assumes that we know the length of our
sequence and that is not always going to
be the case so we've talked about two
good ideas for encoding positional
information that don't quite hit all of
the things that we want first we of
course want to actually encode position
and all of the things that we've tried
so far do indeed do that
however we also have the problems that
we don't want any large numbers we
basically don't want to overwhelm the
information coming from the word
embeddings by adding super huge numbers
for our positional encoding and last we
want our models to be able to adapt to
variable sequence links we don't want to
have to specify what the sequence length
is ahead of time
so how could we possibly do that
well what about some sine and cosine
waves the pros of sine and cosine are
that they're bounded values no matter
what you input into the sine and cosine
their outputs are always going to be
between negative 1 and positive 1. so
we've solved the issue of having values
that are going to be huge the next Pro
is that sine and cosine are functions
that can go on forever meaning that no
matter how long our sequence is we can
still encode positional information
about it however a problem that we have
is that sine and cosine are periodic
functions which means they repeat their
values every so often if we look at this
diagram let's look at this last sine
wave here here we have a sine wave that
has a pretty high frequency and in fact
when we look at the item with position
one and the item at position 3 we see
that they have the exact same sine value
and that's problematic because obviously
these are very different positions in
the sequence so what if instead of using
one sine wave we used multiple
now if we look at a combination of all
four of these sine waves that have
different frequencies we start to see
some differences between our positions
one and our position three thus if we
consider all of the sine waves together
we can get unique information about
where in a sequence something is located
and that's exactly what the attention is
all you need paper does in order to
encode positional information they use a
combination of sine and cosine functions
these sine and cosine functions have a
couple of different variables first we
look at the position of the word we also
have this constant term D which tells us
the dimension of our embedding vectors
basically how many entries we have in
this positional encoding vector and last
but not least we have this variable I I
tells us where in the positional
encoding Vector these values will appear
if you imagine we have a really long
positional encoding Vector low values of
I will occur higher up in this vector
and high values of I will occur later in
the vector other than those variables
these are pretty basic sine and cosine
functions in order to create our
positional encoding Vector we are going
to add pairs of sine and cosine
functions with the variables that we
just talked about and as you can see
here we're adjusting I as we move down
through the vector now because we're
adding pairs of sine and cosine entries
we only need to add D over two pairs in
order to actually get a vector that is
the same length as our word embedding so
instead of I being 0 all the way up to D
the number of dimensions in our Vector
we actually loop from 0 all the way to D
over two that way we have D over two
pairs AKA D total entries in our Vector
that's why you see these repeated values
you see Zero and zero here and then one
and one here because we are actually
creating pairs of sine and cosines if
your brain works better in code here's a
bit of pseudocode that would help you
run this here we are defining the
positional encoding Vector for a single
position here it's going to be the
position 0. in this case our dimensions
of our word embeddings and therefore are
positional encodings is going to be six
that means we are going to have six
entries in our positional encoding
vector first we're going to create a
vector of zeros that is the length D
meaning it has one entry per item that's
going to be in our positional encoding
then we Loop through all of the indices
from zero all the way to D divided by
two remember because we're adding pairs
of things we only need to add D over two
pairs to get our Vector of length D once
we do that for each of these indices we
are going to add both a sine and a
cosine to our vector and of course these
will both vary based on what our
position is as well as what the output
Dimension is of our Vector as you can
see as we Loop through this line of code
is going to add each of our sine entries
and this line of code is going to add
all of our cosine entries because we're
adding two entries on every iteration of
the for Loop that's why we only have to
Loop through from 0 to D over 2. now
this is just a visualization I made of
what these positional encoding vectors
might look like when we take a single
Row from this visualization it tells us
what would the positional encoding
vector be for the item at this position
in this case we're actually looking at a
30 dimensional positional encoding
vector and here this is what it would
look like at position one
and this is what it would look like at
position one and position two and
position three you can see that we could
do this for all the other rows but I'll
stop there and the number of items in a
sequence AKA what positions can be
possible as well as the dimensions of
the positional encoding Vector can be
varied this for example is that same
visualization using the dimensions
mentioned in the paper attention is all
you need where their positional
encodings are 512 elements along and you
can see with these unique combinations
of sine and cosine we can use these
positional encoding vectors to give our
model information about the position of
an item in a sequence so instead of
adding things like vectors of indices or
even scaled vectors of indices we are
going to take our original word
embeddings and add these positional
encoding vectors that are made up of
pairs of sine and cosine but of course
as we move to the next atom in the
sequence the position in the cosine and
cosine functions are going to increase
so then we end up with our original word
embeddings which gives us semantic
meaning of the words along with a
positional encoding that gives us
information about where in a sequence an
item is because we've taken our word
embeddings and injected information
about the position of that item in the
sequence we can now feed all of our
different words into the model
concurrently this of course speeds up
our processing as well as makes these
models parallelizable which means we can
do a lot more in the same amount of time
so today we learned about what happens
when we take individual word tokens and
process them in a way that prepares them
to be processed by the Transformer model
first we talked about the fact that we
have to take our different words and
make word embeddings this could be
something that is learned by the model
itself or we could use pre-trained word
embeddings like word to back then
because we learned that the model is not
going to process data sequentially but
rather all at one time we had to inject
positional information about our
sequence into the word embeddings
themselves we did this through
positional encoding which takes sine and
cosine waves and gives us a way of
representing each position uniquely with
a positional encoding Vector we then
take that vector and add it to the word
embedding so that now our new update in
betting has both information about the
semantic meaning of the word which we
got from the word embedding as well as
the positional information that we added
using positional encoding now our
vectors are ready to get put through the
Transformer model and because the
positional information is encoded in the
word representation we no longer have to
feed our items in sequentially we can
give the model all of the words in our
sequence at a single time
and once we have those sequences of
words that have both word meaning as
well as positional information we can
then actually start to feed them into
the Transformer architecture and take
advantage of that attention mechanism
we've been talking so much about but I
will save that for the next lecture
that's all I have for you I will see you
next time

 
hello and welcome to your second lecture
on neural networks and optimization last
time we ended by talking a little bit
about loss functions remember loss
functions are metrics that help you
assess the performance of your model
where lower values of the loss metric
are better today we're going to talk a
little bit about optimization which
allows us to actually find the values
for our model that help us have the
lowest loss possible we're going to
start today with something very familiar
which is just a quick review of gradient
descent which we've talked about before
then we're going to talk about some
improvements we can make to gradient
descent in order to make it more
efficient so both in the previous class
as well as in our data science review we
talked a little bit about gradient
descent gradient descent is an iterative
optimization method which allows you to
basically take steps towards a better
solution if you imagine our loss
function as a mountain and we want to
get to the bottom of that mountain AKA
minimize our last one in we can think of
the analogy of a hiker hiking in the
pitch black dark this hiker starts off
at some point and they can't see
anything around them but they know they
need to get to the bottom of the
mountain well one way they could do that
is they could get on the ground and feel
around themselves and then feel which
way is the steepest downhill then they
could take one step in that direction
and then they could do the same thing
feel around take a step
take another step another step another
step if they keep doing this they should
end up at the bottom of the mountain AKA
at a point where our loss function is
really low therefore where our model is
performing really well so the first main
idea behind gradient descent is that we
want to minimize a loss function next
let's talk about how we can actually do
that really quickly let's review what
partial derivatives are partial
derivatives help us figure out how
changing one variable in a
multi-variable function will affect the
output of that function for instance in
this multivariable function where it has
x squared plus X Y plus y squared we
have two variables X and Y and we might
want to ask the question well how does
changing X affect the output of f or how
does changing y affect the output of f
in the case of our models this probably
looks like how does changing one of the
parameters of our models or the weights
of our models how does that change the
outputted loss now I don't expect you to
calculate partial derivative by hand but
I've calculated them for you here and
you can see that this partial derivative
of f with respect to X tells us if we
hold y constant how does changing X
affect the output of the function f and
this derivative here the partial
derivative of f with respect to Y tells
us holding x constant how does changing
y affect the output of f now partial
derivatives are great but we often want
to look at all of them at the same time
and one way we can do that is by shoving
them all in a vector called the gradient
here you can see both of our partial
derivatives just shove together in a
vector to make the gradient because the
gradient is just full of partial
derivatives it basically tells us how we
should change each of the variables in
our model in order to optimize our
function f here I made this beautiful
plot on the right that represents that x
squared plus X Y plus y squared function
here you can see different values of our
function with lower values being in
these darker colors towards the middle
and higher values of the function being
these lighter colors towards the edges
remember our goal is to minimize our
functions and this plot can help us do
that let's look at this specific point
in this case it's X is one y is two and
it happens right here the gradient
basically tells us which direction we
should step in order to get from this
place we are now to a slightly lower
value in this case this red line
represents the gradient which tells us
that direction this means that if we're
starting at this point we should take a
step in this direction in order to help
minimize our function but how big of a
step do we take well this on the left
hand side is the update rule for
gradient descent this formula says that
whatever values we want to update
we should take the original values minus
some learning rate that's just like a
number times the gradient which tells us
the direction that we want to move so
again this gradient is telling us the
direction that we want to move and this
Alpha is telling us how big of a step we
want to take in that direction so for
instance I could take a step that's this
big or I could take a step that's this
big all of that's determined by our
learning rate Alpha and while in two
Dimensions like in this picture it's
really easy to just think of a gradient
as a direction that we take a step in
once we get to have a lot of different
parameters and have these really long
gradients it can be more useful to think
of the gradient as having individual
adjustments that we should make to each
of our parameters each of those partial
derivatives tells us what changes we
should make to that variable in order to
help minimize our function just so you
can see what this looks like I've
plotted here our con tour map with the
addition of something called a gradient
field a gradient field just basically
takes all of the gradients for all of
the points on this map and shows them
using those arrows so basically this
tells you that if I started right here
on my plot I would want to move in this
direction the direction that arrow is
pointing however if I started say here I
would want to move in this direction the
direction that the arrow is pointing so
basically you can see depending on where
you start in this plot we can use the
gradient in order to minimize our
function no matter where we start
alright so that gave us our second idea
the gradient tells us what direction to
move in in order to minimize our
function in other words the gradient
tells us for each of those terms in the
gradient what adjustments we should make
to the different parameters or variables
in our model now I mentioned before with
that update rule that the learning rate
controls how big of a step we take and
just generally we don't want our
learning rate to be too big or too small
if our learning rate is too big we tend
to bounce around Minima and maybe never
even converge with a small learning rate
we're taking teeny tiny little steps so
while we may converge it'll take a very
long time so we sort of want to be in
that Goldilocks zone of having a
learning rate that's not too big and not
too small interestingly we actually
often do not want a learning rate that's
constant it can be really beneficial to
start off by taking bigger steps and
having a bigger learning rate and then
slowly bring that learning rate down as
we approach a Minima that's going to
optimize our function so far we've been
talking about using a constant learning
rate that parameter Alpha is the same
throughout the entire gradient descent
process however sometimes we might want
to use learning rate Decay which
basically starts with a larger learning
rate and then slowly tapers it off
throughout that process
the formula that you can see on the
screen just gives you one example of how
we could taper a learning rate by just
making it a little bit less every time
we process a batch of data all right and
that's the last big idea of gradient
descent the learning rate controls how
big of a step we take too big and we're
going to bounce around too small and
will take forever to reach anything and
also we talked about the idea that we
may want different learning rates at
different parts in the process all right
and that's the big ideas of gradient
descent last we talked about the
learning rate controls how big the
adjustments or steps that we take are
too big of a learning rate and we're
going to bounce around all over the
place too small of a learning rate and
will take forever to get anywhere
we also talked about the fact that we
may want different learning rates for
different points in the process maybe
you want to start with a big learning
rate and then slowly taper it off all
right so let's go over a quick example
with linear regression and this is going
to be the simplest linear regression
because it only has one predictor that
means there are two parameters in our
model the intercept and the coefficient
for that predictor for Simplicity of
math we are going to use the sum of
squared errors as our loss function this
measures the squared error so this
squared distance between our models
guess and the actual value as a way to
measure model performance of course we
know that linear regression makes
predictions using this formula the
predicted value is going to be the
intercept plus the coefficient times our
predictor when I plug those two together
you can see the math here this ends up
being the final version of our loss
function that incorporates the way that
linear regression makes predictions
using that formula that we ended on we
can take the partial derivative of it
with respect to both our intercept and
our coefficient remember there are two
variables that we have control over in
our model are that intercepting
coefficient so we might want to know how
does it affect our loss when we adjust
either The Intercept or the coefficient
now again I don't expect you to be able
to calculate this by hand but you can
trust me that this is the gradient for
both of our variables The Intercept at
the top and the coefficient at the
bottom now of course with gradient
descent we always have to start
somewhere so just to make our math
easier I'm going to start at the point 0
0 and we're going to see how we can
change those two parameters from 0 0 to
something that maybe minimizes our loss
function a little more here you can see
that gradient that we calculated on the
previous screen and here you can see
that same gradient but I've plugged in 0
for both The Intercept as well as the
coefficient if you take a second to look
at what the gradient is it should sort
of make make intuitive sense because if
you think about a linear regression with
intercept and coefficient being zero
that just means we're going to predict
zero for every single data point so I've
moved this gradient over here but of
course we need to plug in some data
points to see what the actual loss
values are to calculate our gradient
here because we're lazy we're only going
to have two data points these are the
data points one one and two three that
means X is 1 and Y is One X is 2 and Y
is 3. so every time I see an X or a y in
this formula I'm going to go ahead and
plug in these numbers for example this
one comes from here this 3 comes from
here same thing here and here and then
of course our X's come from here and
here here and here so you can see that
all we've done is we've plugged in our
two data points into this gradient
function once we've done all of that
math we get out this gradient negative 8
negative 14
this tells us what adjustments we should
make to The Intercept on that Top Value
and the coefficient on that second value
notice that the partial derivative for
the coefficient is actually larger in
magnitude than the one for The Intercept
that means in this specific case we
actually want to change our coefficient
more than we want to change our
intercept all right so let's apply our
update rule for gradient descent
remember in gradient descent the new
values for our parameter are the old
values which we set to be 0 0 minus
our learning rate which I've set to 0.01
times our gradient once we do all that
math we get these new parameters of 0.08
and 0.14 just to do a quick sanity check
let's make sure that our loss function
actually was reduced by taking that step
this top formula shows us what the loss
was when we had our original variables
of 0 0 for both The Intercept and the
coefficient
plugging in our two data points you can
see that our loss the sum of squared
errors was 10. when we plug in our new
parameter values again that's our
coefficient and our intercept and we
plug in our two data points we see that
our loss does actually go down now it's
around 4 where it was before around 10.
so we did in fact help reduce our loss
function by changing our parameters
according to the gradient if you're
someone whose brain works a little
better in code than with math symbols
here's the code from our data science
review classwork where we originally
covered gradient descent this basically
does exactly what we did together using
math but with code here we're creating a
data frame with our two points one one
and two three and here this function
gradient actually defines how we would
make an update using gradient descent
here we calculate the gradient using all
of our data points and here we apply our
update rule by taking our old values for
the parameters and subtracting the
learning rate times the gradient now
that we've talked about the basics of
gradient descent let's talk about a
couple of tweaks that we can make now
with regular vanilla gradient descent
often called batch gradient descent
every time we estimate the gradient to
take a step we use every single one of
our data points in order to do that now
of course that's computationally
expensive but it does make sure that we
are efficiently moving downhill
however sometimes we may not want to use
all of the data points to estimate the
gradient in fact we could even just use
one when we use a single data point to
estimate our gradient at every update we
usually call this stochastic gradient
descent now this is a huge Improvement
in computational efficiency because
instead of processing thousands of data
points every time we estimate the
gradient we only need one however it
does lead to a little bit of zigzaggy
steps that are all over the place
sometimes people refer to this as a
drunk man walking downhill because
that's kind of what your updates will
look like so we get a lot more
computational efficiency but the
estimates of our gradients are kind of
all over the place so often we want a
bit of a middle ground in comes mini
batch gradient descent with mini batch
gradient descent we don't use all the
data points to estimate a gradient but
we do use a handful maybe 32 or 64. and
that way we still have the computational
efficiency of similar to a stochastic
gradient descent but we don't have as
much of that drunk man walking downhill
stuff going on alright quickly before we
move on I just want to update the
notation I'm using in the previous
examples we use this update rule which
said that the new values of our
parameters are the old values minus some
we're still going to do that but I just
want to update the math notation because
as we have more and more parameters it's
really not efficient to write out the
entire gradient like this instead I'm
going to write it like this same idea
new weights are equal to the old weights
minus a learning rate times the gradient
this is just a little bit easier to
write all right so we finished with
gradient descent we learned a bunch of
different things including that the
gradient uses partial derivatives to
tell us how sensitive our loss function
is to changes in our different
parameters in other words it tells us
how to change our different parameters
in order to minimize our loss function
in order to update our parameters we use
this update rule where our our new
weights are equal to our old weights
learning rates determine how big of a
step or how big of an adjustment we make
to our parameters large learning rates
can lead us to bouncing around
everywhere whereas low learning rates
can cause us to take a really long time
to get anywhere we also talked about
some improvements we can make to
gradient descent like using stochastic
gradient descent or mini batch gradient
descent which don't use all of the data
points to calculate the gradient and
therefore improve our computational
efficiency now gradient descent is great
but it does have a couple of problems
one of those problems is that choosing a
learning rate is really difficult and
not only is it difficult even if we use
one of those learning rate Decay
functions that starts our learning rate
off high and then slowly makes it lower
that is really difficult to choose ahead
of time next it can also be difficult to
use this same learning rate for every
parameter in our model maybe for some
parameters we want to take big steps but
for other parameters we want to take
small steps gradient descent uses the
same learning rate for all of our
parameters lastly depending on the step
size gradient descent can actually get
stuck in local Minima and miss a global
minimum or it might get stuck on
something like a saddle point so let's
talk a little bit about some
descent in order to address or maybe
even overcome some of these problems
we're going to talk about a bunch of
different optimizers that improve on
gradient descent and this is sort of the
map of how we're going to talk about
them first let's start off with momentum
just like momentum in physics momentum
and gradient descent basically allows us
to build up speed as we're descending a
gradient our previous update rule said
that our new weights are equal to our
old weights minus the learning rate
times the gradient well the momentum
update rule looks very similar except
instead of using the gradient to update
we use this momentum variable but you
can see that the momentum variable is
basically just a moving average
of all of the past gradients momentum
has this really nice property of
smoothing out the steps that we take
let's imagine that without momentum our
gradient descent asks us to sort of make
this type of Step as we approach the
minimum well that does a lot of
zigzagging back and forth with momentum
because we consider the moving average
of the gradients that have come before
that means the gradients at previous
steps impact how we'll move on this step
so instead of that back and forth back
and forth motion we actually get some
smoother steps because we're taking the
average of the gradients every time we
take a step this also allows us to build
up speed if gradients are consistently
in the same direction this means that if
we have something like a local Minima
and we're going down down down down down
we're building up speed as we go
downhill and that speed can persist and
actually kind of shoot us out of a local
minimum allowing us to hopefully
approach this Global minimum so momentum
with a very simple change to our update
rule allows us to build up speed as we
go downhill in our gradient if all of
the previous steps of our gradient were
in the same direction we're more
confident that we're going in the right
direction however if all the previous
steps were in different directions we're
less confident and momentum sort of
Smooths out some of that back and forth
because of this buildup in speed
momentum can in some cases help us
converge faster than gradient descent
and it has the added benefit of in
certain situations allowing us to escape
some local Minima because we've built up
speed as we've been descending the
gradient all right momentum down now
let's talk about the Ada grad Optimizer
while momentum affected the gradient
part of our update rule Ada grad affects
the learning rate part we talked about
earlier that we may want to change our
learning rate throughout the process of
descending our gradient but in regular
vanilla gradient descent we use the same
learning rate across time we also use
the same learning rate for each
parameter adagrad introduces two really
useful updates first we actually have a
different learning rate for each feature
that means that different parameters in
our model can have big steps and other
ones can have smaller stuff next our
learning rate is adapted that's where
the Ada and Ada grad comes from based on
the sum of the squared gradients that
came before here you can see the change
to our update rule does in fact affect
the learning rate
what we do is we take our Baseline
learning rate Alpha and we divide it by
the square root of the sum of the
squared gradients from all the previous
steps this little parameter here is just
a very tiny number like 0.0001 that
prevents us from dividing by zero at the
beginning of our process because we're
taking the sum of our squared gradients
this number in the denominator can only
ever increase that means that at the
beginning of the process it's going to
be pretty small but as we go through the
process of taking more and more steps
this sum of past gradients is going to
be bigger and bigger which will cause
our learning rate to get smaller and
smaller because we're taking a fraction
where the denominator is increasing this
basically automatically does that
learning rate Decay for us that we
talked about before so instead of having
to set hyper parameters and choose
learning rates ahead of time Ada grad
allows us to adapt during the process so
these are huge improvements we no longer
have to hand tune a learning rate and we
no longer have the same learning rate
for every single parameter but we still
have a little bit of an issue with Ada
grad if you notice in the formula we
said that this sum of the past squared
gradients can only ever increase because
we're taking the sum of by default
positive values
this means that our learning rate decays
and it can actually Decay really quickly
and once our learning rate is really
small that means we can't actually learn
anything about our parameters because
the step sizes we're taking are super
super super tiny and not actually
changing our parameter value well root
means squared propagation AKA rmsp to
the rescue rmsp improves on adagrad by
changing that term in the denominator
whereas before we had the sum of the
squared gradients in the denominator
here with rmsp we have this new term and
this term is the moving average of the
previous squared gradients so instead of
taking the sum where we just kind of
build more and more gradients in
creating an increasing value we instead
take a moving average of the previous
squared gradients this means that as
time goes on previous or earlier
gradients have less and less influence
on what this value is this allows us to
take advantage of having a different
learning rate per each parameter and
adapting the learning rate throughout
the process without having the risk of
having our learning rates slow to
absolutely nothing towards the end of
our process this parameter beta
basically controls that weighted average
we typically set beta to be about 0.9 so
Ada grad's learning rate got smaller and
smaller as time went on because that sum
of squared gradients can only ever get
bigger however rmsp solves that issue by
instead replacing that denominator term
with a moving average of previous
squared gradients meaning that as time
goes on earlier gradients become less
and less important all right so we
talked about how momentum changes the
gradient term in our update rule instead
of just considering the current gradient
we consider a moving average of past
gradients that means that the gradient
at previous steps might affect the
direction we move at this step we also
talked about how Ada grad and rmsp
affect the learning rate of our update
rule we talked about before that we
might want to change our learning rate
throughout the process and we may want
to use a different learning weight for
each parameter both adagrad and rmsp
allow us to do so with rmsp making the
additional Improvement of making sure
that our learning rate doesn't just go
to nothing as the process goes on now
you might think well both of those
things sound really good both momentum
and adaptable learning rates and you are
correct and that's where we put these
all together to get the optimizer atom
atom truly takes the best of momentum
and rmsp and shoves them together the
update rule for atom looks like this and
let's take a look at it piece by piece
first of all it looks really familiar
new Wheats are old weights minus some
adjustment
but you might notice some very familiar
terms like a momentum term and this
denominator term for adapting our
learning rates that's because these
terms are from momentum and rmsp
respectively in atom instead of the
gradient we use the momentum term in
order to choose the direction of our
steps again momentum just takes a moving
average of previous gradients which
allows us to build up speed and smooth
out any zigzagging steps we might be
taking and we also have adaptable
learning rates here you can see we have
the same learning rate rule as in rmsp
where we take our original learning rate
and we divide it by the moving average
of the previous squared gradients of
course using this moving average we
improve upon the Ada grad method where
our learning rate might slowly go all
the way to zero causing us to not be
able to learn anymore one additional
advantage that Adam has is it actually
unbiases our momentum and that adaptive
learning rate term when we start our
process we often initialize our previous
gradients to be zero and so as we start
building up gradients our two terms
which rely on our previous gradients our
biased towards being zero in order to
overcome this bias which really is only
in effect in the first couple of steps
that we take Adam actually uses a
formula listed here and here to unbias
these estimates the t's in this formula
represent the time step or the number of
iterations you can see that the 1 minus
beta to the power of T is going to get
closer and closer and closer to one the
higher T is so that means as time goes
on our adjustment gets less and less
meaning it really only affects those
first few iterations so Adam takes the
best of momentum where we can build up
speed and smooth out zigzagging steps
and of rmsp which allows us to have
adaptive learning rates for each
parameter based on the exponentially
decaying sum of the squared gradients
atom also has the advantage of unbiasing
the momentum and learning rate terms so
that as we take our first few steps
those terms aren't biased towards zero
atom typically tends to be sold as the
state-of-the-art optimizer although
there are improvements that have already
been made to it now I won't say that
Adam is always the right tool for the
job it's pretty much agreed upon that we
often want some type of Optimizer that
allows us to have an adaptable learning
rate but there's certainly certain
situations where stochastic gradient
descent works better or atom works
better or adograd or rmsp work better so
often if you're having issues you may
want to try different optimizers to
review today we just talked about a lot
of optimizers we started out with our
vanilla gradient descent and then talked
about a couple improvements like
stochastic gradient descent and mini
batch gradient descent that allowed us
to make a a little bit of an improvement
in the computational efficiency of
gradient descent then we talked about
improvements we could make to our
gradient descent update rule first we
talked about momentum which allows us to
change the gradient part of the update
rule instead of just using our current
gradient we use a moving average of
previous gradients which allows us to
smooth out the steps we're taking and to
build up speed
then we talked about the fact that using
adagrad and rmsp allow us to have
adaptable learning rates instead of
having to set a learning rate schedule
or just a plain basic learning rate
ahead of time both adagrad and rmsp
individually adapt our learning rate for
each parameter as well as adapting the
learning rate through time when we
combine these two insights we get the
atom Optimizer which takes advantage of
both momentum as well as adaptive
learning rights alright that's all I
have for you I will see you next time

 
hello and welcome to your third lecture
on recurrent neural networks but
actually today we're going to take a
little break from learning about
different recurrent architectures we're
going to talk a little bit about text
processing in the examples that we've
looked at in the previous two lectures
we often talk about the fact that
recurrent architectures are used often
for things like machine translation or
generating sequences of text but
computers can't necessarily understand
words instead we'll have to get them
into a format the computers can actually
understand and that involves a lot of
Concepts and steps that we'll talk about
today let's walk through the basic
process that it takes for us to take
actual written text and get it into a
form that a computer can actually
understand through something like a
recurrent neural network the first thing
might be pretty obvious when you think
about it when we get in a piece of text
we need to standardize it to get rid of
some of the quirks that happen when
written text for instance capitalization
or different modes of punctuation for
instance in this sentence the word the
appears multiple times but the first
time we see it in the original sentence
it's actually with a capital T and we
might want to standardize it to
communicate to the computer that these
are the same word despite the fact that
in the original sentence technically
there's a capital letter once we take
our sentence and standardize it we then
need to break it down into what are
called tokens tokens are individual
units for analysis and typically we
think of tokens as words where each
token in our analysis is an individual
word however there's nothing stopping us
from having individual characters be
tokens or larger sequences of Words
which is actually something we'll talk
about a little bit later so we've taken
an input sentence we've standardized it
and now we've broken it down into
individual units called tokens but of
course like I mentioned computers don't
really understand words they understand
numbers and if we're going to feed all
of this input into something like a
neural network we're going to have to
take these tokens and turn them into
numbers one way to do that that's really
easy is with one hot encoded vectors one
hot encoded vectors are very similar to
dummy variables they're really sparse
vectors of zeros and ones where one
indicates which word we're referring to
for instance in this Vector that you see
on the screen you can see the different
lists of words here we have a vector
where there is a one at the word the
this means that we are referring to the
word the similarly if we wanted to refer
to the word sat we would put a one in
this position and if we wanted to refer
to the word that we would put a one here
so we finally have a way to take words
standardize them process them into
tokens and now compute them into vectors
of numbers that are computer or neural
network can actually understand in real
life these vectors are going to be huge
because there needs to be one entry for
every possible word in our entire
vocabulary so these one hot encoded
vectors could actually have tens of
thousands of different possible entries
this is a very high dimensional Vector
but even if we have tens of thousands of
words represented in these one hot
encoded vectors we know that there are
more words than that that actually exist
and could come up in our text for
instance if we take this sentence that
we've been looking at the Cat in the Hat
sat at the table and ate a bat and
replaced one of the words with a word
we've never seen before that could pose
a problem for these one hot encoded
vectors for instance let's say we change
the sentence to be the Cat in the Hat
sat at the table and ate a platypus or
Perry the Platypus so if we have the
word platypus now and this isn't a word
that is represented in our one hot
encoded vectors how do we encode this
new word well the short answer is we
really can't there's no entry in this
Vector for the word platypus so we don't
have a way of telling a computer that
that's the word that is occurring here
what we can do is we can add a new token
or entry that represents an unknown word
something that we're not familiar with
that's not in our vocabulary here we
have a one here with this unknown token
that indicates to us that we do have a
word here but it's not a word that is in
our vocabulary okay so to review the
steps we've talked about we take an
input text we need to First standardize
it get rid of things like punctuation
and capitalization that make words that
are actually the same seem different
then we take that standardized text and
we convert it into tokens while we could
use individual characters as tokens or
actually longer sequences of words like
two words or ten words typically tokens
will mean an individual word but of
course computers and neural networks
need numbers not text as input we then
take these tokens and convert them into
one hot encoded vectors of zeros and one
and well that's the general process I
wanted to talk about a few miscellaneous
text processing topics that will come in
handy as you're processing your own text
Data first concept is stems stems are a
way of taking words that have the same
root and representing them the same way
in a sentence for instance in this
sentence it says I was sitting on the
bench and I thought that it was a nice
place to sit and think about life here
we see two examples of a same root word
being used slightly differently in the
same sentence for example we have the
word sitting as well as the word sit in
this sentence and in many cases it might
be useful for us to encode that while
they're different tenses these two words
are really representing the same concept
the concept of sitting in order to do
this we can replace words with their
stems their stems being a shortened
version of what they have in common for
instance here we can replace both
sitting and sit with the stem sit to
represent the fact that even though
they're different forms of the word
these both refer to the fact that we are
sitting similarly we have the word
thought and think in this sentence both
of those could have the same stem word
of think to represent the fact that we
are talking about the same concept
despite we have slightly different forms
of the word next let's talk about the
idea of a bag of words text processing
is very complicated but for a lot of
things that we do with text like topic
modeling we actually don't need to
consider the sequential nature of the
words that are being used while
recurrent neural networks are truly
sequential models other types of text
related analysis does not require that
we assume that the text that we have is
sequential bag of words refers to times
when we are thinking of text not as an
actual sequence but of a collection of
words whose order does not really matter
for example with the sentence The Cat in
the Hat sat at the table in ate a bed we
could think of that as a collection of
words where their order doesn't really
matter they're just jumbled together in
a bag that represents for instance the
the topic of the document that we're
talking about this goes along with the
idea that we can process text as either
a sequence where the order does matter
which is how things like recurrent
neural networks treat text or we can
treat it as a set where we're just
looking at a collection of words without
really considering their order if you're
interested something we're not going to
cover in this class but it's very common
in text analysis is latent dirichlet
allocation which is a topic modeling
model that considers text as a bag of
words okay I did a little bit mislead
you to the fact that bag of words have
to discredit all order in a sequence we
actually can encode some order in our
sequence by using something called
engrams engrams are collections of sets
of words that we get by sliding a window
over a text for instance n is just a
variable so I could look at a 2 gram a 2
gram means we're going to look at every
collection of words that is too long or
shorter friends instance as I take this
window of 2 and slide it across my
sentence I get all of these pairs of two
words that I move through sequentially
in my sentence here's the end result of
looking at all the two grams in my
sentence you can see it's every
individual word but also every
collection of two words that I got as I
slid my two window over the entire
sequence this allows us to encode a
little bit of positional information
about our data even though we could
still take all of these words and treat
them as a bag of words we are still
giving ourselves a little bit of context
for each of the words by at least in
this case looking at the word before and
after it and nothing stopping us from
looking at higher ends for instance 3
grams four grams five grams Etc the next
concept we'll talk about is TF IDF or
term frequency inverse document
frequency TF IDF is a way of waiting
words both by how frequently they occur
in a specific doc document therefore
telling you how important that is to the
topic the document's about while also
considering that some words are common
across all documents like I and and the
to calculate the TF IDF we have to
calculate both the term frequency and
the inverse document frequency term
frequency like the name implies is
basically a count of how many times a
word occurs in a given text divided by
the total number of words for instance
we could look at a text like the book
Jane Eyre and look at how many times the
word Jane appears next we need to
calculate the inverse document frequency
this basically looks at how many
different documents in our collection
that this word appears in the more often
a word appears in other documents the
less information it actually gives us
about our specific text for instance
like we talked about before the word and
appears in pretty much every document
and probably occurs pretty often but
because because it occurs in every
document it doesn't really give us much
specific information about an individual
document when we multiply the term
frequency and the inverse document
frequency together we get our TF IDF
when words are fairly common in one
document but not common overall it tells
us that that word gives us a lot of
information about the topic of that
document for instance even though the
word and might appear a lot in a
document it also appears in a lot of
documents and therefore is a little bit
less important however a word like apple
which might appear a lot in a recipe for
apple pie and doesn't appear in a lot of
other recipes does tell us a lot about
that individual document next let's talk
a little bit about cosine similarity
cosine similarity is a similarity metric
that measures the similarity between two
vectors by looking at the cosine of the
angle between them according to cosine
similarity vectors that are close
together in terms of their angle are
similar this is important in text
processing because often we have all
those one hot encoded vectors that we
talked about if we add all of the one
hot encoded vectors for every word in a
document we basically get a vector of
word counts we could look at the
similarity between two documents by
looking at the cosine similarity of
their Vector of word count however we
still want to know the similarity
between two documents even if one is
longer for instance imagine you wrote an
article about machine learning and I
wrote an article that's twice as long
about machine learning even though my
word counts will be higher according to
cosine similarity our word count vectors
would probably be very close together
even though for instance mine might be
this long and yours might be this one
because of the way that cosine
similarity looks at the angle between
vectors we would still consider these
two documents to be very similar while
it's not super on topic for what we're
talking about I think another cool
application of this could be in
something like retail marketing we might
want to compare different customers
based on their purchases and their
pattern of purchases specifically for
instance we might work at a Sephora and
we want to know people who are buying
mostly things like skin care and perfume
versus people who are buying mostly
things like foundation and eyeshadow and
lipstick even if two customers are
different in the amount of things they
purchase we still might want to know
that they have similar patterns like
that they mostly buy lipstick and only
rarely buy face wash so again cosine
similarity works as a way to measure the
similarity between two vectors by
looking at the angle between them even
if one vector has higher values overall
if they're similar in their Direction
and pattern then two vectors will be
very similar we can use this of course
on vectors of word counts for individual
documents to see if they're similar
alright let's get to the main topic here
word embeddings so far when we've looked
at text we've converted them into one
hot encoded vectors as a way to
represent an individual word however
these one hot encoded vectors have a few
problems one of which is that they're
going to be incredibly highly
dimensional because we need to have one
possible entry for every single word in
our vocabulary they also have the
disadvantage that two words that are
similar like book and novel aren't
really similar in the way that they're
represented in comes word embedding word
embeddings are vectors of numbers that
help represent a word sort of like a one
hot encoded Vector but with some
desirable properties that we'll talk
about in a minute one advantage of word
embeddings is they tend to be a little
bit lower dimensional than those one hot
encoded vectors word embeddings might
look a little something like this where
the word gorgeous is represented by this
vector and the word python is
represented by this Factor so that
sounds really nice but how do we
actually get those vectors well it turns
out that we get them with neural
networks in order to create a word
embedding we actually create a fake
problem that we don't really care to
solve but we'll output these embeddings
as a byproduct in the case of word to
vac which is a very common word in
Bedding we are going to pretend that we
want to solve the problem of creating a
neural network that fills in the blank
of a word in a sentence in solving this
problem we're going to use context to
learn the different word and beddings
and of course these word embeddings are
just a byproduct of solving this fake
problem of creating a neural network to
fill in the blank let's look at this
sentence here the cat 8. we can take
this sentence and we can create a neural
network that can predict what a word is
based on the surrounding words for
instance here let's pretend that cat is
the blank word so we want to use the
words the an eight as input and have our
model learn to be able to predict the
correct word the blank word cat to do
this we can create a neural network for
instance in this simple case we are
using only the word right before and the
word right after the blank as input we
input these into the neural network as
one hot encoded vectors then we feed
them through a hidden layer and we can
decide how big we want this hidden layer
to be the output of this network is
going to be a vector of probabilities
for which word it thinks should fill in
the blank of course we would like this
to Output the highest probability for
the actual word cat and while this type
of task is a little bit new the
structure of this neural Network should
be very familiar to you now of course
like I mentioned we don't actually care
that our neural network can perform this
task of filling in the blank what we
really want is this hidden layer in the
middle of our Network this layer is
generating the embeddings that we
actually care about this is why choosing
the dimension of your hidden layer is
important because that will be the
dimension of your word embeddings the
bigger you make your hidden layer the
higher dimensional your word embeddings
will be consider review we can take a
neural network and basically solve a
fake problem of trying to guess which
word should go in the blank in a fill in
the blank the input the context words in
this case just one word before and one
word after as one hot encoded vectors we
send them through a hidden layer and
eventually we output what we hope is a
predicted probability where it guesses
the correct word to fill in the blank
but we don't really care about the
actual output of this Network what we're
after is these embeddings that come from
our hidden layer now there's actually
two ways that people will Design these
networks that produce our embeddings the
first one that we just did is a
continuous bag of words The Continuous
bag of words does what we just did it
tries to predict a blank or a Target
word based on the context of the words
around it the other way that we could
build our Network in order to generate
these embeddings is using the skip gram
method in a skip gram we're actually
trying to predict the context words
based on a Target word for instance
taking our same sentence the cat ate we
might put the word cat as a one hot
encoded Vector into our model and want
it to Output both the and eight we're
putting in the Target word and we're
asking it to generate what the context
words are around it even though we have
a slightly different structure to our
neural network what we're after is still
that hidden layer which is going to give
us the embeddings for our different
words words that consider the context of
words around it here's the word
embedding for the word King based on the
glove model which like word to back is a
very famous word embedding model you can
see that the hidden layer in this
network was very long so we have this
very long word embedding to represent
the word King we often do need High
dimensional embedding so that we have
more flexibility for words to be similar
to each other in different dimensions
basically the more Dimensions we have
the more flexibility a vector has to be
similar to one word or another and when
we represent words with their word
embeddings we get some really cool
properties that we can use for instance
let's consider these four words king
queen man and woman remember that points
on a graph can basically represent a
vector that moves from the origin to
that point and one thing to note about
all of these vectors is that relatively
they're pretty close together and this
makes sense because all four of these
vectors are representing people one nice
thing we can do with word embeddings at
least if they work properly is that we
can do vector algebra with our different
words for instance here we can see the
same Vector that gets us from King all
the way to Queen is the exact same
Vector that gets us from man to woman
essentially this Vector is representing
gender or at least the change in gender
from going from man to woman another
cool thing we can do with vector algebra
with these Warden vettings is subtract
them this Vector here represents the
word King Vector minus the word man
Vector semantically we can think of this
as saying take the concept of King but
subtract out the gender of being a man
colloquially we might represent this
concept as royalty because the
difference between a king and a man is
the Royal nature of that person the cool
thing about word embeddings is we can
take this Vector that essentially
represents royalty King minus man and we
can add the vector woman so we're saying
take the concept of King subtract the
gender of being a man and add back the
gender of being a woman when we do this
we actually end up with the vector Queen
and this makes sense when we take the
concept of King subtract being a man an
ad being a woman the concept that
results is the concept of being the
queen to give you another example let's
look at the comparison between King and
man and queen and woman we already said
that when we go from man to King we're
basically adding the concept of royalty
and this royalty Vector is the same one
that takes us from woman to Queen
because when we take a woman and add the
concept of royalty we get a queen and
just like before we can take the concept
of Queen subtract the concept of woman
to get this vector and then add the man
Vector to result in the word King so
again we took the concept of Queen
subtracted the concept of being a one
and added the concept of being a man to
get the vector King another thing that's
nice about these Warden buddings is that
words that semantically mean things that
are similar to each other should be very
close to each other in the embedding
space now of course our embedding space
is going to be a lot higher dimensional
than this two-dimensional example I have
here but even here we can see clusters
of words like meat tidy and clean all
being in the same region because they
refer to very similar Concepts similarly
here we have a group of words like
unkempt gross and messy these are all
very similar to each other and therefore
close to each other but are very far
away from neat clean and tidy because
they mean very different things now we
talked a little bit about word to fact
and glove which are two very common word
embedding models we could also always
build our own embedded model whether on
its own or as a part of a neural network
that we're training these embeddings
take take our one hot encoded vectors
which we created by standardizing
tokenizing and then one hot encoding our
text and turn them into vectors that are
no longer sparse they're no longer full
of zeros but that represent the semantic
meaning of a word these embeddings will
typically have really nice properties
such as the ability to do all that
vector algebra that we talked about
before and certainly that words that
mean similar things will be near each
other or have very similar embeddings
alright that's all I have for you I will
see you next time

 
hello and welcome to your first lecture
on neural networks and optimization
today we're going to cover some of the
basic neural network components as well
as their architecture some of this might
be a little bit of review from 392 but
we're going to dig a little bit deeper
first let's talk about neural network
architecture the first component we need
to learn about is a node a node usually
represented by a circle in drawings of
neural networks is just a container that
holds a specific value for instance
here's a node that is holding the value
5.27 the next component we need to talk
about are weights weights connect two
nodes together in order to get the value
in the second node we take the value in
the first node multiply it by the weight
and add it to the second node but nodes
don't have to have just one input for
instance here on the right hand side our
node actually has two inputs coming into
it to get the value of this node on the
right hand side we take this value 5.8
27 multiply it by its weight and then we
take this value 2.0 multiply it by its
weight and add it to that node so again
this node has two sources of information
it has inputs coming in from this node
and it has inputs coming in from this
node together we get the value of this
node is
2.527 because we get a 0.527 from this
node multiplied by this weight and we
get a 2 from this node multiplied by
this weight in addition to the previous
nodes times their weights we also add
something called a bias to our node
biases move the value of a node either
up or down no matter what the weights
and input values were from the previous
nodes you can think of this term as
similar to an Intercept in a linear
regression together nodes weights and
biases make up the core structure of a
neural network here's a little bit more
of a complicated example that has three
layers of nodes with a lot of different
weights and biases just to note when we
display a neural network we typically
don't show the biases but they're always
there so our diagrams of neural network
structure often look more like this than
the previous example the next term we
need to be familiar with are layers
layers are groups of nodes at the same
level of depth for instance in the red
you can see that these nodes make up a
layer in our neural network so do these
nodes and this node that means that this
neural network has three layers every
neural network needs to have at least
two layers the first is an input layer
the input layer determines what data is
going into the network neural networks
also have to have an output layer the
output layer is where our neural network
is making a prediction anything between
the input and output layer is called a
hidden layer and neural networks can
have multiple hidden layers typically
when we refer to deep learning we're
talking about neural networks that have
at least two hidden layers to review a
little bit of math notation the value of
a node is a linear combination of all
the nodes in a previous layer that are
connected to it we typically will use
this type of notation where the weights
are put in a vector times the input
values plus a bias we can also see it in
this dot product form
to give an example of this let's look at
this red node this red node has four
inputs coming from the previous layer X1
X2 X3 and X4
to get the value of the red node we're
going to have to take each of these
input values multiply them by their
respective weights and add a bias
basically you can see what's happening
using this formula here at the bottom
these formulas that I introduced before
are basically just shorthand for What's
Happening Here
well there are a ton of different
activation functions that you can use I
want to go over a couple of the most
common ones with you the first one is
that boring one we talked about before a
linear activation a linear activation
function takes a value and returns that
value so basically it doesn't do
anything next we have a sigmoid
activation sigmoid activations are very
useful because they take values and no
matter what those input values are they
squish the output to be between 0 and 1.
next we have a similar activation
function called the tan H function like
the sigmoid activation function tan H
squishes all of our input values into
outputs that are between an upper and a
lower bound however the bounds this time
are between negative one and positive
one last but not least we have the
rectified linear unit or rate Loop
regular activations are really cool and
they're inspired by the way that our
brains work
if you've ever taken a neuroscience
class you may have learned that neurons
only fire once they reach a certain
level of activation
rayloactivation functions mimic that by
taking values that are negative and just
squishing them all to zero and returning
positive values as is this means that we
have to reach a certain level of
activation before rayleu will return a
positive value this means that if a node
has a negative value Rayleigh will turn
that value into zero meaning it has no
impact like the relu activation function
for positive values leaky relu Returns
the value itself
for negative values leaky relu instead
of returning exactly zero will return
that value times some small parameter
Alpha this basically reduces the impact
of that number without driving it to
exactly zero we can apply activation
functions to any of our nodes previously
we said the value of this node was going
to be all of the input values times
their respective weights plus a bias and
that's still true but on top of that we
are going to apply an activation
function represented by this F of all of
that input this means that we take that
previous value that we calculated and
send it through some type of activation
function maybe a relu maybe a sigmoid
maybe a tan H then that value is what
gets sent through to the next layer of
the neural network Let's do an example
with actual numbers first we need to
take all of the values from the previous
layer that are connected to this node
and multiply them by their weights so so
we get 1 times
0.25 now these are done plus 2 times 0.1
now these are done then we get plus 0
times 1.7 these are done and last but
not least plus 0.5 times negative 1.
then of course we need to add our bias
which in this case is negative 0.5
altogether this equals negative 0.55
so now we've done this inside part but
we need to then apply an activation
function if we were applying a linear
activation function we would just return
this value negative 0.55 as is but let's
try applying a sigmoid activation
plugging in negative 0.55 into the
sigmoid activation function which is 1
over 1 plus e to the negative X we get
about 0.366
so this is going to be the output value
of this red node it's a linear
combination of all the previous nodes
plus a bias fed through a sigmoid
activation function now that we've
learned the basic architecture of neural
networks let's look at some familiar
models from 392 as neural network first
let's start with linear regressions a
linear regression is literally just a
linear combination of values so our
neural network structure is going to be
very simple
here you can see we have a layer of
input values in this case we're using
age sex salary and rent to predict how
much someone is spending on video games
each of these terms is going to have a
coefficient which we now call a weight
and of course we're going to have an
overall intercept which we in a neural
network Now call a bias
all of those values are going to come
together in our output node and because
this is linear regression we're going to
use a linear activation however if we
change the activation a little bit to be
a sigmoid activation we get a logistic
regression here we have the same input
values but instead of predicting video
game spending we're predicting whether
or not someone is a twitch streamer
before our input values of age sex
salary and rent are multiplied by their
weights a bias is added but then that
value is sent through a sigmoid
activation to get an outputted predicted
probability as you can see neural
networks don't have to be super
complicated they're just a way of
defining the structure of a model even a
model that's really simple like a
logistic or linear regression now that
we know a little bit about the structure
of neural networks we need to revisit
the concept of loss functions the
architecture of your neural network
determines the structure of your model
the loss function determines how you
measure the performance of that model as
a reminder loss functions are metrics
that measure the performance of your
model where lower values mean better
model performance for instance if you're
predicting a continuous value like
height or weight or money you might use
something like a mean squared error or
the mean absolute error the mean squared
error takes the difference between the
actual value and our predicted value
squares it adds it all together and then
divides by the number of samples
similarly the mean absolute error takes
the actual minus predicted or the error
takes the absolute value and then
divides it by the number of samples you
can think of mean squared error as the
average squared difference between your
model's guess and the actual value the
closer your model's guesses are to the
actual value the lower your mean squared
error is going to be you can also think
of mean absolute error in these terms
mean absolute error is the mean distance
between your model's guess and the
actual value if we're predicting a
categorical variable you might use
something like log loss which is also
called binary cross entropy log loss
measures how our prediction matches up
with the actual value for instance if we
have a data point that is in category 1
and we predict that there's a 90
probability that that data point is in
category one that's really a great
prediction however if our data point is
in category 0 and we predict that
there's a 90 chance it's in category one
that's a really bad prediction this is
what log loss or binary cross entropy
measures you may also see a hinge loss
function hinge loss functions basically
ignore or bring to zero very small
errors so if you're very close you get
zero error last let's talk about
Universal function approximation
basically means that no matter how
complicated the relationship is between
the inputs to your model and the output
of your model if your neural network is
sufficiently complex enough it can mimic
or figure out what that relationship
between inputs and outputs are
and this is why neural networks are so
powerful and why we're basically going
to spend a whole semester learning about
them neural networks have a unique
capacity to be incredibly complex which
can lead to its own problems but allows
us to model very complicated
relationships between inputs and outputs
that are associated with important
problems like image recognition or Sound
Processing
all right that is all I have for you I
will see you next time

 
hello and welcome to your first lecture
on generative models
so before we get into the technical
details of generative models I wanted to
give you some examples of what people
have done recently with generative
models one of the most common things
you'll see people talk about in blog
posts or tutorials is using generative
models to generate new images like for
instance new faces on the left you can
see a model that has been trained to
generate new anime faces and on the
right hand side you can see a model that
generates human faces and there are many
types of models that do this including
one like this that creates human faces
although they're a little blurry around
the edges and we don't just have to
generate faces you're probably familiar
with reading news stories or tweets
about models like this that can actually
take in a prompt and generate images for
you according to that prompt you can
also use generative models to do in
painting where we basically occlude part
of an image and ask the model to fill in
what it thinks should go in that missing
piece well it's not perfect you can see
even models like this are pretty good at
figuring out what went behind the
occluded region and of course we are all
familiar with generative models like GPT
that generate text in response to a
prompt very recently someone set up a
generative model to write an infinitely
looping Seinfeld episode and then use
generative art to basically illustrate
it and as as students who are taking
Tech heavy courses you're probably
familiar with github's co-pilot which is
a generative model that creates code for
the more artsy among you generative
music models are also huge right now for
instance there's a model called Muse Gan
that generates new music as well as a
project from Google called magenta that
does generative music AI even the video
game industry can use generative models
for instance for level design to
dynamically create new levels and we
don't just have to create completely new
images style transfer is a huge part of
generative models as well where we take
the style of one image and map it onto
another on the left you can see this
type of model with faces here we're
taking the style from the image on the
y-axis and we're mapping it onto the
pictures on the x-axis to create a
combination where the image has the
style of the style photo but the actual
content of the actual content photo you
can also create some really cool things
with this for instance I'm a big breath
of the wild fan and this is a picture
from breath of the wild that has been
used with a style transfer model to
basically recreate the image in this
style of starry night we can also use
generative models to modify images for
instance this is a generative model that
actually ages or de-ages a photos you
can see what you looked like maybe as a
teen and as a 60 year old we can also
use generative models to upscale images
basically take a blurry image and up
sample all of the pixels to create a
more sharp image you know in movies when
they always say enhance and they take a
blurry photo and they zoom in and
suddenly it's like this perfectly full
resolution photo well that's basically
what this can do another example of this
was something that my husband who's a
video editor actually used this allows
you to take video that maybe looks a
little bit blurry and upscale it so that
it looks like a sharp image and of
course you've probably seen generative
modeling in your day-to-day media for
instance Star Wars has been using a lot
of deep fakes in order to de-age or even
map one actor's face onto another actor
for instance when they did Luke
Skywalker in the book of Boba Fett or
the Mandalorian
and even if you're not familiar with
that example I'm sure you're familiar
with Tick Tock filters traditionally
Tick Tock filters use something called
face mesh this is basically a model that
uses AI to map a bunch of different
points usually a couple of hundred onto
your face so it sort of learns where
different things on your face are and
then it uses that mesh in order to place
things on your face things like makeup
or a bunny rabbit or blush or something
like that and while these look pretty
cool and are really fun like Snapchat
filters they do tend to lack a sense of
realism you can tell they're clearly a
filter and not just because there's a
bunny on top of your head but because
when you move around sometimes it takes
the filter a little while to catch up or
if you move your hand in front of your
face it maybe Maps some eye makeup or
blush or lipstick onto your hand rather
than your face because it gets confused
about where your face actually is but
applique locations like Tick Tock have
also been experimenting with generative
filters for instance if you've ever used
the Bold glamor filter on Tick Tock you
might notice that it looks a lot more
realistic and moves a lot better than
these traditional face mesh filters
that's because these filters are using
generative AI in order to recreate every
single Pixel of your video rather than
just mapping something like makeup or an
animal on top of your video this allows
you to move around or wave your hand in
front of your face move your skin around
and the filter still works because it's
learned how to take your raw video and
map it to a version that has whatever
effect the filter wants to use and what
we've talked a lot about very cool uses
of this generative AI there's also some
nefarious uses for instance scam callers
are now more and more using gender
iterative AI to clone people's voices so
that if they get a sample of you
speaking they can then make a model that
will generate you saying whatever it is
they want you to say for instance
calling up your grandparents and telling
them you need ten thousand dollars in
order to get out of jail and this isn't
the only concerning use of generative AI
for instance we can create videos or
images of people saying or doing things
that they didn't actually say or do and
at this point it can sort of be hard to
tell what's real and what's fake so
we've seen that generative AI is
incredibly popular and it's definitely
integrated itself into our daily lives
so let's learn a little bit about how
this generative AI actually works so
there's basically two main types of
generative models the first type of
model is a density estimation model the
goal of these types of models is to take
in some data and learn a probability
distribution that describes that data
for instance we could fit a normal
distribution to a sample of data once we
know what that actual distribution is we
can then pull new samples from that
distribution thus generating new outputs
the other main type of generative models
are sample generation models these
models have a similar goal we're going
to take a bunch of different inputs and
we are going to somehow learn to just
create new outputs and we don't
necessarily have to create a probability
density function in order to do this and
with all of the examples we talked about
you might be thinking wow generative
models are so cool and so complicated
but it turns out you've already learned
some generative models in cpsc 392.
those models are naive Bayes and
gaussian mixture models let's look at a
really cool example of using naive Bayes
as a generative model this comes from
the book generative deep learning by
David Foster in this example we're going
to look at a bunch of different faces
faces and these faces have a bunch of
different features you can wear glasses
or sunglasses or no glasses you can have
different hairstyles different colors
and different shirt Styles and we can
use naive Bayes as a generative model
here to create new faces in order to do
that we have to learn a distribution of
our data in this case we're going to
learn a discrete distribution that looks
at things like the probability of having
glasses versus the probability of having
sunglasses versus the probability of
having no glasses for instance here 22
out of 50 faces have glasses so the
probability of glasses is 0.44 the
probability of having sunglasses is 17
over 50 or
0.34 and the probability of having no
glasses is 11 out of 50 or 0.22
and we could do this again for each of
the different features like hairstyle
hair color and shirt color and now that
we have these probabilities we can use
them to generate new samples and we're
going to do so by making the naive
assumption in naive Bayes which is that
all of our different features like
glasses hair shirt are all independent
because of this Independence we can
actually generate new samples that have
never been seen before in our original
training data for instance in order to
figure out if a new sample is going to
wear glasses all we have to do is sample
from this probability distribution which
tells us that 44 of people wear glasses
34 wear sunglasses and 22 percent wear
none using all those probabilities we
can then generate new samples like these
Thus We've turned our naive Bayes model
into a generative model but this data is
pretty special it's labeled data someone
has gone through and recorded whether or
not p people are wearing glasses or
sunglasses and what type of shirt
they're wearing and this is often not
the type of data that we will have in
real life so let's look at a similar
example where instead of having labeled
data of different features of faces we
actually just feed the model the picture
of the face itself here you can see
pixelated versions of these faces and
like before we're going to use the naive
Bayes model in order to generate new
images but instead of feeding it
individual features we're going to just
give it all of the different pixel
values you can then learn what the
probabilities of each pixel being a
different color are and Sample new
pixels to generate new faces if we did
that it might look something like this
which doesn't actually look that great
this is because our model is not taking
to account the fact that nearby pixels
are probably going to be similar colors
and represent similar features and this
is something we might I want our
generative models to be able to do next
let's talk about gaussian mixture models
and how those could be generative models
remember in gaussian mixture models
which we've previously learned mainly
for clustering we assume that the data
that we observe in a population is made
up of different mixtures of two or more
groups of different populations for
instance here we have some data on how
much money people spend on fast food per
week we have one distribution which
seems to be pretty heavy fast food
buyers and another distribution that
seems to buy a little bit less food on
average but is still occasionally
spending a little bit of money we're
assuming that the data we observe this
purple line is a combination of these
two groups of people are heavy Spenders
and are not so heavy Spenders and back
in data science this is pretty much
where we stopped we wanted to figure out
which data points were in which
distribution but notice we are explicit
recently modeling a probability
distribution that describes the data we
have so once we do that we can just
sample from it for instance now that
we've learned these two distributions
the red distribution and the blue
distribution we can just sample new
points from them for instance I might
take this sample from the red
distribution and then this one and then
this one and then this one from the blue
distribution and this one from a blue
distribution and because we explicitly
modeled in this case normal
distributions on our data we can
actually generate new data that we
haven't observed in our original data
set before thus generating new data in
two Dimensions it might look something
like this here's our original samples of
groups of data and these red points
represent the new generated values that
we see well they're very similar to the
data we learned because of course we
want realistic samples not all of these
are actual points that we've observed
before thus we're generating entirely
new samples based on the distributions
that we learned so it turns out you
already knew how to do generative
modeling both gaussian mixture models
and naive Bayes do explicit density
estimation by learning problem ability
distributions that describe our data in
gaussian mixture models we assume normal
distributions whereas in naive Bayes we
are typically looking at probability
distributions that describe the
different probability of discrete
variables like glasses Sunglasses or no
glasses however these two models are
pretty simple and therefore they're
Limited in the types of generation that
they can do so let's talk about another
generative model that explicitly does
density estimation the variational
autoencoder autoencoder should be a bit
of a review in the previous lecture on
auto encoders we talked about them as
compression we feed in an input the
autoencoder then creates a hidden
representation that then can be used to
recreate that input the job of an
encoder is to learn a function that
takes our raw input and Maps it to that
hidden representation the job of the
decoder is to take that hidden
representation and regenerate the input
Auto encoders when we were using using
them for compression what we were really
after was this hidden representation
because it represented our original data
in a hopefully lower dimensional way the
success of an autoencoder was measured
by its reconstruction loss how well the
output spit out by the model matched the
input a perfect model would be able to
perfectly recreate the output based on
the input when learning about
autoencoders we also talked about how
sometimes autoencoders are over complete
meaning that their hidden representation
isn't necessarily a lower dimensional
representation compared to the input and
output however in order to avoid
overfitting and still hopefully do some
useful representation we talked about
different ways that we could modify our
model so that it would still learn a
useful representation despite the fact
that the hidden representation was
higher dimensional than the input and
the output one of the ways that we did
that was by penalizing derivative of our
encoder by penalizing the derivative of
our decoder we basically asked our model
to make sure that inputs that were
similar would also have similar hidden
representation when the derivative of
the encoder with respect to the input is
high it means that a slight change in
the input would lead to a large change
in the hidden representation and we
wanted to punish that to make sure that
our model represented similar inputs
with similar hidden representations all
right now that we've reviewed
autoencoders let's talk a little bit
about variational autoencoders which
uses similar Auto encoder structure to
build a generative model one thing
that's really nice about well-built Auto
encoders is that they have things called
concept vectors concept vectors are
things like a smile Vector where as we
move along the vector we can get more
and more of a smile like in this pretty
creepy image similarly if we move
backward along the vector we get less
and less of a smile this is really
similar to a concept we talked about
when talking about language models for
instance when we had word embeddings we
had vectors that represented context for
instance we looked at a vector like this
that represented the idea of a king
minus the idea of being a man put
together this is the vector that
represents the concept of being Royal
thus as we move along the direction of
that Vector we get more and more Royal
going from man who's not very Royal at
all to King who's pretty darn Royal so
how do we build one of these variational
autoencoders typically in an autoencoder
we input something whether it's a vector
of numbers or an image and we grab a
hidden representation where we take that
image and represent it with a vector of
numbers we then take that Vector of
numbers and ask the decoder to recreate
our original input whether it's an image
or just a bunch of data variational
autoencoders do something very similar
except instead of just a vector of
numbers as the hidden representation we
actually use a probabilistic hidden
representation this means when we have
an input our encoder actually learns a
distribution of likely features for
instance in this photo we might have
this distribution for a smile and this
distribution for glasses so instead of
single numbers we actually have our data
being represented by a bunch of
different distributions the idea behind
variational autoencoders is that if we
learn distributions for each of these
features we should be able to sample
slightly different values from that
distribution to generate a new random
hidden representation and any sample
from these distributions should create a
very similar image to our input for
example here you see these same
distributions that we saw on the
previous screen where from the input
image we learned that there's probably a
very slight smile and the gender is
probably slightly more masculine we then
sample different values from each of
these distributions and the red dots
represent the samples for this and the
yellow dots represent the samples for
this both of these are turned into
vectors of hidden representations and
both are sent through the decoder the
decoder then takes all of these values
and recreates an image because an
autoencoder is still judged on whether
or not it can recreate the original
image we expect that both of these very
slightly different samples are going to
generate incredibly similar images both
that are similar to the original input
face this is similar to penalizing the
derivative of a typical autoencoder when
we're penalizing the derivative of a
typical autoencoder we're telling our
model that slight differences in the
input should not lead to huge
differences in our representation a
variational autoencoder on the other
hand says that slight differences in our
hidden representation should still
produce very similar output so
variational autoencode coders are very
similar to our typical autoencoders in
regular Auto encoders we represent our
data using a latent or hidden
representation this representation is
usually just a collection of numbers
that are then used by the decoder to
recreate our input variational
autoencoders on the other hand don't
have just stable numbers as their hidden
representations rather we learn
distributions of values to represent our
hidden representation thus variational
autoencoders are more probabilistic in
their hidden representation the job of
the encoder is to learn what is the
hidden representation given my input and
the job of the decoder is to say what do
we think my input was based on the
hidden representation to review what
this process looks like we are going to
take an input like an image and send it
through our encoder after training our
encoder is going to learn a probability
distribution across the hidden
representation so instead of learning a
single value it'll learn a distribution
of values that are all nearby to each
other then we can take various samples
from this learned distribution and send
them through the decoder no matter what
sample we have we expect that our
decoder is going to generate something
very similar to the input to our model
thus the model has to learn that data
points that are near each other in the
hidden representation need to create
similar output so let's look a little
bit more about how this structure
actually works remember in a typical
autoencoder we have an input we send
that through an encoder which could be
multiple layers of dense or
convolutional neural net layers we then
come to a learned hidden representation
we feed this representation to the
decoder which hopefully spits out an
output that looks a lot like our input
in a variational autoencoder we add a
couple of new steps we still have an
input this is going to be an image or
some data that we're feeding into our
model we then send it through an encoder
but instead of just learning an
individual vector numbers our encoder is
going to learn a distribution in
variational autoencoders this is
typically a multivariate normal
distribution which has a couple of
parameters including the mean of the
distribution mu as well as the standard
deviation of the distribution so Sigma
squared as well as the variance of the
distribution Sigma squared so instead of
learning one individual Vector of
numbers for the hidden representation
our encoder actually learns two and just
like with our typical Auto encoders this
encoder could actually be multiple
different layers of convolutions dense
layers Etc in variational autoencoders
we are not going to learn a single
Vector of numbers as our representation
but rather a vector of means and
variances that describe the multivariate
normal distribution that is our hidden
representation we then need to sample
from that distribution and one way to do
that is to just generate some random
noise to add to our mean and our variant
this noise is completely random and not
something that is actually learned by
the model when we combine these three
things by taking the mean adding the
standard deviation multiplied by the
noise we basically get a random sample
from our hidden representation
distribution again the mean describes
where this center of our distribution is
the variance describes how wide and
spread out the distribution is and the
noise basically generates some
Randomness to sample a random value from
that distribution this is our final
hidden representation like in a typical
autoencoder it is fed into a decoder to
then hopefully reproduce the input as
our output and just like in an
autoencoder this decoder could be
multiple stacks of different layers just
like an auto encoder the goal of a
variational autoencoder when training is
to recreate our output based on our
input by first encoding and then
decoding our input in a variational
autoencoder the encoder first learns a
mean and variants for our multivariate
normal distribution then samples from
that using this noise value and adds
them all together to create our final
sample from our hidden representation
this is then fed into the decoder the
decoder's job is to take this sample of
our hidden represent rotation and
process it in order to create our output
and the goal of our output is to be as
similar as possible to our input if your
brain works a little bit better in code
than in pictures here's some pseudocode
for how a variational autoencoder works
first our encoder is going to learn the
mean and the log variance of our
multivariate normal distribution the
reasons for learning the log variance
compared to the actual variance are more
mathematical than conceptual so we'll
skip over them here once we've learned
the mean and variance of the Hidden
representation we can then sample from
it by using this formula that combines
the mean the variance and a random value
in order to generate a sample from our
distribution for instance if you see
this image on the right that shows a
multivariate normal distribution we
could grab a sample like this from that
distribution now that we have our sample
we can send it through the decoder which
is a function that is trying to learn
how to recreate our input based on this
sample from the hidden representation
because we're taking random samples from
a distribution we expect that similar
but slightly different hidden
representations are going to generate
very similar images because remember the
goal of our variational autoencoder is
still to recreate our original input but
we expect different random samples from
the Learned distribution to generate
that same input so hidden
representations near each other should
generate similar output now that we've
talked about the structure of a
variational autoencoder let's talk a
little bit about how it's trained when
we learn about autoencoders we learned
that they use something called
reconstruction loss in order to train
reconstruction loss basically just
compares the inputs to the outputs
element wise and sees how similar they
are a perfect Auto encoder would have a
very low reconstruction loss because the
inputs would be very very similar to the
output and variational autoencoders are
no different we're still going to
consider this reconstruction loss
because we still care that our inputs
and outputs are similar to each other
but we add another penalty or
regularization term to this loss
function we're going to use the kale
Divergence of the representation we've
learned that multivariate normal
distribution compared to a standard
normal distribution remember that the
variational autoencoder is going to
learn a mean and a variance for our
multivariate normal we're going to
compare that to a standard normal that
has mean of zero and variance of one
remember the KL Divergence is a way to
measure how much information is lost
when we represent one distribution as
another for instance in our math review
lecture we talked a little bit about how
much information we would lose if we
represented this distribution of car
colors using a uniform distribution KL
Divergence is basically a measure of how
similar two distributions are if we lose
basically no information by representing
one distribution as another then
basically these two distributions are
practically the same thus this kale
Divergence term encourages the normal
distributions that we learn to be
similar to a standard normal
distribution and why would we want to do
that well it makes the hidden
representation of our data have really
attractive features here on the screen
you can see some examples of what
happens when we train a variational
autoencoder with only the Reconstruction
loss in mind so all we care about is
that the inputs and the outputs are
similar here the data is representing
individual images and you can see that
different numbers are kind of close to
each other there's little regions of
like ones versus twos versus zeros
versus sixes in the middle we have an
example of an autoencoder trained only
on the kale Divergent well this has a
very symmetric and beautiful normal
distribution of points we didn't really
learn anything about our input basically
inputs that are similar are not
necessarily really close together in
their hidden representation and neither
of these are exactly what we want which
is why we combine them we consider both
the Reconstruction laws as well as the
kale Divergence when we combine them we
get this really nicely behaved latent
space where items that are similar tend
to be very close together in their
hidden representation but we have these
nicer looking distributions compared to
when we trained only on reconstruction
loss when we train an auto encoder
there's nothing telling me autoencoder
that similar input should have similar
hidden representations for instance here
in this example you can see that our
square and our rounded triangle have
very similar hidden representations
despite being very different shape
ideally what we would like is for
similar inputs to have similar hidden
representations for instance like this
where the triangle and the rounded
triangle have very similar hidden
representations another thing we might
want from this latent space our hidden
representation is for us to have a
continuous space that represents real
shapes for instance here we've learned
representations of our different shapes
where circles are near circles and the
hidden representation but if we sampled
a random spot between them we would get
something that is not actually a shape
and we might want to learn a space where
all the points in that space are actual
shapes for instance in this one we have
our triangles our circles and our
squares but if we sample something
between all of those we get basically a
valid shape that is a combination of a
triangle a circle and a square and if I
sampled another Point here I might get
something that's a mesh between a circle
and a triangle or here between a square
and a triangle and this property of the
latent Space is really useful for
example here's a visualization of the
latent space for a variational
autoencoder trained on the mnist data
set you can see that as we sample from
different points in the space we
seamlessly move between digits for
example here you can see we move from a
zero and we get closer and closer and
closer to something that looks like a
six and then a five and then maybe like
a nine or an eight and you can see with
this continuous latent space that we're
able to basically morph between objects
for instance here we have something that
is very clearly a nine and as we move in
any direction you can see it slowly and
slowly morph into something else like
for instance in this direction we're
morphing from an 8 to a 3 to a two and
as we move through this space we
basically move from more and more like a
nine to more and more like an a to more
and more like a three in a continuous
way and I don't know why you'd want to
do this with digits but this is useful
because then we can ask okay I want
something that's you know halfway
between an eight and a three and we can
actually just sample something like that
that's between an eight and a three so
variational autoencoders basically learn
a hidden representation or a latent
space that has some really desirable
properties the main characteristics of
these properties are representation
realism and smoothness an ideal latent
space will have good representation that
means that we can take an input encode
it and then decode it and accurately
reconstruct our original input this is
something that it shares with
traditional autoencoders but the ideal
latent space of a variational
autoencoder will also have realism
realism means that we can sample any
point in our latent space and we should
get something that looks like a valid
input for instance here in this
variational autoencoder trained on
shapes we should be able to sample from
any point and get something that looks
like a valid shape finally an ideal
autoencoder will have good smoothness
smoothness means the inner latent space
we can basically pick two different
points and gradually move between them
morphing from the first point to the
second for instance in this example from
a variational autoencoder trained on
human drawings we can take this drawing
of a cat's face and this drawing of a
pig's body and slowly morph between them
creating things that look like valid
drawings at every step for instance here
we start with the cat and then we move
over as we move through the latent space
to things that are more and more
pig-like compared to the original cat
face and as we move all the way here we
finally get something that looks pretty
like our original Pig this smoothness
means that we can sample this latent
space all the way from cat face to Pig
body and get accurate represent stations
that move from very cat face like all
the way to very Pig body like alright
before we move on I just want to make
one quick technical note we've been
talking a lot about variational
autoencoders that process images and as
you can imagine the encoder and decoder
both have convolutional layers in them
however there's some differences
compared to how we build typical
convolutional nets for classification
and how we build our convolutional
encoders and decoders in a variational
autoencoder first convolutional neural
networks for learning features preferred
Max pooling to strides in order to down
sample our image that's because in the
convolutional networks that we were
learning before we really cared about
learning features like is there a wheel
in this image is there a face in this
image we really cared more about whether
something was present compared to where
something is in the image thus we had a
really strong preference for Max pooling
which allows you to do that it asks
basically is there a heart in this image
rather than where is the heart in this
image however Max pooling does not
preserve the location information of a
feature we basically just know there is
a heart here not where that heart is but
in variational autoencoders it's often
really important to preserve that
location information because our goal is
to recreate the original image just as
it was not just that there's a heart
somewhere in it so in convolutional
variational autoencoders we often use
strides rather than Max pooling in order
to down sample our images that way it'll
preserve information location thus when
we use convolutional variational
autoencoders we often use strides rather
than Max pooling in order to dance
sample because it preserves the
information about about the location of
objects in an image as a general rule
you should use max pooling when you care
more about what is in an image and you
should use strides for down sampling
when you care more about where things
are in that particular image last but
not least I want to share with you this
really cool musical variational
autoencoder which is demonstrating the
smoothness of the latent space in a
variational autoencoder the smoothness
that we talked about about the latent
space where we can go from one sample to
another with all of the continuous
values in between that look like real
samples but slowly move from our first
sample to our second sample it's often
referred to as interpolation where we go
from one sample to another slowly
morphing between them this musical
variational autoencoder from magenta
does exactly that it morphs between two
samples of Music giving you samples in
between that slowly move from the first
sample to the second one

 
hello and welcome to your first support
Vector machine lecture support Vector
machines are classification models which
means they predict a group or a category
over the course of the next two lectures
we're actually going to talk about three
related models maximal margin
classifiers support vector classifiers
and then finally support Vector machines
each of these models makes an
improvement on the previous one however
they all have one very important thing
in common they use hyperplanes in order
to divide space so that we can
distinguish between two groups of data
points first let's start with a little
terminology what is a hyperplane if you
ever read any books or articles or even
watch any videos about support Vector
machines you're probably going to hear
the term flat affine Subspace which is a
very fancy way of saying a hyperplane
let's go through what each of these
terms mean so that you understand what
people are trying to communicate to you
when they use this phrase the word flat
basically just means that the hyperplane
is not curved it's going to be a
straight either line or hyperplane that
means that in each Direction it either
increases or decreases constantly affine
is just a fancy way of saying that the
hyperplane or line that we're working
with doesn't have to pass through the
origin zero zero and the word Subspace
just is a fancy way of saying a subset
of vectors in a larger Vector space for
those of you that have taken a class
that covers subspaces hopefully this is
review but for those of you that haven't
let's look at an example in two
dimensions
our Subspace is a one-dimensional line
in three dimensions
are Subspace
is a two-dimensional plane so you can
see that anytime we have a Subspace at
least for our purposes it's going to be
an N minus one dimensional object in two
dimensions that's a one-dimensional line
in three dimensions that's a two
dimensional hyperplane just as a quick
check pause the video and look at these
three pictures which of these represent
flat affine subspaces
all right hopefully you guessed that the
left hand picture is the only one that
is a flat affine Subspace the second
picture is not flat and actually neither
is the third picture nor is it a
Subspace so basically we can expect
whether it's in two Dimensions or more
that we have straight lines or hyper
planes all right now that we've gotten
some of that terminology out of the way
let's talk about why we care so much
about these hyperplanes in the first
place hyperplanes divide spaces in half
in all three of these examples you can
see that our hyperplane AKA a line in
this two-dimensional case divides the
space that we have into two sections we
have sections above the line represented
with these pink dots and we have the
section below the line represented with
these blue triangles the equation of our
hyperplanes can be written like this W
transpose which is our weights times our
input values which are stored in the
vector X Plus
an intercept or bias term when we set
that equation equal to zero we get all
of the points on our hyperplane or in
this case a line
by the way this notation is just another
way of writing this so This W transpose
X plus b is basically the same thing as
writing an intercept plus a weight times
an input plus another weight times an
input Etc and Etc and then setting that
value equal to zero
this notation is just a little easier
especially if we work with a lot of
different input variables and weights so
why do we care about these hybrid planes
that split our spaces in half well
because it'll help us classify between
two groups of points we're going to use
this hyperplane as a classifier we can
classify all the data points above the
line as one category and all the data
points below the line as another
category unlike some previous examples
in data science we're going to classify
data points that are positive with a 1
and data points that are in our negative
class as a negative one instead of a
zero which we've used in previous cases
so we take our data points and we plug
them into this formula if the value of
this formula is above zero it's going to
be in this region and we'll classify
that as a positive case remember if this
formula equals exactly zero that means
that the data point is actually on the
line precisely if we take this formula
and plug it in and our data point has a
value less than zero that means it's
below the hyperplane and will classify
it as a negative value so now we have a
way to classify data points if we plug
it into this formula and we get a value
greater than zero we classify it as
positive or A one
if we get a value and we plug it in and
it has a value of less than zero then we
classify it as a negative case or a
negative one now I want to simplify
these two formulas in two one in order
to do that let's do a little algebra so
to combine these two formulas we're
going to do something interesting we're
going to take the target value which is
going to be positive 1 if the case is a
positive case and negative 1 if the case
is a negative case and we are going to
multiply it by that formula W transpose
xn plus b now when the target value is 1
and we multiply by it it just leaves
everything the same so we get our
formula is greater than zero when the
value of our Target is a negative class
or negative one when we multiply it this
part stays the same however we change
this less than sign to a greater than
sign because we're multiplying by a
negative number and oh those actually
look really similar it turns out we can
write those two formulas as a single
formula the target variable one for
positive classes and negative one for
negative classes times our formula is
always going to be greater than zero
this implies that all of our positive
cases must be above the line and all of
our negative cases must be below the
line
so we've just seen that we can use
hyperplanes to classify data points into
a positive group and a negative group
for example here we have two groups of
data the dark larger data points are our
actual data and the background colors
represent points in our graph that are
either going to be classified as
positive if they're above the line or
negative if they're below the line this
hyperplane perfectly divides our two
groups of data all of the positive cases
are above and all the negative cases are
below so if we ever want to classify a
new data point that we don't know the
group of we can just see does it occur
above or below this hyperplane and
classified as positive if it's above and
negative if it's below so we just saw
why hyperplanes are really useful for
classifying data points as positive or
negative but we have a little bit of a
problem if a hyperplane like the one
over here exists that perfectly divides
between the positive class and the
negative class then by definition
they're in an infinite number of
hyperplanes that could do the exact same
thing for instance on the right hand
side you can see that there are a bunch
of different hyperplanes with their
slopes or intercepts just changed very
slightly that can all divide between our
positive and our negative classes
so if there's an infinite number of
hyperplanes how do we choose the best
one enter the maximal margin classifier
the first of our models that we'll talk
about we just said that we have a
problem if a hyperplane exists that can
perfectly divide between positive and
negative cases by definition there will
be an infinite number of hyperplanes
that could do the same thing
so we have a problem which one do we
choose well the maximal margin
classifiers is we should choose the one
that is furthest from the training
examples if you look at these three
examples on the bottom you can see the
same exact data with different hyper
planes dividing our two groups
for each of them we show the distance
between the hyperplane and the closest
data point as our hyperplane moves the
data point that is the closest will
change and the distance between the
hyperplane and the closest data point
will also change of these three examples
this hyperplane has the furthest closest
distance that means the distance between
the hyperplane and the next closest
point is as far as possible and that's
how we choose which hyperplane to use
for maximal margin classifiers we want
to maximize the margin which is the
distance between any data points and the
hyperplane on either side one
interesting thing that we'll talk about
a little more later is if you notice you
could take any of these data points that
are not in or on the margin and you
could move them around and nothing would
actually change about where our
hyperplane is because all it's looking
at is making sure that it maximizes the
closest distance between a data point
and the hyperplane so as long as you
don't move our data point into the
margin or even onto the incorrect side
of the hyperplane it will have no effect
on what the hyperplane actually is the
data points that do have an effect are
the ones that are on the margin and
these are called the support vectors our
support vectors are the only data points
in the data set that actually impact
where that hyperplane will go
all of the other data points have
essentially no influence alright let's
talk a little bit about math now before
we start I just want to acknowledge I
know that the notation can be a little
overwhelming I'll try and pause at each
point that we have to make sure that you
understand the concepts that we're
trying to enforce by going over this map
first just for reference this is the
formula to find the distance between a
data point and a hyperplane remember a
maximal margin classifier wants the
closest data point to be as far away
from possible from the hyperplane in
other words we want the largest margin
possible here you can see that this is
just this distance formula Rewritten
this is just saying that we're going to
find the distance between every data
point and the hyperplane and then figure
out which of those data points are the
closest to the hyperplane that's what
this Min does it's trying to find the
closest data point to the hyperplane and
then of course because we're trying to
maximize our margin we are going to
choose weights and biases that maximize
this closest distance so even though
this is a bit of a mouthful you can
think of this as maximizing the minimum
distance between the hyperplane and the
closest data point however because we're
required to have all of the positive
cases above the line and all the
negative cases below the blind we have
this constraint that we talked about
earlier that our Target variable either
one for positive cases or negative one
for negative cases times this formula
always has to be greater than or equal
to one data points where it's equal to
one are the ones that are actually on
the margin and data points for which
this is greater than one are ones that
are on the correct side of the
hyperplane outside of the margin now
because the math is a little bit easier
instead of maximizing this function we
are going to minimize this function
which is essentially the same thing so
again our goal is to minimize this
function subject to the fact that this
has to be true for every data point this
constraint is important because we could
choose a line that's like off the graph
right here and the distance between this
line and the closest data point is huge
but this is also a really bad classifier
remember we want this hyperplane to
divide our two groups of data points and
this one complete neatly fails at that
by enforcing this constraint we ensure
that our hyperplane actually will divide
our positive cases from our negative
cases all right so now we're going to
talk about constrained optimization and
there's going to be some math but again
I promise we'll stop and talk about the
concepts that are important as we go
through the math first let's take a step
back and figure out what we're actually
doing
constrained optimization is the idea
that we have two things a function that
we would like to optimize optimize means
just to find the minimum or the maximum
of the function but we have a constraint
that we have to follow while minimizing
or maximizing that function for instance
we might want to maximize the function
2x plus y subject to the constraint that
x squared plus y squared equals one so
we want to figure out how big or small
we can get our function f while still
operating under the constraint that x
squared plus y squared has to equal one
well it turns out that the Minima and
Maxima or Optima of the function occurs
when the function and the constraint are
tangent to each other you can see this
visually on the graph here is the
maximum of our function and here is the
minimum of our function subject to our
constraints so if we want to find the
Minima or the maximum of our function
subject to a constraint we need to find
where that function is tangent to the
constraint now you can just take my word
for this but when two functions are
tangent to each other it means that
their gradients are parallel
you can see an example of this here
where the gradient of our function is
basically the same line as the gradient
of our constraint except it's scaled in
function terms we can write it like this
the gradient of our function is equal to
some scalar value that's just like a
number like 1 or 2.7 times the gradient
of our constraint if we can find the
values that make this equation true we
find the Minima and the Maxima of our
function subject to our constraint in
order to do this we have to find the
gradient of f
and the gradient of x
here I've done it for you I don't expect
you to be able to calculate these
partial derivatives just understand that
we're calculating the gradient now we're
going to plug them into our formula we
know that the gradient of f is going to
be equal to some scalar value Lambda
times the gradient of our constraint so
we just set them up in this formula and
I'm not going to make you do the math
but when you solve for these values you
find these two points the minimum and
the maximum of our function subject to
our constraint so to review if we have a
function that we want to optimize and a
constraint that we have to adhere to we
need to find where that function and the
constraint are tangent in order to do
that we use the gradients and we just
solve for these two values now let's
talk a little bit about the lagrangian
lagrangians are a way to do constrained
optimization like we just did by hand
before but it makes the math slightly
easier the lagrangian rewrites our
function and constraint in this form
adding the term Lambda which is called
the LaGrange multiplier this basically
takes our function and subtracts Lambda
times our constraint minus the value
it's constrained by in other words this
value for the example we did by hand
before this would be its lagrangian form
we have our function minus our LaGrange
multiplier times our constraint minus
the value it is constrained with now why
would we rewrite these two perfectly
reasonable functions into one bigger
function well it turns out it's because
it makes the math a little bit easier
well at least if you understand partial
derivatives when we take the partial
derivative of our lagrangian with
respect to each of these three variables
some interesting things pop out for
instance and I don't expect you to be
able to take these partial derivatives
by hand when we take the partial
derivative of the lagrangian with
respect to our LaGrange multiplier this
pops out negative g x y plus C and when
we set that equal to zero that only
happens when G of X Y equals c and oh
look that was our constraint interesting
next if we do the same thing with the
partial derivative with respect to X and
set that equal to zero another familiar
thing pops out this looks a lot like
this function
and that's because it is and without
spoiling anything if you plug in the
partial derivative of the lagrangian
with respect to Y the same thing pops
out but with Y so basically the
lagrangian is a way to rewrite our
function and our constraint where if we
set its partial derivatives to zero or
in other words it's gradient to zero we
solve for the values that we wanted
before interestingly this LaGrange
multiplier that we find tells us how
changing the constraint value of our
constraint would change the possible
minimum and Maxima for our function all
right that was a lot of information
about constrained optimization so let's
really quickly go through what the main
points were if we have a function that
we want to optimize AKA find the Minima
and Maxima of subject to some constraint
that we have to stick to we need to find
where the function and the constraint
are tangent to each other that would be
at these points
for example say you want to maximize the
profit of your company but you have to
stick to the constraint of a budget for
labor and materials next if we want to
find where two functions are tangent we
need to find where their gradients are
pointed in the same direction to do that
we can solve this equation which says
some number times the gradient of our
constraint last but not least we can use
a lagrangian in order to find all these
values by plugging them into a single
function and setting that function's
gradients to zero so why did we just
spend our time talking about constrained
optimization and doing some potentially
confusing math well it's because we can
use constrained optimization in order to
find a hyperplane for our maximal margin
classifier and later down the road for
our support Vector machine remember from
before that we have a value that we are
trying to minimize and we have a
constraint for each data point that is
every single data point has to be on the
correct side of the hyperplane with all
the positive examples above the
hyperplane and all the negative examples
below the hyperplane we can use a
lagrangian in order to do this for
instance this would be the formulation
of the lagrangian for this function with
this constraint because technically we
have one constraint per data point we're
actually taking the sum of all of these
constraints rather than having one big
constraint like we did before here a n
is the LaGrange multiplier for the
constraint imposed for data point n that
is that it is on the correct side of the
hyperplane now I'm not going to make you
do any of the math but there are
resources in our resources page that are
linked if you want to follow along
however I do want to show you some cool
things that pop out when we let other
people do the math for us first of all
when we solve for w and we plug that
back into our lagrangian something
really interesting happens remember
remember LaGrange multipliers are just
numbers that are greater than zero and
our Target values one for positive cases
and negative one for negative cases are
also just scalar values so that means in
order to find the hyperplane it really
only relies on just some random numbers
and then the dot product between two
data points this will come up later and
is very useful next from that we can
derive that this is the formula to
classify a new data point x so if we
have a new data point and we want to
classify whether we think it's a
positive case or a negative case we just
plug it into this formula and of course
we are subject to these constraints now
this last constraint is the most
interesting one this says that our
LaGrange multiplier times this formula
right here has to equal zero for every
single data point now because we have a
value multiplied by another value this
can only be zero when one or both of
these is zero that means that either the
LaGrange multiplier must be zero or this
value must be zero and when is that
value 0 again oh it's for vectors that
are exactly on the margin this
reinforces that point that we talked
about earlier only the data points that
are on the margin determine where the
hyperplane is all of the other data
points don't really matter as long as we
don't move them so that they're on the
wrong side of the hyperplane this
constraint is just a very mathematical
way to prove that point either data
points have no influence on the
hyperplane because their lagrangian is
zero or they're on the margin and they
they do have an influence by the way the
fact that only a few support vectors
influence where the hyperplane is makes
support Vector machines and maximal
margin classifiers very computationally
efficient because really only a few data
points matter alright so we've done a
lot of math and we've talked about
maximal margin classifiers remember
maximal margin classifiers find a
hyperplane that perfectly divides our
positive cases from our negative cases
however we need to improve on this model
because real life is uh never so clean
that we can perfectly classify between
our positive and negative cases often
our groups of data will overlap and
they're not linearly separable there's
not a line that we could draw that would
perfectly classify positives as
positives and negatives as negatives
it's also the case that even if we could
perfectly classify positives and
negatives we might not want to for
instance if we look at the classifier on
the left compared to the one on the
right we can see that the addition of
this single pretty far out there may be
outlier point is changing the hyperplane
a ton and maybe we don't want a single
point to have that much influence enter
the support Vector classifier the
support Vector classifier relaxes that
constraint the data points either have
to be on or outside of the margins
instead support Vector classifiers allow
data points like this 8 and this one to
actually be inside the margin in fact
data points can not only be inside the
margin they could be on the entirely
incorrect side of the hyperplane by
relaxing this constraint the data points
have to be on or far away from the
margin support Vector classifiers allow
us to use a hyperplane to classify data
that is not linearly separable
mathematically the way we do this is
with the introduction of something
called a slack variable which are these
little curly q guys right here the slack
variable basically represents an error
so data points that are on the correct
side of the hyperplane and outside of
our margin our data points that are
being very confidently classified by our
support Vector machine so data points
that are in this region have a slack
variable of zero of course this would
also be true for the other group of
points on this side data points that are
violating the margin but are on the
right side of the hyperplane have a
small slack variable less than one that
occurs in this region right here
while technically our support Vector
classifier is correctly classifying
these data points it's not as confident
about its classification so we want to
penalize that a little bit and for data
points that are on the completely wrong
side of the hyperplane like this one
right here the slack variable will have
a value greater than one with the
introduction of these slack variables
we're also going to change the function
that we're trying to optimize as well as
the constraint whereas previously we
tried to minimize this value now we're
adding a constraint on how big the sum
of the different slack variables can be
this hyper parameter C basically
controls how large of a cost it is to
have a slack variable that is greater
than zero similarly whereas before we
had a constraint that required all of
our data points to be on or outside of
the margin our new constraint allows
some data points to violate the margin
or even be on the Raw wrong side of the
hyperplane by adding in this slack
variable term now back to that hyper
parameter C remember the hyper parameter
C controls how big of a penalty we're
going to give to slack variables that
are greater than zero when we have large
values of c the penalty for having a
slack variable greater than zero is huge
this leads to less regularization of our
model and narrower margins overall
this is because it's so costly to have a
slack variable greater than zero that we
try and find a hyperplane that can
perfectly separate our data
in terms of the bias variance trade-off
this leads us prone to overfitting this
should make sense intuitively when it's
costly to have a slack variable that is
greater than zero we're going to have
very narrow margins with very few
support vectors and remember where the
hyperplane is is solely determined by
your support vectors the fewer support
vectors you have means the fewer data
points are influencing where that
hyperplane is on the other hand when we
have very small values of c our model is
more regularized and we tend to have
wider margins this is because it's not
so costly anymore to have a slack
variable that is greater than zero
this leads us to be prone to
underfitting visually you can see what
happens here on the top left corner you
can see that we have a very small value
of C the margins are so wide you can
barely see them on the graph and there's
a ton of support vectors those data
points that are either on or inside the
margin as we go through the other images
you can see that the value of C is
increasing leading to narrower margins
with fewer support vectors again we'll
typically choose C using something like
hyper parameter tuning but it's really
helpful to intuitively know what it's
controlling alright that's all I have
for you I will see you next time

 
hello and welcome to your lecture on
auto encoders auto encoders are our
first type of unsupervised neural
network so far everything we've done
with a neural network has been to
predict a certain value that's doing
supervised machine learning now for
reasons I'll talk about a little bit
later in the lecture some people call
Auto encoders semi-supervised but in my
opinion this is the first time we're
actually doing unsupervised machine
learning with a neural network in order
to understand what autoencoders do we
have to review principal component
analysis that we learned in the previous
course remember principal component
analysis is a dimensionality reduction
method that means that we take a lot of
information and we squish it down to
represent at least most of that
information in a smaller way for
principal component analysis the way we
do this is we take the axes of our data
like the x-axis and the y-axis and we
rotate them to create a new more
efficient set of variable tables that
explain the information in our data a
lot more efficiently for example in
principle component analysis the first
principle component is going to explain
the most variation the second the second
most the third the third most and so on
and so forth this allows us to create a
cutoff point where we only take the
first 5 10 or n principal component
because principal component analysis
creates these more efficient variables
we're able to represent most of the
information from our original data in a
smaller number of variables thus
reducing the dimensions of our data you
can think of principle component
analysis as like a lossy compression
we're taking a large amount of data and
we're compressing it and we're losing a
little information along the way but not
too much on the screen you can see what
are called eigenfaces eigenfaces are the
result of applying principal component
analysis to all the pixels in the image
of a face on the left hand side you can
see the original images and on the right
hand side you can see the reconstructed
images where we've only kept 100 out of
about a thousand principal components
and then on the very right you can see
where we've only kept 10 principal
components this helps us visualize
principal component analysis as a lossy
compression we're taking the photos and
we're compressing them and while we're
retaining a lot of the original
information we are losing a little bit
you can see that the fewer principal
components we keep the more information
we lose but the more compression we have
because the images are represented with
far fewer variables now principal
component analysis is linear this is
because all of the principal components
that are created by PCA are linear
combinations of the original variables
remember a linear combination is just
where we take a bunch of Weights
multiply them by some values and then
sum them all up and that's exactly how
all of your principal components are
calculated PCA uses something called
eigen decomposition to figure out what
weights we should use then multiply them
by our original variables to get out all
of our principal components but what if
we didn't want the relationship between
our predictors and our compressed
features to be linear well that's
exactly what autoencoders allow us to do
auto encoders are a type of neural
network architecture that basically does
a non-linear principle component
analysis the structure of autoencoder
sort of looks like a butterfly because
the input and the output of our Network
are exactly the same essentially what we
want to happen is we want to be able to
put our data through this neural network
and then have it come out the other side
almost unchanged and you might think
well that's really boring I could just
have a single layer neural Network and
return the input unchanged and that
would just fulfill everything you said
but the beauty of an auto encoder is
that we're not just returning our values
and change rather we're forcing the data
to go through a more simple
representation shown here in the middle
which basically compresses the data and
then we have to be able to uncompress it
thus we're trying to find a compression
that allows us to have the input and
output be as similar as possible while
still doing that compression the
autoencoder is made up of two different
parts first is the encoder the job of
the encoder is to actually compress all
of our information so we take it from an
input layer that is quite large to a
middle layer that is quite small thus
compressing the information the
encoder's job is to learn this function
f that takes our original variables X
and turns them into their compressed
version H the other half of our
autoencoder is the decoder the job of a
decoder is to take our compressed
representation of the data and
uncompressive essentially it's learning
this function G that takes our
compressed version of the data and tries
to recreate the original data so you can
tell that what we're really interested
in when we build an auto encoder is this
middle layer this this middle layer is
the compressed version of our data which
we'll call H and we might want this
because we want to run a neural network
on a more compressed version of our data
when learning the neural network is
using a loss function that compares the
original variables that we put into the
model with the compressed and then
uncompressed version which is the output
of our model our loss function measures
how close these two things are and a
perfect model would actually return the
original variables because it learned
perfectly how to compress and then
uncompress our data and this is why we
refer to Auto encoders as doing
non-linear PCA if we build a very simple
autoencoder where the encoder was linear
meaning that the compressed data was
just linear combinations of the original
data and we use MSC as our loss function
Auto encoders would basically
approximate PCA but of course as I
mentioned we don't have to have a linear
encoder we can add multiple layers and
non-linear activation functions in order
to add some non-linearity into our
encoder so basically autoencoders do a
similar process to principle component
analysis but they have a lot more
flexibility with how they do it now I
mentioned before that autoencoders
typically have this butterfly shape
where the middle layer of the auto
encoder aka the compressed data has
lower dimensionality compared to the
input and the output this is so that the
autoencoder is forced to represent the
data in a smaller fashion before
uncompressing it these types of
autoencoders are called under Complete
Auto encoders because the middle layer
is a lot smaller than the input and the
output layer again the benefit here is
that it forces that compression because
it has to take a large amount of data
and represent it with a small number of
nodes in the middle of the network
however Auto encoders can also have
hidden layers that are larger in
dimension than the input and output
these types of Auto encoders are called
over complete autoencoder so again when
the hidden layer is smaller than the
input and the output that's an under
complete autoencoder and when it's
larger than the input and output it's an
overcomplete autoencoder but you might
be asking well why would I ever do that
because the whole point of the
autoencoder is to do compression well
that is still the goal of an
overcomplete auto encoder however when
we use an overcomplete autoencoder we're
also going to need to add something else
that forces this hidden layer to be
sparse sparse just means that a lot of
these nodes should be forced to be zero
having an over complete hidden layer
allows the encoder to have a little bit
more flexibility with how it's
representing that compressed data
however when we apply something like a
penalty that encourages sparsity it
means that we're still going to have a
low dimensionality compression one way
to encourage this sparsity is with
something we're really familiar with a
penalty for instance when we are
training our Network we can take our
original loss function and add a penalty
on the compressed middle layer this
basically penalizes when the nodes have
values that are far away from zero thus
encouraging the network to have as many
of these as possible at zero instead of
penalizing just the values of that
hidden layer we can also penalize the
derivative a penalty on the derivative
basically forces our Auto encoder to
learn a function that doesn't change
very much when the inputs change a
little bit another way we can regularize
our Auto encoders in addition to putting
penalties on the values of the Hidden
layer and penalizing the derivatives is
to use something called a denoising
autoencoder a denoising autoencoder has
the same structure as a typical
autoencoder however before we put data
into the auto encoder we add a little
bit of noise to it this might look like
just adding tiny random values to each
of our inputs then the autoencoder tries
to learn and the original data so the
input it's getting is a slightly noisy
version of our data but the output that
we want it to learn is the original data
without going thus the autoencoder is
actually doing denoising it's taking in
a noisy version of the data and trying
to compress and uncompress it so that
the output is an unnoisy version of the
data this addition of noise means that
flight perturbations in the input of our
data should still result in the same
unoisy output thus regularizing our
autoencoder and autoencoders don't just
have to have typical dense feed forward
architecture in the encoder and decoder
we can also use convolutional layers
thus we could build a convolutional auto
encoder which takes an image compresses
it and then uncompresses this by the way
typically requires that transpose
convolutional layer that we talked about
when we talked about convolutional
neural networks because of course we're
going to use convolutional layers to
compress our image and then those
transpose convolutional layers to
uncompress it and we can apply all the
same Concepts that we talked about with
typical autoencoders to convolutional
autoencoders for example with these
mnist digits we could actually take them
add a little noise to the image send
those through the network and ask it to
return the unnoisy images to us last but
not least as promised I want to talk to
you a little bit about why some people
call Auto encoders semi-supervised
rather than unsupervised machine
learning because autoencoders
technically do have a correct answer
right the input would be the same as the
output so our input is also our correct
answer some people consider this to be
semi-supervised we do have a correct
answer that we're using in order to
train our Network work however the
answer is the input data itself so I
really consider this to be unsupervised
neural networks I just want you to be
aware of the distinction because you may
see them referred to as unsupervised or
semi-supervised depending on where you
read about them to review today we
learned a little bit about Auto encoders
which are our first unsupervised or at
least semi-supervised neural network
architecture the goal of an autoencoder
is to learn how to compress information
and then uncompress it in a way that the
input is as similar as possible to the
output while still doing that
compression autoencoders are often
referred to as non-linear PCA because
they're doing a similar type of
compression as a principal component
analysis however they're not limited in
their structure to be linear we also
talked about three different ways we can
regularize our Auto encoders the first
way was to put a penalty on the hidden
layer in our Network thus encouraging
sparsity in the this compressed version
of the data next we talked about a
different penalty where we penalized the
derivative this forces our Network to
learn a function where small changes in
the input data don't result in large
changes in the compressed version lastly
we talked about adding noise or creating
a denoising autoencoder this regularizes
our Network by adding a little bit of
noise to our input data and then asking
the network to actually recreate the
original data without the noise and
lastly we talked about the fact that we
don't have to use Simple feed forward
architectures for our encoders we can
also include things like convolutional
layers which allow us to build an auto
encoder that compresses and then
uncompresses an image alright that's all
I have for you I will see you next time

 
hello and welcome to your first
recurrent neural networks lecture in the
previous section of lectures we talked a
little bit about convolutional
architectures in deep neural networks
convolutional architecture has allowed
us to take into account spatial
relationships in data like images in
convolutional layers we take filters or
kernels and we slid them across a 2du
image allowing us to model the 2D
relationships or spatial relationships
in the data but another type of data
that we might have and that we might
want to model with a neural network is
sequential data similar to data like
images that has spatial relationships
sequential data has sequential
relationships meaning that as we go from
one side of a sequence to the other
there's meaningful relationships there
for example you might think of something
like a Time series like the stock price
each day across time unlike data that we
used in convolutional neural networks
that had spatial relationships this has
one dimensional or temporal
relationships basically the
relationships that we're modeling in the
data happen across time and sequential
data is probably very familiar to you
for example I just mentioned that things
like stock prices over time that's
sequential data also a lot of sensor
data think about all of the things you
wear like an Apple Watch that's
measuring your heart rate or your steps
and lastly written text is sequential
when we read or write or talk all of the
words that we're saying have a
meaningful order in a sequence the first
word can't suddenly jump to the middle
of a sentence and have everything still
make sense and so with sequential data
all around us it makes sense that we may
want to model some of this data using
neural networks now what are some things
that we might want to do with sequential
data well the first and most common and
obvious one is to do something called
forecasting forecasting is when we look
at a time series or a sequence of data
over time and we try and predict what
the next time point will be for example
we could predict what the price of Apple
stock is going to be 10 days from now
next we might want to take a sequence
and classify it for instance we could
take in a string of text and try and see
if we can classify who the author of
that text is another thing we might want
to do is something you might be really
familiar with which is machine
translation for example we might want to
translate one sequence like a sentence
into another sequence like that same
sentence in a different language we also
might want to do anomaly detection where
we're looking at a sequence and seeing
if there's anything that happened out of
the ordinary we can also summarize data
for instance if you take a huge chunk of
text and want a couple of sentences
summarizing the information in that text
and the last example we'll talk about
now is sequence generation for example
imagine a model where we could feed it
in all Jane Austen's work and then ask
it to generate a new chapter of a Jane
Austen book and these are all really
interesting things that we could do so
let's go ahead and take a look at some
of the model architectures we might try
in order to do some of these the first
thing we might try and do is just throw
everything in a feed forward neural
network why not try it it might at least
provide us with a good Baseline and we
can take our sequence flatten it and
just shove it through all of those
layers and honestly sometimes this might
work just in the same way that we used a
feed forward neural network to classify
image data using the mnist data set
however this type of model doesn't
consider time-based relationships in the
input data that we have therefore we're
sort of just throwing away information
if we have a sequential data say the
price of a stock over time we know that
there are meaningful time-based
relationships there for instance the
price of a stock yesterday tells us a
lot about what the price of a stock
might be today okay so we want to take
into account these sequential
relationships well why not just do that
with convolutional layers well actually
we can we so far have been using 2D
convolutional filters because typically
what we're using is something like an
image that is two-dimensional but
there's nothing stopping us from using
one-dimensional convolutional layers in
order to process a sequence just like
with typical convolutional neural
networks we are taking filters and
applying them to input but instead of
two-dimensional filters and
two-dimensional input we now have
one-dimensional filters and
one-dimensional input and just like
before we take those filters slide them
across our data and as we do we will
take the filter values multiply them by
the input values and get out our output
for example here we have the filter
negative 1 2 negative one and the first
time that we put this filter on our
input data we are going to take the
filter and multiply it by these three
numbers here resulting in the output of
5. and we'll repeat this process until
we get to the end of the sequence giving
us our final output because our filters
are looking at chunks of the sequence
together we're not throwing away all of
that sequential information that we were
when we were using a feed forward neural
network and just like with our typical
convolutional networks we often have
multiple filters so they can detect
different features about our sequence
and just like before we can also use max
pooling in order to down sample our
information for instance here I have a
two by one filter that's going to slide
across our sequence and return whatever
the maximum value from that filter is
you can see that at the first placement
the maximum value is 5 and so that's
what the filter returns then four then
three then six then six again and
finally two we could also use average
pooling which like Max pooling slides a
filter across our sequence but instead
of returning the maximum value we return
the average value thus sort of smoothing
out our sequence and while it makes
sense that we can use this architecture
to process sequential data 1D
convolutional and pooling layers are
typically not our go-to for modeling
this type of data like when we're
processing images the filters we're
using in these 1D convolutional layers
assume translational and variants and
while that's true for images that's not
always true for sequences also pooling
which we often pair with our
convolutional layers can interfere with
the sequential information in our data
so well this is an improvement from feed
forward neural networks where we just
completely disregard any relationship
between our inputs we can do better and
the way we can do better is with
recurrent architectures a recurrent
neural network is one where the output
at a certain time point is then fed back
into the input for the next time point
in other words the predicted price for
tomorrow is going to affect what our
predicted price of a stock is for the
following day and typically we represent
recurrent neural networks using a
diagram like this which basically shows
that we have an input and an output but
that output is also then fed back into
the model for the next time step and if
we remove that pesky recurrent
connection then this just looks like a
typical feed forward neural network we
have an input we have an output and we
do some Matrix algebra in the middle to
get from one to the other all we're
doing with a recurrent architecture is
we're having this Loop that allows us to
take previous outputs at different time
points and feed them back into our model
thus this takes into account the
time-based or sequential relationships
in our data very well and this will
continue to happen over and over as we
feed new data in get a new output feed
that back into the model use that to
make a new output feed that back in new
output feed that back in it creates this
General Loop for the entirety of our
sequence for this reason we often also
show our recurring architectures
unrolled which basically means we're
just taking that diagram we saw
previously and we're showing how it
would look unrolled over time for
instance here we can see that our input
is generating an output which is then
fed into the next node which also takes
an input and generates an output which
is then fed into the next node which
also takes an input generates an output
so on and so forth for the entire
sequence until we get our final
prediction which is the last output for
example we could feed this neural
network five days worth of stock prices
and then ask it at the end to Output its
prediction for what the following day's
price would be you all right so now you
know how recurrent architecture Works
generally let's take a little bit of a
look at what's happening under the hood
and how we're generating these values so
the outputs of our model are called
hidden States represented here in these
orange squares by H we want to get the
current hidden State and in order to do
so we need at least two things we need
the actual input for our current time
Point say the stock price for that day
and we need the hidden state from the
previous node then all we need to do is
combine this input and the previous
hidden state by using weights and biases
that our model is going to learn here
you can see we take some weights and
multiply it by our input plus some
weights multiplied by our previous
hidden State and then of course all of
that added to a bias this value is then
fed through a hyperbolic tangent
function which if you remember from one
of our earlier lectures takes values and
squishes them between negative 1 and
positive 1. the output of this
hyperbolic tan is then our current
hidden state which we will then either
use to make a prediction or feed into
the next node in just a quick update to
notation for when we get to some more
complicated networks oftentimes what
we'll do is we'll take our previous
hidden State and our current input and
we'll squish them into a single vector
and that way we only have one set of
weights and we multiply it by this
concatenated Vector of both our hidden
State and our actual inputs and this is
an example of what our simple recurrent
neural network will look like unrolled
we start with a blank hidden State we
have our first input and that gives us
our next hidden state which is then
outputted but also fed in to the next
node we then combine that with the next
day's input and get our new hidden State
and again and again until the very end
of the sequence where we can make a
prediction for example if we wanted to
use this to predict stock prices we
could feed in the day Zero stock prices
the day one stock prices the day two
stock prices Dot dot all the way until
day eight and then after we've fed that
sequence through the recurrent neural
network we can then ask it well what's
your prediction for day nine
thus the recurrent neural network has
taken into account the sequential
information in the data when making that
final prediction now even though I'm
showing this network unrolled like this
where it sort of looks like there's
multiple nodes chained together remember
this is just a repeated copy of the same
exact node that means that every time
our data is pushed through this node
with input previous hidden State and
then combined using our weights and
biases these are the same weights and
biases that are being used at every
single step just to repeat that that
means that this node that we have that
we continuously send our data through is
the same exact node meaning that when we
update these weights and biases we're
just updating this set of weights and
biases there's not one set of weights
and biases per step in the sequence to
review recurrent architectures are very
special because they allow us to take
into account sequential relationships in
our data the way they do that is by
forming a node that takes in an input
and a previous output if there is one
and combining them to make our new
prediction us when we have a sequence of
data we can feed it through this same
node over and over and over always
taking the output of our previous time
step and feeding it back in for the next
one and this type of Architecture is
especially helpful for things like
forecasting where we want to predict the
next day's stock price or your heart
rate in 10 minutes or the next word in a
sentence but for some sequential tasks
we actually want to approach the problem
from both sides for instance in
forecasting we can't use future data to
predict what tomorrow's stock prices are
but if we're doing a task like say
trying to fill in the blank of a
sentence like this one where it says the
quick brown blank jumped over the lazy
dog we might want to actually attack
this problem from both sides by
processing our sequence both forward
third and backward and then combining
that information this is exactly what a
bi-directional recurrent neural network
does it basically takes that recurrent
neural network architecture that we
talked about and it does it once for
going forward through the sequence from
the quick all the way to Lazy Dog and
then it does the same thing for the
sequence going backwards so going from
dog all the way to the at each time step
it combines the output from the forward
sequence and the backward sequence so
that we can quite literally look at this
problem from both sides in cases like
this where we do have data both before
and after the blank that we're trying to
predict this can be incredibly helpful
to consider context from both before and
after the word here's another example of
a bi-directional recurrent neural
network using a slightly more
complicated model that we'll cover in
the next lecture to review we talked
about sequential data today this could
be things like stock prices your
biometric data measured by a smart watch
or even sentences in text like the way
that images had spatial relationships
sequential data has sequential or
temporal relationships and so when we're
building models that try and use
sequential data we may want to take
those into account first we just tried
to brute force it and we shoved all of
our data into a feed for neural network
but we realize that that doesn't take
into account the sequential
relationships in the data and therefore
is just kind of throwing all of that
useful information away next we try to
adapt convolutional architectures in
order to fit this sequential data
problem and this did a lot better by
using one-dimensional convolutional
filters we were able to actually process
our data while taking the surrounding
data say yesterday's stock prices and
tomorrow's stock prices into account
thus we introduced recurrent
architecture recurrent architectures are
similar to feed forward architectures
although we take the output at a
specific time point and we feed that
back into the model thus when we make a
prediction for today we're also taking
into account what our prediction for
yesterday was and so on and so forth
throughout the entire sequence because
we're often taking our outputs and
feeding it back in at the next time step
sometimes we show recurrent neural
networks unrolled meaning we show the
same node repeated over and over across
a sequence lastly we learned that
recurrent neural networks typically
process our data from the beginning of
the sequence to the end however
sometimes you might have an occasion
where we want to process sequences from
both sides first going forward and then
going backward in order to do this we
can basically combine two recurrent
neural networks one that processes a
sequence forwards and one that processes
it backwards then we combine their
output in order to do things like for
instance predict what a blank should be
and it fill in the blank alright that's
all I have for you I will see you next
time

 
hello and welcome to your fourth neural
network lecture
so I think it's appropriate that we end
our neural network lecture series by
talking about the biggest problem in
data science which is the bias variance
trade-off in our previous course we
talked a little bit about the bias
variance trade-off and what it meant for
training machine learning models when we
look at this chart we can see that as we
increase model complexity we go from an
area of high bias where our model is too
simple to approximate the relationship
between our inputs and our outputs and
an area on the right hand side that has
high variance where our data is too
complex and it over fits to the data
that we have meaning that it performs
very differently on data it's seen
versus data it hasn't seen basically the
bias variance trade-off says that if
we're in this area of really high
variance we can actually move over by
increasing our bias and therefore
decreasing our variance
hopefully this will get us in this sort
of Goldilocks zone where both our scene
data the training data and our unseen
data the testing data are both
performing well in previous lectures we
talked about the fact that neural
networks are really powerful because
they allow us to approximate really
complicated relationships between the
inputs of our model and the outputs of
our model when we stack a bunch of
different layers together we can get
some really complicated relationships
between inputs and outputs however with
great model complexity comes a huge need
for regularization
regularization are practices that help
us move from that area of high variance
hopefully closer to a middle Zone where
we're not overfitting our model to data
it's seen before so today we're going to
talk about a couple of different ways
that we can regularize neural network
models let's start with a way that you
should be very familiar with from the
previous course which is penalization
penalization typically works best for
smaller networks however it works pretty
much the same as like a lasso or a ridge
penalty that we learned about that we
can apply to things like linear
regression
neural networks just like a linear
regression or a logistic regression have
a bunch of different weights in the
model
and all of these weights can be
penalized the penalty basically serves
to make sure that the weights are not
too big in the same way as a lasso or a
ridge regression penalization works by
taking our typical loss function
whatever it is that we chose and adding
some penalty term multiplied by a
regularization strength term which
basically determines how big of a
penalty we're putting on the weights and
this penalty term here represented by
this Omega function can pretty much be
anything but common versions would be an
L1 or an L2 regularization just FYI we
typically only penalize the weights of
the model not the biases this is because
biases tend to be more easily estimated
and when we regularize them it can
actually lead to our model underfitting
this type of regularization basically
works by saying if a certain parameter
does not actually improve our loss that
much then we can make it a lot smaller
or regularize it by applying that
penalty
this point here basically represents
what our weights would be if there were
no penalization
as you can see when we move in this
horizontal Direction changing our weight
weight one doesn't actually have that
big of an effect on the loss of our
function at least compared to the other
variable
because it doesn't have as large of an
effect our regularization actually drags
that weight from let's say right here
all the way over here
however if we look in the vertical
Direction when we change our weight the
loss is actually increased quite a bit
very quickly
this means that weight 2 has a huge
effect on our loss and you can see that
the regularization that happens taking
our weight to from about here to about
here is much smaller now I won't make
you work through all the math but if we
apply L2 regularization to our weights
which is similar to a ridge regression
we end up with a new update rule when
applying something like gradient descent
we know that our typical update rule is
that our new weights are our old weights
minus some learning rate times our
gradient we can see that the new update
rule that's derived from applying an L2
penalization on our weights basically
changes our update rule we're still
applying the same change the learning
rate times our gradient but before we do
that we actually take our previous
weights and Shrink them down a little
bit therefore you can see that at each
step we're actually shrinking all of our
weights a little bit before applying our
update and L2 regularization is not the
only way we can penalize our weights we
can also use an L1 penalty which is
similar to lasso just like with lasso an
L1 penalty still penalizes large
coefficients for being big if they don't
improve our loss function but unlike L2
penalization L1 penalization encourages
sparsity meaning that it'll drag some of
our weights to exactly zero that's
basically removing them so in summary
just like we applied penalties with
lasso and ridge regression we can also
apply penalties like an L1 or an L2
penalty to the weights in our neural
networks this basically penalizes large
weights that don't improve the loss of
our model thus regularizing and
simplifying our model another way that
we can regularize our models is with
something called bagging bagging should
be familiar from back when we learned
random forest in our previous course
bagging which is short for bootstrap
aggregating is essentially when we take
a bunch of different models train them
on different subsets of data and then
average them in the end bootstrapping is
where we take our original sample of
data and basically resample it by
randomly selecting different rows from
that original data with replacement this
means that we end up with a new sample
of data where some rows might appear
multiple times and other rows might not
appear at all this again has the effect
of having every single model seeing
different samples of data bootstrapping
basically ensures that the different
models see different inputs and
therefore have different structures and
we can apply this idea to neural
networks we could use bagging to get
different sub samples of data to train
different neural networks on and then in
the end average them all together but it
turns out we may not actually need to do
that neural networks are very sensitive
to random initializations like where we
start our weights off at or things like
which data points are in which batch as
we're updating our parameters at each
step so it turns out that randomizing
those things will still give us slightly
different neural network structures that
we can then combine or average together
in the end however one big downside of
this is that it's very computationally
expensive depending on how big your
neural network is it might be
prohibitive to try and train a bunch of
different neural networks just to then
average them together in the end all
right let's talk about a similar related
way of regularizing our neural networks
called dropout dropout actually operates
on a very similar idea to random forests
which have bagging and random feature
selection the idea in a random Forest is
that while individual trees might
overfit to the data that they see not
every tree sees the same data so while
individual trees can overfit as an
ensemble the model can't overfit because
different trees are seeing different
information so they can't all over fit
to the same patterns of noise in the
data Dropout operates on a very similar
principle during training Dropout will
randomly select nodes in each of your
layers and just completely zero them out
or remove them this means that during
training we never know which nodes are
not going to be there which means that
our Network can't over rely on the
connection between certain nodes let's
look at an example of what this might
look like
here we have a small but fully connected
neural network typically when we train
this neural network we'll do a forward
path which will send all of our inputs
through the network and get a prediction
and then we'll do a backwards pass where
we do back propagation in order to
update the weights in our model the idea
behind Dropout is that when we're
training every time we update we will
drop random nodes from each layer in our
model just completely removing them from
the model this means that when we do our
forward and our backwards path some of
those nodes just aren't there and
they're not a part of training the next
time we want to make an update we may
drop different nodes like these instead
of the ones that we dropped before again
we'll continue on with our training
process but this time with these nodes
removed so as we train our model it
can't over rely on certain connections
between nodes because during some points
in training those nodes were just
completely removed from the model
mathematically the way we do this is by
taking an output from a layer and then
randomly deciding which of our nodes
we're going to set to zero in this case
I wanted about 20 percent of my nodes to
be set to zero and randomly it chose to
drop this one so we took the previous
values and left all of them the same
except this one which we then set to
zero thus basically dropping it out of
the network now just a quick
mathematical note when we zero out some
or many of the outputs from a layer that
means the values in the next layer on
average are going to be closer to zero
because we just set a bunch of the
values coming into that layer to be zero
in order to combat this what we do is we
take the remaining non-zero values and
we scale them by one over one minus the
probability of a node being dropped out
in this case my dropout rate was 0.2 or
20 percent and so I scaled all of these
up by 1 over 0.8
this ensures that the expected values
that are feeding into that layer remain
around the same even though we're
setting some of them to zero and while
sometimes we might want to have the same
dropout rate for every single layer for
instance here for each of the different
layers we drop out about on average 20
of our nodes we actually can set these
to completely different values so for
instance because these two layers are
very densely connected we might want to
set the Dropout right here to be about
0.5 so that it's losing more of its
connections every time we're training
every iteration that we're doing when
we're training we're going to randomly
drop out different nodes but this
Dropout only happens during training
once our model is fully trained using
Dropout when we're doing a prediction
from our model we actually don't use
Dropout because we want to use now the
full fully trained Network so again drop
out of random nodes only happens during
training not when we're actually making
prediction questions from our model just
like a random forest was an ensemble of
decision trees with slightly different
structures you can think of Dropout as
basically approximating an ensemble of
different sub networks all of the
different possible networks we can get
by dropping out some of the nodes
Dropout is really cool for a few reasons
one as we're focusing on this lecture it
helps regularize our model taking it
from over fit to hopefully in that
middle ground where we're doing well on
both our scene data and our unseen data
it's also pretty computationally
inexpensive and because of how simple it
is all we're doing is zeroing out
different outputs from different layers
it's really easy to implement with all
different sorts of structures of neural
network all right on to early stopping
early stopping is another way to
regularize our Network and actually I
can explain it with just this picture
often when we're training our models
what we'll see is that both the training
and and the validation error at the
beginning start to both descend and then
as we get to a certain point our
training error keeps going down down
down but our validation error actually
sort of plateaus and then starts Rising
again once it starts rising and there's
a difference between the training
performance and the validation
performance this is when overfitting is
happening so wouldn't it be nice if in
the training process we could just keep
going until we start to see our
validation error rise and then just stop
yes it would be and that's exactly what
early stopping does early stopping
basically says that as we're training
our model we can watch our validation
error and if it stops improving or if it
even gets worse then we should probably
stop training our model because it's on
its way to overfitting and it's pretty
much as simple as it sounds we watch our
training in our validation error through
this entire process and if we see that
the validation error isn't improving
then we stop and we return whatever
parameters of our model got us to this
place now typically we want to give our
Network a little wiggle room and so we
have a variable called the patience for
early stopping which basically means
that we set a certain number of
iterations where our validation error
can remain the same or maybe even get
worse and we'll still continue to train
the model just to see if maybe the
validation error will get a little bit
better in a few iterations in other
words we can basically set something
like five iterations and say look if I
see my validation error plateauing or
getting worse I'm gonna to give you five
more iterations to see if it keeps
improving and if it doesn't I'm just
gonna stop there and that's pretty much
all early stopping is we're monitoring
the training process using a validation
set in order to see if we can detect
when overfitting starts happening and
stop our training at that point all
right last but not least let's talk
about batch normalization now if you've
heard of batch normalization before you
may have not heard about it in the
context of regularization but actually
it turns out that it does regularize our
models in our previous course we talked
a little bit about the importance of
scaling our data or normalizing it
before putting it into our model for
instance we could see score Data before
putting it into the model batch
normalization essentially works very
similarly but instead of just
normalizing Data before we put it in the
model we actually normalize the output
of all or sum of the layers inside of
our neural network so when we have the
output of a layer we then normalize it
it before sending it off to the next
layer now there's not yet been any
consensus about exactly why batch
normalization works but we know that it
has a couple of really cool effects one
of those effects is it sort of Smooths
out the optimization that we're doing
for instance here you can see the
Contour plot of a loss function and it's
really asymmetric right it moves a lot
in this direction and not so much in
this direction this can make training a
little slower because our updates might
sort of zigzag across our space with
batch normalization it tends to smooth
out that surface to make it look a
little more like this
in this case no matter where we start we
have a very smooth and clear path to
quickly get to the minimum of our
function thus speeding up our training
so again batch normalization allows us
to speed up our training by having a
smoother surface that we are optimizing
on and that's allowing us to maybe
increase the learning rate that we use
in order to optimize it also lessens the
impact of the initial weights we talked
very briefly about when we start
training a neural network we first
randomly initialized all of the
different weights of our Network and
then we use something like gradient
descent in order to adjust those weights
to get better and better and optimize
our loss function where we initialize
those weights can have a huge impact on
the solution that our model ends up on
however with batch normalization it
reduces the impact of what those initial
weights are because in between our
layers we're just still normalizing our
output before feeding it into the next
layer and this also has the impact of
regularizing your model and when we talk
about the math in a second you'll see
why so as we're training our Network we
know that typically we're not using all
of our data in order to estimate the
gradient rather we usually use mini
batches subsets of data that we use to
estimate our gradients at every step
batch normalization takes the outputs of
a layer and normalizes them using the
mean and the variance estimated from the
current batch of data points that we're
using to update our gradient in other
words we calculate the mean of our batch
and the standard deviation or the
variance of our batch and then we take
all of our outputs and we just normalize
it by subtracting that estimated mean
and dividing by the standard deviation
this little Epsilon here is just a very
tiny number to make sure that we're not
dividing by zero now that our outputs
are normalized they have a mean of zero
and a standard deviation of one however
sometimes we don't want a mean of zero
and a standard deviation of one so after
we've normalized all of the inputs of
our layer we multiply them by a scaling
Factor gamma and then add a centering
Factor beta these are just trainable
parameters of your network that
essentially rescale and re-center the
new normalized data this allows for the
output to have a mean other than zero
and a standard deviation other than one
now because these estimates of the mean
and variance are just estimated using a
small sample of data this actually adds
noise to the process of training our
model and that noise may be why it has a
regularizing effect on our neural
network all right so today we talked
about why it's important to regularize
our often very complex neural networks
we also talked about different methods
we can use to regularize our model first
we talked about something very familiar
penalization which adds a turn to our
loss function that penalizes the sizes
of our different weights in our model
for instance we could have L1
regularization or L2 regularization
which are like last so and ridge
regression that we covered in our
previous course then we talked about the
concept of bagging bagging is when we
take different sub samples of our data
set and train different models on those
different sub samples then once those
models are all trained we use them
together by averaging their output in
order to make a prediction while
computationally expensive we talked
about how it's possible to do this with
neural networks or just to train neural
networks with different initial weights
or other randomized processes so that we
get a bunch of different neural networks
that have slightly different structures
that we can then average together this
helps regularize our model because not
every one of our models is seeing the
exact same data meaning that it can't
over fit to a certain pattern in the
data this is very similar to the idea
behind Dropout which is a very common
during training Dropout selects random
nodes to just completely remove from the
model by multiplying them by zero we do
this over and over during the training
process so that while our model is
training it only ever sees some of the
different nodes
this results on a network that can't
over rely on certain patterns or
Connections in our Network because while
it's training we never know when those
nodes are suddenly not going to be there
next we talked about early stopping
which is monitoring the training process
to see when our validation error starts
to Plateau or even get worse at that
point it seems like our model is going
to start overfitting and so we want to
stop our training process there so that
hopefully we can prevent that
overfitting thus regularizing our model
and last we talked about batch
normalization batch normalization takes
the output of a layer and centers it
using the estimated mean and standard
deviation from the current batch of data
we're training on in order to have that
output have a mean of zero and a
standard deviation of one these
normalized values are then multiplied by
a scaling value and added to a constant
so that our output can have a different
mean than zero and a different standard
deviation from one while we don't
totally know why AI batch normalization
works we do know that it has a
regularizing effect on our model and it
also can speed up the training of our
model by sort of smoothing out our
gradient and allowing us to take bigger
steps no matter which techniques you use
regularization is a very important part
of building neural networks which often
can be very complicated and prone to
overfitting often it's said that the
best way to build a neural network is to
build one so complicated that it starts
to overfit and then use regularization
to reduce that overfitting alright
that's all I have for you I will see you
next time

 
hello and welcome to your second
recurrent neural networks lecture in the
last lecture we talked a little bit
about recurrent architectures in neural
networks recurrent architectures are
special because they take the output of
a cell and they feed it back in at the
next time step this is unlike feed
forward neural networks which simply
pass values from layer to layer we are
now taking outputs of a layer and
feeding them right back in now this
structure represents a single cell which
technically we cycle through as we feed
values of a sequence in and for that
very reason we often show recurrent
neural networks in their unrolled state
where we basically take copies of that
same cell and show them repeated over
time so that you can kind of get a sense
of what is happening when we feed
sequential data through that cell for
example here you can see with the simple
recurrent architecture we are taking in
inputs and doing some math to produce a
hidden State and that hidden state is
both an output as well as an input in at
the next time step and then we're going
to take in a new input create a new
hidden State feed that to the next time
step input hidden State feed that
through all the way through the end of
the sequence where we finally get our
output value in a simple recurrent
architecture in order to create a hidden
State we take in some input like a stock
price or a word and we take in the
previous hidden state if there is one
and we combine them using weights and
adding a bias then we send all of that
through a hyperbolic tan activation
function which as you remember squishes
values between negative 1 and positive
one the resulting output is our new
hidden state which could either be our
final output or could be fed to the next
cell in the series one thing we didn't
talk about in the last lecture is that
these hidden states don't necessarily
have to be a single value they could be
vectors of different values and
therefore when you're building a
recurrent Network you often have to
choose the dimension that you would like
to have of your hidden state but simple
recurrent neural networks have a big
problem and for this reason we actually
typically do not use Simple recurrent
architectures in real life understand
what this problem is with these simple
recurrent architectures we need to
review gradient descent remember that
gradient descent is how neural networks
learn and update all of their parameters
gradient descent uses the gradient along
with a learning rate to make adjustments
to all of the different parameters in
our model since the learning rate and
the gradient determine exactly what type
of change that we're going to make to
our parameters if either of these values
are zero then we're not going to update
our parameters at all and this can be a
challenge especially for earlier
parameters in our model like say this
wait one now this is an incredibly
simple architecture there's no
activation functions there's only one
layer of nodes but still we have a very
deep Network and so when we're trying to
figure out the partial derivative of the
loss function with respect to this
really early weight we need to go
through the entire network because
changing weight one will then change X
which will then change K which will then
change J which will then change H which
will change the output of our model
which will then of course affect the
loss in order to figure out how weight
one is going to impact our loss we have
to multiply many partial derivatives
together even in this very simple
Network first we have to consider how
does changing the output change the loss
and then how does changing H change the
output then how does changing J change H
then how does changing K change J that
is changing X Change K and last but not
least how does changing weight One
affect X when we multiply all these
partial derivatives together we get the
effect of changing weight one on the
loss function and again just to remind
you this is a very simple network with
only one node per layer and we don't
even have activation functions yet but
when we think about these really long
chains of partial derivatives that we're
multiplying together ask yourself what
would happen if a lot of these values
were very small say less than one well
if that were the case and we multiply a
bunch of very small numbers together
eventually our final gradient is going
to be very small and might even approach
zero and remember we just talked about
the fact that if our learning rate or
gradient are zero then effectively our
parameters are going to stop updating
this is the vanishing gradient problem
if our gradient gets very small close to
zero then effectively our network is not
going to be able to learn for instance
with this weight one we might not be
able to effectively update weight one
because the gradient is so small and the
longer the chain of partial derivatives
that you have to multiply the more
chances we have to have these partial
derivatives that are very small which
then lead to that very small gradient
one thing that might make a bunch of
these partial derivatives very small is
if we're adding activation function to
our Network that are using something
like a sigmoid or a hyperbolic tan
activation function if you look at the
graphs on the bottom of the screen you
can see both the sigmoid and hyperbolic
tan activation functions as well as
their derivatives and what you can see
from their derivatives is that their
derodas tend to be very small and most
importantly at the extreme ends their
derivatives are very very close to zero
thus if we have some partial derivatives
in our chain that are relating to the
sigmoid or the hyperbolic tane
activation function we may have a bunch
of different very small derivatives and
that can lead to the vanishing gradient
problem
and if you remember recurrent
architectures tend to have these
s-shaped activation functions and thus a
lot of the values in these chains of
partial derivatives are probably going
to be very small
in recurrent neural networks this is
especially difficult because we're
taking long sequences of values and
sending them through the same cell over
and over and that cell contains a
hyperbolic tan activation function
therefore once we enroll that recurrent
neural network we essentially have this
very deep architecture each layer of
which has one of those hyperbolic tan
activation function thus simple
recurrent neural networks especially
suffer from the vanishing gradient
problem and just a reminder that when we
have the vanishing gradient problem it
means that the parameters in our model
will not effectively be able to update
and thus we won't actually be able to
learn good values for them so we
probably want to do something about this
vanish ingredient problem well there's
been updated more modern versions of
their simple recurring architecture one
of which is the lstm or long short-term
memory Network before we get into the
details of an lstm let's quick quickly
revisit the idea of a residual
connection that we talked about when we
learned about convolutional
architectures the idea behind residual
connections is that we want to preserve
the original information of our input
even as we send it through different
processing layers like in a
convolutional net a convolutional filter
or a pooling filter with a residual
connection we basically take our input
and have two split paths that it goes
through the first path is just your
typical path we take our input and we
put it through our processing layers
like convolutional filters and pooling
but then we have another path and in
that path basically the input is
unultured and when we do that we take
these two paths and then combine them at
the end so we take the altered version
of the input as well as a copy of the
original input and add them together
this allows the signal in the original
input to be preserved while also taking
advantage of all that processing that
we're doing residual connections help
deal with the vanishing gradient problem
and therefore allow us to build deeper
and deeper networks which typically will
give us better performance to review
residual connections basically maintain
a copy of the input across time even as
another copy of that input is being
processed then at the end we add those
two things together in a recurrent
architecture this idea of residual
connections takes the form of something
called a cell state which is essentially
like a long-term memory this could be
useful especially when processing
sequences for instance take this
sentence my friends are going to the
market today to see Bob he is a really
cool butcher a long-term memory could
help you remember important things like
what the subject of a sentence is for
instance at the beginning of the
sentence we see that the subject is my
friends my friends are going but then as
we move through the sentence we get an
updated subject when we see the word Bob
now Bob is the subject he is a really
cool butcher this long-term memory can
be helpful to preserve and allows you to
record information over time this is why
an lstm actually has two states that we
need to keep track of first we have the
typical hidden state that we're used to
from a simple recurrent network but now
we also have a long-term memory or the
cell state in this architecture the cell
State basically Works sort of like a
residual connection preserving
information over the long term while the
hidden state is more like a short-term
memory alright I know this looks
complicated but we are going to go
through each part of this lstm cell just
like our simple recurrent architectures
we're going to have a couple of inputs
first we have our actual input so this
could be a word a stock price or
anything else that we're recording in a
series then we have our previous hidden
State this if it exists is the hidden
state from the previous cell and
similarly we have the previous cell
state which is the cell state from the
previous cell we we are going to take
these three things and combine them
we'll talk about how in a minute and
then get a new outputted current cell
State as well as a current hidden state
so the inputs and the outputs to our
cells are relatively similar to a simple
recurrent network with the addition of
this long-term self-state so one thing
that's different about an lstm compared
to a simple recurrent architecture is
that we now have these gates gates
basically determine how much of
something we should forget or remember
or update the first gate we're going to
talk about is the forget game the forget
gate takes in two inputs our previous
hidden State as well as our current
input it then combines them together
using weights and a bias and sends them
through a sigmoid activation function
like the hyperbolic tan activation
function the sigmoid function is an
s-shaped curve that squishes all of our
input values however a sigmoid
activation squishes them between 0 and 1
rather than negative 1 and 1 which is
what the hyperbolic tan function does
the output of the forget gate basically
tells us how much of the previous
long-term memory the cell State we
should forget we take these output
values and we multiply them by the cell
state which allows us to remember or
forget various parts of it for instance
if our new input shows us that we have a
new subject we should then forget that
part of the long-term memory you can see
what's going on here mathematically
we're taking in our previous hidden
State as well as our current inputs we
are combining them using some weights
and adding a bias and then we're sending
everything through a sigmoid activation
function the output of this forget gate
tells us how much of our long-term
memory the cell State we should forget
next we have the input gate and the
input node first let's talk about the
input node which is this part right here
it takes in two values the previous
hidden State as well as the current
inputs combines them together using
weights and a bias and then sends them
function the resulting values are
basically a proposed update to the cell
state or long-term memory again
mathematically you can see what's
happening here we're taking our previous
hidden State our current inputs
multiplying them by some weights adding
a bias and sending it through the
hyperbolic tan activation function the
result give us a proposed update that we
can make to our long-term memory to go
along with the input node we have an
input gain like the forget game the
input gate takes in two values the
previous hidden State as well as the
current inputs it then combines them
output of this input gate basically
tells us how much of that proposed
memory that we generated using the input
node should actually be added to our
long-term memory the self State thus in
order to actually update the cell State
we take the proposed update generated by
the input node multiply it by the values
generated by the input gate and add them
with the previous cell State this gives
us our updated cell State representing
the changes we made to our long-term
memory last but not least we have an
output node and an output gate the
output gate and output node basically
determine how much are now updated cell
State should affect our hidden State
just like our other Gates the output
gate takes in two values the previous
input it then combines them using a set
of weights and a bias and sends them
through a sigmoid activation function to
squish the values between 0 and 1. this
will determine how much of our long-term
memory the cell state is going to affect
our hidden state in order to update that
hidden State we take our now updated
cell State and send it through a
hyperbolic tan activation function we
then multiply it with the results of
that output gate and that gives us our
new hidden State just to review we
covered a few different Gates and nodes
first we have the forget game the forget
gate tells us how much of our long-term
memory that's in the cell State should
be forgotten then we talked about the
input gate the input gate determines how
much of our new proposed memory should
be added to our long-term memory the
cell State the weights generated by the
input gate are then multiplied by that
proposed update to our cell State last
but not least we talked about the output
gate the output gate determines how much
of our now updated cell State should be
added to our hidden State when we
combine these Gates and nodes together
we get our new updated cell State as
well as our new updated hidden state
which can then be fed into the next cell
in the sequence even though it's a more
complicated architecture the ideas
behind an lstm are similar to a
recurrent Network thus we can also show
an lstm unrolled where we have our input
cell State our input hidden state which
then goes through a bunch of math
outputs in new cell State and hidden
state which is then fed into the next
cell in the sequence this of course is
repeated over and over until the end of
the sequence where we get our outputted
values in this case when we make
predictions for example of words or
stock prices we're using the final
hidden state in that sequence like with
feed forward neural networks deeper
architectures often perform better and
this is mostly the case with lstms as
well typically when we build an lstm
we'll have three main layers we'll have
an input layer that basically brings in
the data then we'll have our lstm layer
which of course will sequentially
process that data finally returning our
desired value that value is then fed out
through an output layer but there is
nothing stopping us from having multiple
lstms in a row where basically this lstm
layer generates a sequence and this lstm
layer processes the sequence generated
by the previous lstm layer thus we also
build deep lstms which have multiple
lstm layers in between the input put an
output however unlike feed forward
neural networks there tend to be
diminishing returns with the number of
lstm layers that you have so in practice
most people use about two or three and
after that it tends not to be super
effective and while the lstm gets around
some of that Vanishing gradient Problem
by having that long-term memory through
the cell state it does have a lot of
operations and parameters to learn thus
people have proposed the Gated recurrent
units or grus as an alternative to the
lstm architecture grus are still a
recurrent neural networks but they're a
bit simpler this is a Gru and one thing
you might notice is that we no longer
have a cell State instead we only have a
hidden State and actual inputs much like
a simple recurrent cell unlike an lstm
which had three gates the forget gate
the input gate and the output gate a GRU
only has two you an update gate and a
reset gate let's talk a little bit about
how these work first let's talk about
the reset game like all the gates in an
lstm it takes in two values the previous
input it combines those together using
weights and a bias and then sends it
again you can see what's happening here
mathematically we are taking our
previous hidden State our current input
combining them using weights and a bias
and sending it through a sigmoid
activation function the reset gate is
going to help us determine how much of
our proposed update to the hidden State
should be based on the previous hidden
State remember because the sigmoid
activation function squishes values
between 0 and 1. the output of this
reset gate basically says What
proportion we should remember from our
hidden state if the output of the reset
gate is very close to zero our new
proposed update is going to have very
little to do with the previous hidden
and a lot to do with our input if the
outputs of the reset gate are very close
to one then our previous hidden state
will play a big role in what the new
updated hidden State should be next we
have the update game the update gate
determines how much of our new hidden
state is just going to be a copy of the
previous hidden State like all of the
gates before it the update gate takes in
two inputs the previous hidden State as
well as the current input it then
combines them together using weights and
a bias and sends them through a sigmoid
activation function again mathematically
you can see what's happening here the
update gate basically generates a value
between 0 and 1 that represents the
proportion of our new hidden state that
will be due to the update and What
proportion will be due to the previous
hidden State we also then have a node
that determines what our proposed update
to the hidden State should be this takes
in two inputs first it takes in the
current input then it also takes in the
previous hidden state which has then
been multiplied by the output of our
reset gate you can see in the formula
what is happening we're taking in our
current inputs unchanged but when we
take in our previous hidden State we're
multiplying it by those values between 0
and 1 that we generated using our reset
gate these are then combined together
using weights and a bias and sent
function the resulting output is our
proposed update to our hidden state that
we can make in order to actually get our
new hidden State we're going to combine
the proposed update with the previous
hidden State and to do that we're going
to use the output of the update game
remember the output of our update gate
is just a value between 0 and 1 and it
represents the proportion of our new
hidden state that will be affected by
the proposed update the bigger this
value is the more our new hidden state
will be due to the updated value and the
less it'll have to do with the previous
hidden state so we multiply our previous
hidden state by 1 minus the value of our
update gate and add the value of our
update great times the proposed hidden
state this combined gives us our new
current
just to review the gru has only two
gates first it has our reset gate which
tells us how much of the previous hidden
state is going to go into the proposed
update that we make then we have our
update gate which determines how much of
the new hidden state is just a copy of
the previous hidden State and how much
of it is due to this new proposed update
that we make like with simple rnns and
lstms Gru's are just cells that we're
going to sequentially send data through
and that's we can also show them
unrolled where we show the same copy of
the gru cell at multiple steps in a
sequence very similarly to our simple
recurrent Network at every state we take
in the previous hidden state if there is
one and a current input we then output a
updated hidden state which is then fed
into the next cell which generates its
own hidden state which is then again fed
into the next cell like with the
previous two architectures we talked
about the final hidden state is going to
be our prediction for instance for a
stock price or for a current word so
we've talked about a lot of different
architectures in the previous lecture we
talked about a simple recurrent
architecture this just takes in our
input values and our hidden States
combines them and sends them through a
hyperbolic tan resulting in our new
hidden State however simple rnns suffer
greatly from the vanishing gradient
problem and thus proposed updates to the
architecture have been made for instance
we then talked about the lstm the lstm
actually has two states to keep track of
first we have our traditional hidden
state which represents basically a
short-term memory of the cell and then
we have a cell state which basically
represents the long-term memory of the
cell lstms have three gates the forget
gate the input 8 and the output gate
that help us make adjustments both to
the cell State as well as to the hidden
State lastly we talked about grus which
are a much simpler architecture than the
lstm but perform a similar function in
the gru we no longer have a cell State
and in fact we have fewer Gates as well
instead of forget input and output we
just have two the reset gate and the
update gate together these two gates
decide how much of our previous hidden
State we should use in the update and
then how much of the new hidden State
should be affected by that update do you
use because they're a simpler
architecture and have fewer parameters
can often be a little bit simpler and
easier to train than an lstm however
there's no consensus about which is
better and often there's different
situations where one might work better
than the other and recurrent
architectures in all three forms have
been used to do some very cool things
one thing is image captioning where we
take an image input it into a network
and ask it to generate a caption for
instance here you can say a cute little
dog sitting in a heart drawn on a Sandy
Beach next recurrent architectures have
also been used for speech to text for
instance if you're using Siri to send a
text message it turns a sequence of
sound into a sequence of words like if
you say hey Mom can you pick up milk
another common application of these
recurrent architectures is machine
translation for example things like
Google translate where you put in a
sequence of words in English like I like
dogs and then ask for the same sequence
in a different language for instance you
could say
and while recurrent architectures have
they do still have some problems one of
the limitations of these types of
architectures is that they're difficult
to parallelize and they're slow to train
this is largely due to the fact that
training these types of architectures
requires you to feed items in a sequence
into the network sequentially this makes
it very difficult to parallelize that
process because we can't give different
parts of that process to different cores
lastly even though architectures like
the lstm and the gru do help with the
vanish ingredient problem that we
especially observe in simple recurrent
networks it still doesn't go away
especially for a very long sequences and
speaking of very long sequences these
type of architectures can sometimes have
difficulty remembering things long term
thus if something really important was
said very early in a sequence later in a
sequence it can be hard for that Network
to remember that earlier thing that was
shown thus while recurrent architecture
is especially lstm and grus are still
used today there are more modern models
for processing sequential data which
we'll cover in a future lecture alright
that's all I have for you I will see you
you next time

 
hello and welcome to your first lecture
on convolutional neural networks today
we're going to talk about some new types
of layers that we can use that can help
us process spatial data usually in the
form of an image so first let's talk a
little bit about spatial data like I
mentioned spatial data is data that has
meaningful spatial relationships so far
the data that we've been using like just
a bunch of predictors or like Health
variables to predict if someone is going
to have a cardiac event none of those
have spatial relationships to them but
when we start processing things like
images they do have spatial
relationships the different inputs aka
the pixels do have spatial relationships
together that we want to take into
account when processing them take this
8-bit junamo picture from stardew Valley
as an example when we're processing an
image it doesn't just necessarily matter
what each pixel value is rather we might
want to look at certain areas of the
picture as a whole for example just
knowing this specific pixel value
outlined in red doesn't really tell us
much about what's going on in the
picture but if I look at a little bit of
a larger area now we're starting to get
more spatial context which helps us
understand what's going on in the
picture and we can have an even bigger
space that we look at the basic idea
here is that because images have spatial
relationships we want to take advantage
and consider those spatial relationships
when processing them typical dense
layers that we've been using so far for
our feed forward neural networks don't
actually take into account those spatial
relationships so we need layers of a
neural network that can let's talk about
some of the cool things that you can do
when you do consider spatial
relationships one thing we might often
want to do with images is put filters on
them to try and find features or do
something to the image for example
here's a gorgeous perfect picture of my
Corgi puppy let's apply some image
filters to it so we can see what happens
first here we're applying something
called a sharpening filter this
increases the contrast between pixels by
taking a pixel and subtracting the
neighbors around it making it stand out
more compared to the neighbors and you
can clearly see the effect that this had
on the image as we took this filter and
slid it across the image it increased
the contrast giving us this kind of
grainy looking result another popular
filter is a blurring filter
this filter when slid across the image
to transform it does exactly what you
see here it blurs the image it does this
by replacing a pixel value with the
average of all the pixels around it
making everything more average and
therefore looking a little more blurry
next we have two really interesting
filters this is a horizontal Edge
detector and you can tell that the areas
in this output that are white are areas
where we had horizontal edges in our
original image for example look at the
original image here is my dog's collar
this is a very clear horizontal Edge
because the collar color contrasts
really starkly with the color of her fur
so we have this very clear edge of where
her fur ends and where the collar begins
we can clearly see this Edge come out
using the edge filter here
in just like we have horizontal Edge
filters we can also have vertical Edge
filters here you can see anytime we had
a vertical Edge it pops up in this
output when we take this filter and
slide it across our image it will detect
any time there is a very stark contrast
in a vertical Edge
for example we see a really bright line
right here looking at the original image
we can see that that's where my dog's
fur goes from white to this tan color so
we see there's a very sharp demarcation
there and that shows up perfectly
with our vertical Edge filter so we just
saw how different types of filters can
be swept across images to give us more
information about that image for
instance where edges shapes colors Etc
are applying a filter to an image in
this way is called convolution the idea
behind convolution is that we're taking
a filter sometimes called a kernel and
we are sliding it across our image bit
by bit you can see in this GIF here how
that filter moves across the image it
basically just moves pixel by pixel
until it's covered the entire image this
process is called convolution every time
the filter lands on a place on the image
it does a mathematical operation in
order to calculate an output that output
is what is shown in the output of our
filtering process and again here's
another example of we've just slid the
filter a little bit and we're going to
do a mathematical operation to get our
output now we'll talk more about this
math a little bit later but for now the
idea is that a convolution takes a
filter and slides it across our image
giving us some kind of output maybe
something like an edge detection or a
blur one concept that's really important
in convolutional neural networks is this
idea of weight sharing we just talked
about the fact that convolution takes a
filter and slides it across the image
and that filter doesn't change depending
on if it's in the top right corner or
the bottom left side of the image this
idea is called weight sharing we use the
same weights for our filter no matter
where we are in the image this is really
helpful because if you think about the
way that you process images you can
recognize that there is a cat in the top
left corner of an image in the same way
that you can recognize that there's a
cat in the bottom middle of the image by
weight sharing our filter can detect
different objects no matter where in the
image it is for example we don't have to
learn how to detect flowers in the top
right differently than we learn to
detect flowers in the body bottom left
because we're using the same filter if
it learns to detect flowers it can
detect them no matter where it sees them
and while weight sharing does have some
biological inspiration it also has
another huge benefit it makes the
parameters of our model a lot smaller if
we had to learn a filter for every
single location on the image that would
hugely increase the number of parameters
that our model has to learn and
therefore weight sharing both has
biological inspiration from how we see
but also makes our convolutional neural
networks a lot easier to fit alright so
I've been hinting at it the whole time
we've talked a little bit about
convolution and how it takes a filter
and slides it across an image to give us
some type of output but how does that
relate to neural networks and everything
we've been talking about so far well
what if we didn't have to choose the
filters we used on our images I showed
you a couple of really popular image
filters but honestly it's kind of
difficult to figure out what filter you
would need in order to classify an image
or have a computer classify an image so
what if we didn't choose them but let
our neural network choose them well
that's exactly what happens in
convolutional neural networks we have
filters that are learned through
gradient descent and back propagation
just like the normal weights in a feed
forward neural network convolutional
neural networks have a very similar
structure to feed for neural networks in
that we're just sort of building
sequential stacks of layers most of the
layers in this network represented by
Blue in this image are convolutional
layers where multiple filters are
applied to an image and then the output
is fed into the next layer then we often
take that output and compress it to make
it a little smaller and then do more
filters and then compress and do more
filters and compress and do more filters
this allows convolutional networks to
have hierarchical feature detection this
basically means that the purpose of all
of these convolutional layers is to
learn bigger and bigger features so for
instance this is a neural network trying
to detect if something is a cat or not
and maybe the first few layers of the
network learn very basic shapes or edges
but then in the next layer we learn a
little bit more complex features so for
example we might learn what the eye of a
cow looks like or the nose or an ear
and then we take all of those together
at the very end to use and see if an
image is a cat because we're thinking of
these convolutional layers as sort of
feature detection and generation we
often use relu activations on each of
these layers remember radioactivations
are sort of modeled like the neurons in
your brain which require that you meet a
certain threshold of activity before it
will activate the relu activation takes
any input that's negative and sets it to
zero any input that's positive is return
unchanged meaning that the more
activation there is the more output
there is as long as you've reached this
threshold these radioactivations
basically act like a is this feature
there or not filter on the output of
your model so going back to this image
of a really popular convolutional neural
network you can see that it's made up
mostly of a bunch of convolutional
layers with a relu activation applied
thus it's sort of doing feature
detection with the filters and then
having the rayloactivation say whether
or not those features are there alright
it's time let's dive a little bit deeper
into the math of what's happening when
we do a convolution remember the idea
with a convolution is that we're taking
a static filter and we're sliding it
across our image what's actually
happening when we slide that filter
across the image is we're taking the
values or the weights in the filter and
we're multiplying them with the pixel
values of whatever area of the image our
filter is currently Above So for that
current Corner that our filter was in
what we're doing is we're taking all of
the weights in our filter and element
wise multiplying them with each pixel so
what that looks like is we're taking
this weight and multiplying it by this
value
and then we're taking this weight and
multiplying it by this value and we do
that for all of the different pairs of
weights and pixels and in the end we add
them all together once we do that we get
the output and so this is the output of
this filter multiplied by this area of
the image to give you a slightly more
mathematical look at what's happening
here's another example of what a
convolution is again we're taking a
filter often also called a kernel and
we're sliding it over the input which is
our image the resulting value is often
called the output or the feature map and
you can pause here if you'd like to see
a little bit more about how that works
so let's apply a blurring filter like
the one we saw earlier on my Corgi puppy
to this simple grayscale image what you
see here on the screen is a live output
of the results of taking a filter
multiplying it by the little area of the
image that it's over and then adding
them all together to get the resulting
output and this is our resulting output
this should make sense when we look at
our original image we see that there's a
lot more contrast a lot more really
light colors and a lot more really dark
colors but we're using a blurring filter
which averages pixels with the pixels
around it so it makes sense that we have
this sort of medium gray output now one
interesting thing to notice here is that
our original image was a 10 by 10 image
it had 10 pixels across and 10 pixels
down but the output that we see here is
an eight by eight Matrix that means that
when we have a 10 by 10 image and a
three by three filter our output is
actually going to be smaller than the
input it's going to be 8x8 and this
should make sense when you kind of think
about how the filter is moving
throughout the image so this is our
original 10 by 10 image and I've
basically shown all the possible places
that the filter could be at least when
it's in that top row because the filter
can't move past our image there are only
eight valid placements that it can have
as it moves horizontally throughout the
image and this is actually true when it
moves vertically as well hence why we
get an 8x8 output even though our input
image was a 10 by 10 image so what if we
don't want the output to have a
different dimension than the input it
could be really useful if our input
image and our output image have the same
size well one way we can achieve that is
with padding padding is basically the
idea that we take some extra pixels
along the edge of our image that allow
our filter to move across our image so
that the output is the same Dimension as
the end here because we're using a three
by three filter all we need to do is add
one pixel around the edge of the image
typically we just set these pixels to be
zero because of this extra padding we
can see that the filter actually has 10
valid placements that it can have as it
moves horizontally across our image that
means that our output will be 10 by 10
just like our input was
so adding padding in this case one pixel
along the border of our image allowed us
to have the same input and output size
typically when we don't have padding the
size of the output is reduced by n minus
1 for an N by n filter because we have a
three by three filter that meant that
our size was reduced by three minus one
or two hence why a 10 by 10 image became
an eight by eight so let's talk about a
few different types of padding the first
padding is just no padding at all this
is often referred to as valid padding
because our filter will only go over the
valid pixels in our image this has a
disadvantage of having output that is a
different size than the input we can
also have same padding which is what we
just talked about before this this is
where we add pixels to the edge of our
image so that the input and the output
shapes are exactly the same but one
thing to note is that when we use this
type of padding pixels around the edge
of our image have less influence on the
final output of our output this is
because pixels at the edge don't
actually get to go through our filter
the full nine times for a three by three
filter for example this current pixel
will never be in any of these positions
in the filter just because of how the
filter slides over the image thus it
will only have four opportunities to
influence the output of our image pixels
in the middle of our image however will
get nine chances to influence our output
because it will get to take every single
one of the nine different positions in
our filter as the filter slides over the
image if we want to correct this we can
use something called Full padding full
padding adds enough padding around the
outside of your image that every single
Pixel in the air original image has the
same influence on the output for
instance you can see now this pixel in
the top corner will have nine chances to
influence the output of our convolution
now a really quick detour before we move
on we've so far been working with
grayscale images so that our
convolutions are really simple we're
just taking a simple two-dimensional
filter and sliding it across a
two-dimensional image however most
images are actually three-dimensional
because they have a red a green and a
blue Channel instead of just one that
has this scale of gray it can be a
little bit tricky to think about how
convolution works when your image is
technically a stack of three images red
green and blue so I wanted to show you
an animation I found that is really good
at explaining this this animation shows
how convolution works when you have a 3D
image that has a red green and blue
channel here you can see the r g and B
channels of the input image here with
some zero padding around the edge and
then here you can see our first filter
and our second filter convolutional
layers often have multiple filters
instead of just one and in this case we
have two you can see that the idea is
the same we take our filter which is now
three dimensional and slide it across
our now three-dimensional image we still
do the same element-wise multiplication
and then add a bias and that's our
output which you can see here as the
animation moves through all of the
different places that the filter can be
you can see that we get our output this
top one is the output for the first
filter and this bottom one is the output
for the second filter I highly recommend
going and checking this animation out
it's linked in the extra resources on
our course GitHub because you can also
see here a little bit about how the
dimensions of input filters and output
works you can see here that because we
had two filters the output depth is two
if we have 70 filter then the output
depth would be 70. we've learned a
little bit about convolution which is
basically the process of taking a filter
or kernel and sliding it across our
input image to get an output we also
learned about padding which determines
how many times each pixel on the edge of
our image is seen and can help match our
input shape to our output shape but
there's one more thing we need to talk
about and that's strides so far when
we've talked about convolution we talk
about our filter moving one pixel at a
time throughout the entire image however
there's no rule that we have to move one
pixel at a time rather maybe we could
have a stride of two meaning that every
time our filter moves it doesn't move
over by one pixel but two pixels but
let's take a second to think about what
happens when the stride of our
convolution is greater than one alright
I am trusting that you have paused if
you think about it when you slide our
filter and don't go pixel by pixel that
means that our filter is landing in
fewer places as it slides across our
input image that means that our output
will have much smaller Dimensions
because our filter is not Landing as
many places to calculate an output we
briefly mentioned earlier that we often
want to do something like this we want
to down sample or reduce the size of our
images as we go through the different
layers in our convolutional neural
network starting with a big original
image and then hopefully an ending up
with something much smaller this helps
us learn much more compact
representations of our image however
using stride to adjust the size of your
output is typically not the best
practice rather we use something called
pooling pooling takes unstrided output
from a convolutional layer and basically
down samples it the type of pooling
we're going to use most often is called
Max pooling where we look at a region of
our output and just return the maximum
value from that region pretend this is
our output and we have a two by two
filter to do Max pooling now quick note
when we use the term filter for Max
pooling this filter doesn't have any
weights that our network has to learn
it's just a way to compress our image so
when you see the different areas that
are filter
can land on our image what we're going
to do is every time we see one of these
regions we are going to return the
maximum value for that region for
instance in this one it's going to be
six and that's how we get the output
here
when the filter is here the maximum
value is eight and you can see that's
exactly what it output so we went from
this larger input to this smaller output
that preserved or summarized a lot of
the information in that original input
again when we're using Max pooling we're
sliding a filter across our image and we
are taking the maximum value from the
region of that filter for instance when
we look at this grayscale image you can
see we take the maximum value here which
is 127 and we return that
and then we slide our filter over and we
say now what's the maximum value oh it's
157 okay let's return that and again
Etc if you think through the process of
Max pooling you'll see that this reduces
the size of our image thus giving a more
compact representation of that image Max
pooling also helps with translational
invariance translational invariance
basically means that we're still able to
recognize a feature or an object even if
it's changed just slightly so for
example I have these two images that
both have a heart but this one is
slightly moved to the right because Max
pooling summarizes the information in a
region it doesn't really matter if our
heart occurs right here or slightly to
the right Max pooling will just
basically say oh there's a heart
detected in this region and that heart
detection is true no matter if it's here
or just a little bit shifted over so to
summarize pooling which we usually use
max pooling is a way to down sample
feature Maps or the output of our
convolutional layers because we think of
the convolutional layers in our Network
as figuring out and detecting features
Max pulling works best because when
you're looking for a feature you just
kind of want to know whether it's there
or not not what its average presence is
over the entire space for instance say I
have this 4x4 image and there's a heart
in there I just want to know is there a
heart in the region not what is the
average presence of the heart across
this entire region which would be you
know a quarter of a heart because
pooling allows us to down sample but
still take advantage of the full output
of the convolution it's best practice to
use unstrided convolutions and then down
sample with Max pooling rather than to
use strides as a way to reduce the size
of our output alright so now we know a
lot more about the structure of this
convolutional neural network you can see
that just like a feed forward neural
network we're just stacking a bunch of
layers together typically what we'll do
is one or more convolutional layers with
a Rayleigh activation followed by a
pooling layer to reduce the size of our
output and then we repeat the process
maybe a couple convolutional layers and
then a pooling layer and then some
convolutional and then pooling and then
convolutional and pulling etc etc until
we have a very small representation of
our image remember the purpose of the
convolutional and pooling aspects of our
confidence are to generate these
hierarchical feature representations and
once we have those we might want to do
something with it for instance we might
want to classify whether an image is of
a cat or not
in order to do that we can take the
output of all of our different
convolutional and pooling layers and put
it into a traditional feed forward
neural network just like the types
you've learned about before this feed
forward neural network takes the
features learned by the convolutional
part of the architecture and then uses
it to classify an image as is a cat or
isn't a cat alright so to review we just
learned about convolutional architecture
and neural networks convolution is the
process of taking a filter AKA a kernel
and sliding it across our image that
filter gives us information about what
is in that image convolutional neural
networks use convolutional layers that
learn what the different filters should
be to filter our image at each stage
these are combined with Rayleigh
activations to sort of do a little bit
of feature detection to see whether or
not a feature learned by the filters is
present in order to get a more compact
representation of our image we often use
pooling layers to down sample the output
of our convolutional layers into a
smaller output size this gives
convolutional networks a bit of a
pyramid shape where the bottom is quite
wide and it gets narrow and narrow as
you get to the top once the
convolutional part of the neural network
has learned features from the image we
often flatten and then feed these
features into a feed for neural network
in order to do something like image
classification to say whether or not a
picture is an image of cat alright
that's all I have for you I will see you
next

 
hello and welcome to your second
convolutional neural network lecture
today we're going to talk about more
Advanced Techniques when working with
convolutional neural Nets as a review
from the last lecture we learned about
basic convolutional neural network
architecture for example we learned
about convolutional layers and pooling
layers convolutional layers take
multiple filters slide them across our
image and use them to generate features
then we often put those features the
output of the convolutional layer into a
pooling layer this allows us to down
sample our output so that we have a more
compressed representation of our image
typically confidence will have deep
architecture meaning that there's
multiple layers of this convolution
pooling convolution pooling architecture
throughout the entire network once we've
reached our final convolutional layer we
typically flatten and then feed that
output into a feed for neural network to
do something like an image
classification now one thing we talked
about last lecture is that convolutional
neural networks have a lot of parameters
and sometimes we just don't have a lot
of data in order to train the model
labeled images that are labeled in the
exact way that you want for your problem
are sometimes difficult to find this can
lead to overfitting in our neural
network one way to regularize our neural
network that's specific to convolutional
neural networks is data augmentation
when we don't have a lot of data one
thing that we can do is we can feed the
same image into our network over and
over but with slight changes for
instance here you can see that our
original image of the cat has been moved
a little bit maybe it's twisted or
rotated maybe it's slid over a little
bit and all of these pictures are just
slightly different representations of
our original cat image but we want our
neural network to be able to recognize a
cat no matter if it's rotated a little
bit or a little bit to the left and so
feeding in these images to the network
can be helpful for it to learn what a
cat is regardless of tiny changes in its
position there's many ways we can tweak
our image when doing data augmentation
we could crop our image we could flip we
could translate it or rotate it we could
zoom in on the image or maybe mess with
the contrast and brightness
on the right hand side you can see how
the picture of the dog has been
augmented in different ways by flipping
maybe rotating translating zooming or
making any of those other changes using
data augmentation to change our images
slightly before feeding them through the
neural network allows it to learn a
little bit better and hopefully be a
little bit less overfit another issue
that our networks can have especially
these very deep networks that we tend to
be building is something called The
Vanishing gradient problem if you think
about a neural network it's really just
a function of a function of a function
of a function our first layer transforms
our output which is then transformed by
the next layer which is then transformed
by the next layer Etc and Etc one thing
that can happen when we're feeding
information through these very deep
neural networks is that at every one of
these Transformations at each one of
these layers noise is added to the
process and that noise once we do this
over and over and over can sometimes
overwhelm the gradient leading to our
model not actually being able to learn
anything about our parameters your
textbook Likens this is the game
telephone where you whisper in someone's
ear and then they whisper what they
think they heard into someone else's ear
and then they whisper Etc and Etc down
the Chain by the time you get to the end
of that chain the message is pretty
garbled and the same thing can happen
with your neural network again causing
your model to not be able to learn and
this is a problem because deeper
networks allow us to learn more complex
relationships which often lead to better
performance but because of problems like
the vanish ingredient we can only build
our Network so deep so we're always
looking for ways to increase the depth
of our networks and prevent problems
like the vanish ingredient one important
way is a residual connection the idea
here is that we basically take the
output of a layer and add it back to the
original input you can see a diagram of
this here when we have been input into a
layer we first pack asset you know
through a typical layer maybe do some
convolutions but then we also take an
original
of that information and add it to the
output of our convolutional layer so
again what we're doing is we're taking
an original copy of the data and we are
adding it to the output of our
convolutional layer this preserves the
original signal in the data allowing us
to build deeper and deeper networks with
less of an issue with the vanish
ingredient however one thing I do want
to note is that this was popularized
with a model called resnet back in 2015
and resnet allowed people to build
deeper and deeper models much deeper
than typical models beforehand however
there is some argument about what
actually causes the model to overcome
the vanishing gradient problem because
the network not only introduces residual
connections but also uses relu
activations and has batch normalization
all which might help prevent the
vanishing gradient issue like they say
deep learning is more of an art than a
science and sometimes we don't know
exactly how things work but we do know
that adding residual connections to our
Network allows us to build deeper neural
networks which often can improve the
performance of our model next as I
mentioned in the last slide a lot of our
convolutional neural networks use batch
normalization because we went over this
in the regularization neural networks
lecture I won't dwell too much on it
here but remember it's taking the output
of a layer and normalizing it just like
with residual connections batch
normalization allows us to build deeper
networks which we are always in favor of
next let's talk about something that can
vastly speed up the computational
efficiency of your network typically
when we do the convolution process we're
taking a filter and we're sliding it
across our input maybe something like an
image and this input that we're sliding
our filters over typically has more than
one channel for instance a typical color
image has three channels the red green
and blue channel that means that our
filters tab depth so anytime our filter
lands somewhere and we're actually doing
a multiplication in order to get an
output we are going to have d k by DK
by m multiplications happening these
decays represent the size of the filter
the M represents the depth
so for instance if we have a 5x5 filter
and we have a depth of three because
we're doing it on an RGB image then we
would have 75 multiplications happening
and we know from the previous lecture
that filters land in multiple places
along the image
we'll use the value DP to represent the
number of places that that filter can
land so if our filter can land in DP
places horizontally and DP places
vertically that means that each filter
does this multiplication process DP
times DP times
and of course we have to do this for
every single one of our N filters when
we multiply all of this together we get
that every time we do a convolution
there's going to be M times n times d k
squared times
DP squared multiplications happening and
depending on the size of our inputs and
our filters that's going to be a lot of
multiplications
so what if I told you that there was an
alternative that reduced the
computational expense of doing a
convolution well that's exactly what
depthwise separable convolutions do in a
depthwise separable convolution we take
a one-dimensional filter and apply it
separately individually to each of the
channels in our image just like before
when a filter lands somewhere it will
have to do Decay times Decay
multiplications and then of course it
will have to do this once for each of
the M channels that we have because a
filter can land in DP places
horizontally and DP places vertically we
then have to multiply this by DP times
DP
now because we're applying a filter
separately to each Channel instead of
having a deep filter that filters all
the channels together we now have to
combine this separate input for each
channel to do that we have to do a
pointwise convolution in order the in
order to do this we use an M by one by
one filter so each time this smaller
filter lands it's going to be doing M
multiplications and it can land in DP by
DP places in this Matrix
and of course we'll have to do this once
for each of our filters
now doing a little algebra we can take
this m times DP squared out and we end
up with M times DP squared times
d k squared plus n
so well that was a lot of math that we
just saw what you can see is that the
complexity of applying a typical
convolution and the complexity of doing
a depthwise separable convolution are
different with the depthwise separable
convolution often being a lot smaller or
less complicated to compute now because
in a depthwise separable convolution
we're applying filters separately to
each layer we're assuming the
information in each channel is
independent but this happens to be a
pretty good assumption which is why
duckwise separable convolutions often
work just as well as a typical
convolution again the idea behind
depthwise separable convolutions is that
we take a pretty computationally
expensive process of convolving our
filter with our input and we make it a
little bit simpler instead of using a
filter that has the same depth as our
number of channels so that we can
process all of our channels together we
use individual filters and filter each
Channel separately then we use another
smaller convolution in order to combine
that information from those separate
convolutions of each Channel it turns
out that doing this can often save us
some computational expense alright so we
just talked about a couple tweaks we can
make to convolutional neural networks in
order to improve them now let's talk
about how we can take the output of
convolutional neural networks and
visualize it to learn a little bit more
about the network one criticism people
have about deep learning is that it's
not interpretable you may hear the term
Black Box thrown around because it's
often difficult to tell why neural
networks are making the decisions they
are however with convolutional neural
networks it's often a little bit easier
to tell what the network is doing
because convolutional neural networks
lend themselves really well to
visualization of all of their components
this helps compnets be more
interpretable well there's multiple ways
that we can visualize what's happening
being in a convolutional neural network
let's talk about these three main ways
first we're going to talk about
visualizing the layer activations this
means that we're looking at the output
of each of the layers in our
convolutional neural network next we can
visualize the filters remember in
convolutional neural networks our
network is learning what filters to use
through BRAC propagation and gradient
descent and we may want to know what
were the filters that we ended up with
and last but not least we can look at
visualizing Class Activation heat Maps
this allows us to basically see what the
network is looking at when it's deciding
to classify an image for example if we
classified something as a cat this
visualization could tell us what in the
image did the network seem to think was
most cat-like so in the last lecture we
talked about the fact that convolutional
neural networks have a bunch of
convolutional layers the output of each
of these layers are called that layer
activations because convolutional layers
are taking a filter and applying it to
our image these outputs basically tell
us what was the results of that filter
did we see Edge detection do we see
pattern recognition what do we see in
order to visualize layer activations we
take an image that we're interested in
and we feed it through the neural
network and at each layer of the neural
network we save the output then all we
do is plot it for example here you can
see that for a cat image we have plotted
the layer activation and you can see
that this layer seems to be doing some
sort of edge detection as the edges of
the cat are pretty lit up and remember
that we don't just have one layer we can
plot the activations from each of our
convolutional layers and each of our Max
pooling layers this can help us see what
the neural network is seeing as it
processes our image one interesting
thing to note here is the increasing
sparsity as we get deeper and deeper
into our Network remember that we think
of our filters as learning individual
features but we apply relu activations
to these so you can see that a lot of
these have been zeroed out later in the
process likely due to that
rayloactivation remember that for each
layer we have multiple filters so you
can see here that each of these little
square images is showing us the output
from that layer for that feature
visualizing layer activations is useful
for a couple of different things for
instance we just talked about the fact
that our filters are basically trying to
detect features and for this specific
cat that we looked at a lot of those
features weren't there especially in the
deeper layers of the convolutional
neural net next if you take a deeper
look at some of the outputs you can see
that increasing abstraction of the
deeper layers in your network remember
we talked about the fact that the first
few layers of a convolutional neural
network typically learn more General
broad patterns like vertical edges
horizontal edges or certain patterns as
we get deeper and deeper into the
network we see increasing abstraction
for instance instead of just an edge we
might see an eye of a cat visualizing
our layer activations allows us to
confirm this and see what it looks like
for different images as we push them
through our neural network next instead
of visualizing the activations the
output of each layer when we pass an
image through it we can visualize the
filters that our network has learned to
visualize our filters we ask what image
would this filter maximally respond to
in other words what image would activate
our filter the most in order to
calculate this we use gradient descent
we start with a blank image and then we
use gradient descent to make adjustments
until the filter responds maximally to
our image here you can see a bunch of
visualizations of filters from a
convolutional neural network
you can see that in the first block our
filters tend to be looking for very
simple patterns like horizontal or
vertical lines noise or whatever this is
as we get deeper and deeper in our
Network the patterns that maximally
activate our filters tend to be a little
more complicated here's one of those
original filters from the earlier layers
in the convolutional net we can tell
just by looking at it that this filter
seems to be most activated when it sees
this kind of horizontal line pattern it
kind of looks like fur and because this
network is trying to classify
something's a cat or not that makes
sense that it would want to look for
them all right so we talked about
visualizing the output of our layers and
visualizing the actual filters that our
Network learns one other thing that's
really important to visualize is Class
Activation heat Maps a common algorithm
for doing this is grad cam this creates
heat maps of how intensely the input
images activate the class for instance
let's look at this image here which has
two elephants in it when we send this
image through our Network and hopefully
it gets classified as an elephant we
might want to know what about the
picture told the network that this
should be an elephant you can see here
that we have a superimposed heat map
that basically tells us exactly where in
the image that it's looking for instance
you can see that it's pretty activated
around the ears and maybe even the trunk
of our elephants this is probably
because knowing that something has a
trunk or really large floppy ears helps
the network know that this should be
classified as an elephant in order to
get these heat Maps we have to think
about two separate things first is how
intensely an input image activated
different channels in the last layer
remember the whole point of the
convolutional process is to generate a
bunch of features that we can then put
in a feed forward neural network to try
and predict the class of something so if
you think of that last layer of the
convolutional part of the convolutional
neural network as features this
basically says which of those features
are most intensely activated by this
image when we send it through the
convolutional part of our Network so for
example if we have a bunch of different
features in the last layer from our
convolutional part maybe for an elephant
this one is really highly activated and
this one and this one and maybe this one
as well
again in order to get these values all
we have to do is send our image through
the neural network and stop it at the
last convolutional layer next we need to
ask how important are each of these
features or Channels with regard to
predicting a class for example how
important is it that big floppy ears are
in a picture when we're classifying it
as an elephant in order to figure this
out we calculate the gradient for a
specific class with respect to the
activations of this final layer that we
just calculated in order to create our
heat map we combine those two things the
measure of how intensely each of our
channels are activated in the last
convolutional layer and how important
each of those channels are for
predicting the class in this case
elephant like I mentioned before we can
see that in this picture of the
elephants first of all it's mainly
paying attention to this area of the
image which makes sense as that is where
the elephants are but within that we can
again see that one thing that it's
really paying attention to tends to be
this trunk and ear area which makes
sense even how we visually process
elephants those features probably are
really important to helping the neural
network figure out whether or not this
is an elephant now one thing important
to know is that we typically make these
Class Activation heat maps for the
predicted category so for instance
because this network classified this
image as elephants we make the Class
Activation heat map for elephant in
other words we say well you classify
this as an elephant so what made you
think that this was an elephant however
there's nothing stopping us from making
Class Activation heat maps for
non-predicted categories so for instance
we could look at this image and say what
about this image looks like a cat to you
one example that we talked about in 392
is this example of a network that was
trying to predict whether or not
something with a husky or a wolf well
the network worked really well and was
highly accurate it turns out that what
the network Learned was to detect
whether or not there was snow in the
images images of wolves that were put
into the training set tended to have
snow in them because there's just a lot
of snow where these wolves were images
of huskies however which are common pets
tended to be without know and in our
previous course we took this as an
example of making sure that our networks
are learning what we think they're
learning Class Activation heat Maps
would be a really great thing to use in
this situation because when we put an
image through the network we might be
able to see that what the image is
really paying attention to is the snow
not the actual features of the wolf or
the Husky another way to determine what
is important in an image is by using
occlusion occlusion is where we take our
original image and we just delete part
of it and when we delete that part of
the image and we send it through the
neural network we want to see how that
changes the classification of the image
for instance here you can see that the
part of the image we removed really
doesn't have to do anything with whether
or not this image is of an elephant and
you can see that when we put it through
the network it does in fact say that
it's very confident 0.95 but this is an
elephant however when we occlude
something a little more important in the
image and send it through the
probability that this picture is an
elephant actually goes down to about 75
percent this tells us that the pixels
that we occluded are very important in
helping the model predict whether or not
something is an elephant one last
related thing I wanted to talk about is
tricking convolutional neural networks
these are often called adversarial
examples adversarial examples are
created by taking an actual image and
then adding a very specific pattern of
noise that is specifically designed to
trick the neural network into
misclassifying the image for example
when we take this Panda and add this
noise we output an image that as a human
we still think looks like a panda
however the convolutional neural network
classifies this as a given a type of
monkey if we have access to the inner
workings of the neural network we can
figure out what is going to activate
that neural network the most and make it
misclassify this image even though to a
human this image looks practically the
same as the original all right to wrap
it up let's talk about different types
of computer vision tasks that we might
use convolutional architecture in order
to perform
the first one that we've pretty much
been focusing on this entire time is
image classification AKA is this image a
cat is this image a hot dog but there
are other types of things we might want
to do with images the first one is image
segmentation image segmentation is
basically seeing which pixels are a part
of different objects in our image here
on the left we have semantic
segmentation this basically is trying to
say whether or not a pixel in the image
is a cat it doesn't matter which cat it
is we're just saying is this cat or is
this not cat we can also do instant
segmentation in this case we not only
care whether a pixel is a cat or is not
a cat but we want to know where the
separate cats in the image are in image
segmentation we're taking an image and
we want to produce a mask that tells us
where the cat is and where is not cat
now normally we've talked about in
convolutional architectures that we take
a large image and we make it smaller and
smaller usually through Max pooling
throughout the process of the network
however a filter the output of what we
want our Network to have would need to
be the exact same size as our original
image this means that once we go through
that typical convolutional process we
actually have to try and undo it or
upscale the image remember Max pooling
takes an image and down samples it but
once we've done that multiple times we
actually need to re-up sample the image
so that our mask is the same size as our
original image luckily Keras has layers
to do exactly this and we'll talk about
that more in class something else you
might want to do with an image is object
detection in image classification we're
just saying yes or no this image is a
dog this image is a cat Etc in object
detection we actually want to detect
multiple objects in a picture and we
want our neural network to be able to
draw a box around where it thinks that
object is
for instance in this example you can see
that it's drawn boxes around these two
people so not only does it know that
there is a person in the image but it
knows where that person is and actually
also that there are multiple people and
multiple other objects in this image and
last but not least we might want to do
something called neural in painting the
idea behind neural impainting is that we
can take part of our image remove it and
then ask a neural network what should be
in that removed section here you can see
some examples where we take an original
image remove a huge square of it and
then ask a neural network to basically
replace what we removed now it's not
perfect but it's actually pretty good
now this can help not only when we just
crop part of our image out but it can
help remove things like logos or things
in our image that we don't want and it
can also help with things like really
old faded images that have sun spots or
cracks or something like that alright
that's almost all I have for you but I
want to leave you with one note because
this is something we're going to talk
about next week in more detail we
learned that the purpose of
convolutional neural networks is to
learn features from our image we start
with really simple features like lines
or patterns and then we get to more
complicated features deeper in the
network and if we had a convolutional
neural network that was trained on a
vast amount of data then those features
that it learned might be applicable to
multiple problems for example if we had
a convolutional neural network that was
trained on a huge range of images we
could use the features that it learned
to predict whether or not something is a
cat but we could also properly apply
those features to different problems
this actually happens a lot and you'll
notice your textbook talk a little bit
about some pre-trained models that are
available these are convolutional neural
networks that have already been trained
on a large batch of data we can then
take these models and either just use
them as is and append our own
classification or regression problem to
them but we can also take them and we
can tweak them a little bit and then do
whatever we want this is called transfer
learning where we take a Network that
has been trained and we adapt it to be a
little more specific to our problem that
we want to solve we can do this with
multiple types of neural networks and
again we'll talk a little bit more about
this next week all right now that is all
I have for you I will see you next time

 
hello and welcome to your second
generative models lecture in the last
lecture one of the things that we talked
about was the two major forms of
generative modeling the first one which
we covered last time is density
estimation these are models that
explicitly learn a density function like
in naive Bayes gaussian mixture models
or variational autoencoders that we then
sample from in order to generate new
inputs and what we focused on that last
time we also mentioned that there was a
different type of generative modeling
that's common sample generation sample
generation models still aim to generate
new values new images that look like the
original input but instead of doing this
through explicit density estimation
rather we take training data and we
train some type of generator in order to
create new samples we reward this
generator when it creates things that
look like our Training Day data and we
punish it when it doesn't so let's talk
about one of the most common types of
sample generation models the generative
adversarial Network or Gan one of the
most important Concepts behind again is
this adversarial part again is really
two models in a trench coat fighting it
out with each other we can think about
what a gan does through analogy and
again the generator is basically like a
counterfeiter the generator's job is to
create new inputs that look like the
original inputs this is like a
counterfeiter trying to generate new
fake bills that look like real currency
and the generator is battling it out
with the discriminator the
discriminator's job is to look at both
real and counterfeit examples and learn
how to discriminate between the two in
other words the discriminator is like a
cashier or a cop whose job it is to
figure out when the generator has
provided a fake example versus when
something is real we call this
adversarial training because the
generator's job is to try and trick our
discriminator by creating samples that
look as close as possible to the
original input but this isn't the only
way that people use adversarial training
for instance in our convolutional neural
net lecture we talked a little bit about
adversarial training where we take
specific batches of noise that are
created specifically to trick our
Network into classifying images that to
us look like a panda as something else
like in this case a given just like in a
gan we are here trying to deliberately
trick our model in order to help it
learn to discriminate better and many
people have hypothesized that we could
use these GPT detectors as adversarial
training for further iterations of GPT
models for instance if someone is
training gpt4 or five they might want to
use some of the most common detectors
and train their models in order to trick
these detectors alright so let's talk a
little bit about the architecture of
these two models the generator and the
discriminator the discriminator part of
the model is pretty straightforward we
actually build discriminator models all
the time like a logistic regression all
this model does is it takes in some
input maybe some tabular data maybe an
image and it tries to classify the input
as either fake or real so the
discriminator is doing a very simple
binary classification task the
discriminator is going to take in
batches of data usually half of which
that are real samples from our training
data and then half of which are fake
samples which are created with our
generator when the discriminator is
training it's just trying to classify
real samples as real and fake samples is
fake and that's what its loss function
depends on can it accurately classify
real samples is real and fake ones is
fake the generator is a bit more
complicated
the generator is trying to create fake
examples but the only way we can tell if
it's done that successfully is if it can
trick the discriminator into thinking
that fake samples are real the generator
creates fake images by taking in random
noise usually this is from a uniform
distribution inputting it into the
generator and then outputting something
that looks like a fake example at first
the generator is going to be pretty bad
and its examples would be very easily
distinguishable from real samples but
that feedback is crucial to the
generator because as the discriminator
decides which is real and which is fake
we use that information to train the
generator into producing better outputs
so again the generator takes in some
random noise and it's trying to learn
some function that can take that noise
and turn it into something that looks
like a real example from our training
data the way it learns is by getting
information or feedback from the
discriminator based on what can trick
the discriminator or not because our Gan
is basically two neural networks in a
trench coat we're going to train each
part separately typically what we're
going to do is we're going to train the
discriminator holding the generator
constant so while the generator is
creating its fake inputs we are then
going to use those to train our
discriminator updating the weights of
the discriminator only then once we've
trained the discriminator we are going
to train the generator holding the
discriminator constant and then we
repeat these steps until hopefully our
gain converges and is generating pretty
realistic images people often talk about
Gans as kind of a zero-sum game where
both the generator and the discriminator
are trying to win and what are they
trying to win well let's talk a little
bit about the loss function we use for
again the original Gan loss function
that was proposed looks like this
basically it consists of two components
let's look at the left hand side first
we're going to look at this term D of x
d of X tells us that when we put a real
sample referred to as X into our
discriminator we're gonna get out of
predicted probability that that sample
is real now because we specify that X is
a real sample from our data set we want
this value to be very high in other
words we want our discriminator to be
fairly confident that this real sample
actually is real then we take the log of
that value and the expectation over all
possible samples in our training data
basically this term refers to the
expected value of a discriminator's
output when this sample is real if our
discriminator is doing a really good job
a d of X should be very high in other
words there's a high predicted
probability that the sample is indeed
real when our predicted probability is
real so will the log of that predicted
probability and therefore the overall
expectation for all samples should also
be high if the discriminator is
consistently good at detecting that real
samples are real on the right hand side
of this loss function we have another
output D of G of Z the Z here represents
that random noise that we feed into the
generator and G of Z is what happens
when we put that noise through the
generator basically it's our fake output
that the generator is creating just like
before D of G of Z refers to the
discriminator's estimate that a fake
sample is real just like before D refers
to the discriminator's output so D of G
of Z is basically saying when we take
noise put it through a gender generator
and then feed that output to the
discriminator what is the
discriminator's predicted probability
that that sample is real now remember
when we take some noise Z and put it
through our generator G we are creating
a fake example so if the discriminator
is working well D of G of Z should be
very low because the discriminator will
go that is very unlikely to be a real
sample thus for a well-performing
discriminator this entire term here
should be very small and when we take
one minus a very small number we'll get
something very close to one thus when
the discriminator is performing well
this portion of the loss function should
also be high like before we're going to
take the log of this value and the
expectation over all possible fake
samples if the discriminator is
consistently good then across all
possible fake examples that we have this
D of G of Z should be very small and
thus this side of the equation and
should be quite large now remember our
Gan is really two separate neural
networks battling it out so as we look
at this function you can tell that the
discriminator wants to maximize this
function in remember a well-performing
discriminator will have a high value for
the estimate that real samples are real
and a low estimate for fake samples
being real if you plug in those values
you'll notice that that maximizes this
loss function but the discriminator is
battling it out with the generator the
generator would like to minimize this
loss function now the generator really
has no control over how the
discriminator classifies real samples so
this part of the loss function doesn't
really matter to our generator rather we
focus on this side we can change the
parameters of the generator in order to
create samples the discriminator is
unsure about or maybe even thinks are
real thus if we have a good generator
this D of G of Z is actually going to be
quite high in other words if our
generator is good at creating fake
examples the discriminator is going to
be confused and think that those
examples are real thus this value will
be high and this overall value will be
low so now that we know a little bit
about the original gain loss function
let's talk about how we optimize it
here's some pseudo code looking at how
the training process for again might
work first we have a for Loop this is
going to repeat once for each iteration
or update to our Network the first step
is to train the discriminator to do this
we're going to hold the generator
constant so we're not going to be
updating any weights in the generator at
this point only in the discriminator and
while we don't have to do this it is
traditional that we might update the
discriminator a couple of times in each
iteration that's what this second for
Loop is doing for each of these steps
we're going to generate a batch of real
data by just sampling from our training
data and a sample of fake data we're
going to do this by putting random noise
into our generator and asking for fake
samples we're then going to take these
real and fake samples and use gradient
Ascent because remember we're trying
trying to maximize the loss function at
this point in order to update our
discriminator's parameters once we've
updated the discriminator parameters we
then move on to the generator to train
the generator we are going to hold our
discriminator constant this means we're
going to use all of the parameter
updates we made here but we're not
actually updating the discriminator in
this part in order to train the
generator we're going to generate a
bunch of fake samples and then we're
going to send those samples through the
discriminator and see what the predicted
probabilities are using these predicted
probabilities we're going to use
gradient descent because of course we
would like to minimize our loss function
for the generator to adjust the
generator parameters remember in this
step we're not touching the
discriminator all we're doing is
updating the generator parameters we
then repeat these steps holding
generator constant updating
discriminator holding discriminator
constant updating generator over and
over until hopefully our Gan converges
however in practice training Gans can be
very difficult and this is an open
problem that people are still working on
so let's talk about some of the things
that can go wrong when training again
the first thing that can happen is
actually basically too much of a good
thing what if the discriminator in our
model is just too good one common
problem is that the discriminator in our
Gan is too good at predicting that fake
images are fake with really high
confidence you can think of an analogy
here it's easier to recognize a van Gogh
painting than it would be to generate a
van Gogh painting for yourself thus the
discriminator often has an easier task
the generator in one of his papers about
Gans the Creator Ian Goodfellow says
this in the Minimax game this is where
we're having the generator and the
discriminator compete the discriminator
minimizes a cross-entropy function but
the generator maximizes that same cross
entropy this is unfortunate for the
generator because if the discriminator
successfully rejects generator samples
with high confidence the generator's
gradient vanishes basically this means
that if the discriminator is too good at
telling whether generator samples are
fake then the generator can't really
learn how it might be able to trick the
discriminator we can see what this looks
like using this image from Ian
goodfellow's paper on the y-axis we have
the predicted probability of a fake
sample created by the generator of being
real remember a successful discriminator
will be able to tell that this image is
fake and therefore have a very low
predicted probability that a fake image
that would be represented sort of in
this region of the x-axis a poor
discriminator would not be able to tell
that this fake image is real therefore
we would be more in this region of the
graph on the y-axis we have the loss
function of the generator remember that
we've talked about in gradient descent
that if our gradient or our learning
rate are zero then we can't actually
make updates to any of our parameters
and therefore our model can't actually
learn anything this graph shows us that
when the discriminator is really good
AKA when we're in this region of the
graph the gradient is going to go to
zero or near zero when this gradient is
zero or near zero this means that we
can't actually make updates to our
generator and therefore generator is
unable to learn how to generate
realistic looking samples again what
this graph shows us is that when our
discriminator is too good AKA it's
rejecting generator samples by
predicting very confidently that fake
samples are fake then our generator is
not really able to learn how to generate
effective samples that can trick the
discriminator this is because when our
discriminator is too good the gradient
of our generator loss function is 0 or
near zero one modification that's been
suggested in order to help fix this is
by switching the way that the generator
is measuring success in the original
gain loss function the generator is
trying to minimize the probability of an
image being classified as fake but we
can reframe that as the generator
wanting to maximize the probability that
the image is real this helps make
updates to the generator be more stable
so that the generator can still learn
how to trick the discriminator another
common issue that games can face when
training is something called mode
collapse mode collapse is when the
generator just creates the same sample
over and over or a few of similar
samples over and over instead of
generating a broad range of real inputs
you can see this happening visually in
this mnist data set where we're
generating values that look like digits
in these top images you can see that
there isn't any mode collapse we are
seeing a wide variety of different
digits being produced by our model on
the bottom however you do see mode
collapse basically this happens because
the generator learns that there is an
optimal example that can trick the
discriminator into thinking that fake
output is real so instead of learning a
wide variety of things the generator
just keeps creating the same exact fake
example no matter what you input into
the generator this is a problem because
we want our generator to be able to
create a wide variety of samples that
look like all of the different samples
that we had in our training data we
don't just want it to keep creating the
same fake image over and over and this
is an open problem in Gan training so if
you ever are training again it's really
important to make sure you're checking
to see if mode collapse is happening
last but not least Gans also have a
problem with convergence remember that
unlike other neural networks Gans are
two neural networks that are really
battling it out and that have opposing
goals the discriminator's job is to be
able to tell really accurately whether
samples are fake or real and the
generator's job is to trick the
are so close to the real examples that
the discriminator can't tell the
difference last but not least a problem
that Gans face when training is a lack
of convergence remember unlike most
models that we've talked about Gans are
two separate models that have opposing
purposes that are being trained together
while sometimes these two things
training in tandem will be able to reach
some type of equilibrium oftentimes they
won't be able to as Ian Goodfellow says
in his paper on Gans sometimes the two
players the generator and the
discriminator will eventually reach an
equilibrium but in other scenarios they
just repeatedly undo each other's
progress without actually learning
anything useful so it sounds like Gans
have a lot of problems when it comes to
training and we already talked about one
potential solution using a
non-saturating gan loss however there's
other proposed solutions that people are
commonly using now one of those is the
wasserstein Gan the wassersting Gan or W
Gan changes the loss function of the Gan
entirely in a typical Gan our
discriminator is predicting a
probability a value between 0 and 1 that
indicates How likely it is that an item
is fake or real thus the discriminator's
output is strictly bounded between 0 and
1 and we could use for example 50 as a
threshold to classify things as real
versus fake in a wassersting game the
discriminator doesn't generate a
probability ability that something's
real rather it generates more of a
continuous score that's not bounded
between 0 and 1. the goal of a
wassersting gan when you're training is
for the discriminator to maximize this
value this means that our wasserstein
Gan would like for the discriminator to
Output higher values for real samples
than for fake same when the
discriminator's output is higher for
real samples this number will be very
high and this number will be very small
thus when we take a hopefully big number
and subtract a small number we should
get an overall large number the
generator on the other hand wants to
maximize the discriminator's output for
a fake sample thus the generator
maximizes this function because of
wasserstein Gans discriminator doesn't
output a probability but rather just a
score we often refer to it not as a
discriminator but as a Critic the critic
scores real and fake items and a good
critic will have higher scores for real
items and lower scores for fake items
using the wasserstein Gan can help
address two of the problems we just
talked about the saturating gain loss as
well as the mode collapse there's a few
other changes that wasserstein Gans make
in comparison to the original Gan
architecture which we won't go over here
but what's important to remember is that
because of some of the issues that we
were talking about previously a
wasserstein gan has a Critic instead of
a discriminator instead of generating a
probability that something is real the
wasserstein Gan critic will create a
score and the goal is that real samples
will have a higher score than fake
samples the generator of course wants
the opposite it wants to have as high of
scores as possible for fake samples
lastly let's talk about one other
modification that we can make to Gans
not necessarily to improve some of the
issues with the loss that we're talking
about but in order to make our Gans a
little bit more useful I can additional
Gan is a simple Gan architecture that
takes in an extra input into both the
discriminator and the generator let's
think of the mnist data set that has a
bunch of different digit images those
digit images actually have class values
right we have a class of zeros of ones
of twos of Threes etc for each of our
digits we can feed that class
information into both our generator and
discriminator in order to help us learn
how to generate items from a specific
class
so instead of telling the generator give
me a digit we could tell the generator
give me a one specifically our hope is
that our generator is going to learn how
to produce samples that involve
information about the class a
conditional Gan has a very similar
architecture to a typical game but the
discriminator and the generator will
just both have an additional input which
is usually something like a one-hot
encoded Vector indicating what group a
specific sample is from for instance
here if we were using digits we would
feed in which digit this item is we'd
feed that into the generator along with
random noise in order to generate a fake
sample we'd also feed that into the
discriminator in order to allow the
discriminator to decide whether or not
that image is fake or real given that
it's a one to review today we talked
about generative adversarial networks
this is an example of generative models
that are doing sample generation
contrasted to density estimation which
which we covered with variational
autoencoders generative adversarial
networks or Gans are basically two
neural networks in a trench coat that
are playing a game against each other
again has two main components the
generator whose job it is to create fake
inputs and a discriminator whose job it
is to tell whether something is real or
fake the real data in the discriminator
comes from our actual training data so
for instance images of animals or Foods
the fake samples come from the generator
and they're created by feeding random
noise into the generator and asking the
generator to turn that in to a fake
example the generator's goal is to trick
the discriminator into thinking that its
fake output is real the discriminator's
goal is to be able to easily tell the
difference between fake and real samples
these two parts of the Gan battle it out
and in ideal situation will converge so
that our generator is creating items
that are so similar to the original
training data that the discriminator
can't tell the difference however we
also talked about some issues that Gans
have when training namely The Vanishing
gradient for the generator mode collapse
and lastly a lack of convergence while
these are still pretty open problems we
did talk about a few solutions to some
of them first of all we can use a
non-saturating gan loss that helps us
make sure that the gradient of the
generator is not Vanishing and we can
also switch to a wasserstein gan or W
Gan instead of the discriminator
outputting a predicted probability the
wasserstein critic outputs a continuous
score the goal of the discriminator is
to always have higher scores for real
samples and the goal of the generator is
to have as highest score as possible for
fake exams lastly we talked about
conditional gains which feed in
additional information in order to have
a model that can generate items from a
specific class for instance we could
train scan on various animals and ask it
specifically to create a fake cat for us
all right that's all I have for you I
will see you next time


